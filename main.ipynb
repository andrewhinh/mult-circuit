{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "Imports, device, and utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import get_ipython\n",
    "\n",
    "ip = get_ipython()\n",
    "if not ip.extension_manager.loaded:\n",
    "    ip.extension_manager.load(\"autoreload\")\n",
    "    %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.io as pio\n",
    "\n",
    "pio.renderers.default = \"notebook_connected\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"circuits-vis-ead331a8-8274\" style=\"margin: 15px 0;\"/>\n",
       "    <script crossorigin type=\"module\">\n",
       "    import { render, Hello } from \"https://unpkg.com/circuitsvis@1.43.3/dist/cdn/esm.js\";\n",
       "    render(\n",
       "      \"circuits-vis-ead331a8-8274\",\n",
       "      Hello,\n",
       "      {\"name\": \"Andrew\"}\n",
       "    )\n",
       "    </script>"
      ],
      "text/plain": [
       "<circuitsvis.utils.render.RenderedHTML at 0x773e12be0bf0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import circuitsvis as cv\n",
    "\n",
    "# Testing that the library works\n",
    "cv.examples.hello(\"Andrew\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import stuff\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import einops\n",
    "import numpy as np\n",
    "from circuitsvis.attention import attention_heads\n",
    "from fancy_einsum import einsum\n",
    "from IPython.display import HTML, IFrame\n",
    "import tqdm.auto as tqdm\n",
    "import plotly.express as px\n",
    "\n",
    "from jaxtyping import Float\n",
    "from functools import partial\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import transformer_lens\n",
    "import transformer_lens.utils as utils\n",
    "from transformer_lens.hook_points import (\n",
    "    HookPoint,\n",
    ")  # Hooking utilities\n",
    "from transformer_lens import ActivationCache, HookedTransformer, FactoredMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disabled automatic differentiation\n"
     ]
    }
   ],
   "source": [
    "# save GPU mem for inference\n",
    "torch.set_grad_enabled(False)\n",
    "print(\"Disabled automatic differentiation\")\n",
    "\n",
    "# clear cache\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting helpers\n",
    "def imshow(tensor, renderer=None, **kwargs):\n",
    "    px.imshow(\n",
    "        utils.to_numpy(tensor),\n",
    "        color_continuous_midpoint=0.0,\n",
    "        color_continuous_scale=\"RdBu\",\n",
    "        **kwargs,\n",
    "    ).show(renderer)\n",
    "\n",
    "\n",
    "def line(tensor, renderer=None, xaxis=\"\", yaxis=\"\", **kwargs):\n",
    "    px.line(utils.to_numpy(tensor), labels={\"x\": xaxis, \"y\": yaxis}, **kwargs).show(\n",
    "        renderer\n",
    "    )\n",
    "\n",
    "\n",
    "def scatter(x, y, xaxis=\"\", yaxis=\"\", caxis=\"\", renderer=None, **kwargs):\n",
    "    x = utils.to_numpy(x)\n",
    "    y = utils.to_numpy(y)\n",
    "    px.scatter(\n",
    "        y=y, x=x, labels={\"x\": xaxis, \"y\": yaxis, \"color\": caxis}, **kwargs\n",
    "    ).show(renderer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = utils.get_device()\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1234\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model\n",
    "\n",
    "HookedTransformer is a somewhat adapted GPT-2 architecture, but is computationally identical. The most significant changes are to the internal structure of the attention heads:\n",
    "\n",
    "- The weights (W_K, W_Q, W_V) mapping the residual stream to queries, keys and values are 3 separate matrices, rather than big concatenated one.\n",
    "- The weight matrices (W_K, W_Q, W_V, W_O) and activations (keys, queries, values, z (values mixed by attention pattern)) have separate head_index and d_head axes, rather than flattening them into one big axis.\n",
    "  - The activations all have shape `[batch, position, head_index, d_head]`\n",
    "  - W_K, W_Q, W_V have shape `[head_index, d_model, d_head]` and W_O has shape `[head_index, d_head, d_model]`\n",
    "\n",
    "The various flags are simplifications that preserve the model's output but simplify its internals.\n",
    "We verify this by comparing the logits of the original model and the HookedTransformer model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assert_hf_and_tl_model_are_close(\n",
    "    hf_model,\n",
    "    tl_model,\n",
    "    tokenizer,\n",
    "    prompt=\"12 x 34 = \",\n",
    "    atol=1e-5,\n",
    "):\n",
    "    prompt_toks = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "    hf_logits = hf_model(prompt_toks.to(hf_model.device)).logits\n",
    "    tl_logits = tl_model(prompt_toks).to(hf_logits)\n",
    "\n",
    "    assert torch.allclose(\n",
    "        torch.softmax(hf_logits, dim=-1), torch.softmax(tl_logits, dim=-1), atol=atol\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model Qwen/Qwen2.5-0.5B into HookedTransformer\n",
      "Moving model to device:  cuda\n"
     ]
    }
   ],
   "source": [
    "# NBVAL_IGNORE_OUTPUT\n",
    "\n",
    "model_path = \"Qwen/Qwen2.5-0.5B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "\n",
    "hf_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    device_map=device,\n",
    "    trust_remote_code=True,\n",
    ").eval()\n",
    "\n",
    "tl_model = HookedTransformer.from_pretrained_no_processing(\n",
    "    model_path,\n",
    "    device=device,\n",
    "    dtype=torch.float32,\n",
    "    center_unembed=True,\n",
    "    center_writing_weights=True,\n",
    "    fold_ln=True,\n",
    ").to(device)\n",
    "\n",
    "assert_hf_and_tl_model_are_close(hf_model, tl_model, tokenizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Names\n",
    "\n",
    "Here is a list of the parameters and shapes in the model. By convention, all weight matrices multiply on the right (ie `new_activation = old_activation @ weights + bias`).\n",
    "\n",
    "Reminder of the key hyper-params:\n",
    "\n",
    "- `n_layers`: 12. The number of transformer blocks in the model (a block contains an attention layer and an MLP layer)\n",
    "- `n_heads`: 14. The number of attention heads per attention layer\n",
    "- `d_model`: 768. The residual stream width.\n",
    "- `d_head`: 64. The internal dimension of an attention head activation.\n",
    "- `d_mlp`: 3072. The internal dimension of the MLP layers (ie the number of neurons).\n",
    "- `d_vocab`: 50257. The number of tokens in the vocabulary.\n",
    "- `n_ctx`: 1024. The maximum number of tokens in an input prompt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blocks.0.attn.W_Q torch.Size([14, 896, 64])\n",
      "blocks.0.attn.W_O torch.Size([14, 64, 896])\n",
      "blocks.0.attn.b_Q torch.Size([14, 64])\n",
      "blocks.0.attn.b_O torch.Size([896])\n",
      "blocks.0.attn._W_K torch.Size([2, 896, 64])\n",
      "blocks.0.attn._W_V torch.Size([2, 896, 64])\n",
      "blocks.0.attn._b_K torch.Size([2, 64])\n",
      "blocks.0.attn._b_V torch.Size([2, 64])\n",
      "blocks.0.mlp.W_in torch.Size([896, 4864])\n",
      "blocks.0.mlp.W_out torch.Size([4864, 896])\n",
      "blocks.0.mlp.W_gate torch.Size([896, 4864])\n",
      "blocks.0.mlp.b_in torch.Size([4864])\n",
      "blocks.0.mlp.b_out torch.Size([896])\n"
     ]
    }
   ],
   "source": [
    "for name, param in tl_model.named_parameters():\n",
    "    if name.startswith(\"blocks.0.\"):\n",
    "        print(name, param.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embed.W_E torch.Size([151936, 896])\n",
      "unembed.W_U torch.Size([896, 151936])\n",
      "unembed.b_U torch.Size([151936])\n"
     ]
    }
   ],
   "source": [
    "for name, param in tl_model.named_parameters():\n",
    "    if not name.startswith(\"blocks\"):\n",
    "        print(name, param.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation + Hook Names\n",
    "\n",
    "Let's get a list of all model activations/hook names by entering in a short prompt and add a hook function to each activations to print its name and shape. To avoid spam, let's just add this to activations in the first block or not in a block.\n",
    "\n",
    "Note 1: Each LayerNorm has a hook for the scale factor (ie the standard deviation of the input activations for each token position & batch element) and for the normalized output (ie the input activation with mean 0 and standard deviation 1, but _before_ applying scaling or translating with learned weights). LayerNorm is applied every time a layer reads from the residual stream: `ln1` is the LayerNorm before the attention layer in a block, `ln2` the one before the MLP layer, and `ln_final` is the LayerNorm before the unembed.\n",
    "\n",
    "Note 2: _Every_ activation apart from the attention pattern and attention scores has shape beginning with `[batch, position]`. The attention pattern and scores have shape `[batch, head_index, dest_position, source_position]` (the numbers are the same, unless we're using caching).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num tokens: 14\n",
      "hook_embed torch.Size([1, 14, 896])\n",
      "blocks.0.hook_resid_pre torch.Size([1, 14, 896])\n",
      "blocks.0.ln1.hook_scale torch.Size([1, 14, 1])\n",
      "blocks.0.ln1.hook_normalized torch.Size([1, 14, 896])\n",
      "blocks.0.ln1.hook_scale torch.Size([1, 14, 1])\n",
      "blocks.0.ln1.hook_normalized torch.Size([1, 14, 896])\n",
      "blocks.0.ln1.hook_scale torch.Size([1, 14, 1])\n",
      "blocks.0.ln1.hook_normalized torch.Size([1, 14, 896])\n",
      "blocks.0.attn.hook_q torch.Size([1, 14, 14, 64])\n",
      "blocks.0.attn.hook_k torch.Size([1, 14, 2, 64])\n",
      "blocks.0.attn.hook_v torch.Size([1, 14, 2, 64])\n",
      "blocks.0.attn.hook_rot_q torch.Size([1, 14, 14, 64])\n",
      "blocks.0.attn.hook_rot_k torch.Size([1, 14, 2, 64])\n",
      "blocks.0.attn.hook_attn_scores torch.Size([1, 14, 14, 14])\n",
      "blocks.0.attn.hook_pattern torch.Size([1, 14, 14, 14])\n",
      "blocks.0.attn.hook_z torch.Size([1, 14, 14, 64])\n",
      "blocks.0.hook_attn_out torch.Size([1, 14, 896])\n",
      "blocks.0.hook_resid_mid torch.Size([1, 14, 896])\n",
      "blocks.0.ln2.hook_scale torch.Size([1, 14, 1])\n",
      "blocks.0.ln2.hook_normalized torch.Size([1, 14, 896])\n",
      "blocks.0.mlp.hook_pre torch.Size([1, 14, 4864])\n",
      "blocks.0.mlp.hook_pre_linear torch.Size([1, 14, 4864])\n",
      "blocks.0.mlp.hook_post torch.Size([1, 14, 4864])\n",
      "blocks.0.hook_mlp_out torch.Size([1, 14, 896])\n",
      "blocks.0.hook_resid_post torch.Size([1, 14, 896])\n",
      "ln_final.hook_scale torch.Size([1, 14, 1])\n",
      "ln_final.hook_normalized torch.Size([1, 14, 896])\n"
     ]
    }
   ],
   "source": [
    "example_problem = \"12345 x 54321 = \"\n",
    "print(\"Num tokens:\", len(tl_model.to_tokens(example_problem)[0]))\n",
    "\n",
    "\n",
    "def print_name_shape_hook_function(activation, hook):\n",
    "    print(hook.name, activation.shape)\n",
    "\n",
    "\n",
    "def not_in_late_block_filter(name):\n",
    "    return name.startswith(\"blocks.0.\") or not name.startswith(\"blocks\")\n",
    "\n",
    "\n",
    "tl_model.run_with_hooks(\n",
    "    example_problem,\n",
    "    return_type=None,\n",
    "    fwd_hooks=[(not_in_late_block_filter, print_name_shape_hook_function)],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task: Multi-digit multiplication\n",
    "\n",
    "The next step is to verify that the model can _actually_ do the task!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68067 x 44086 = 3000801762\n"
     ]
    }
   ],
   "source": [
    "# Generate an example multiplication problem\n",
    "problems = []\n",
    "answers = []\n",
    "num_digits = 5\n",
    "\n",
    "# Create numbers with num_digits digits\n",
    "num1 = np.random.randint(10 ** (num_digits - 1), 10**num_digits)\n",
    "num2 = np.random.randint(10 ** (num_digits - 1), 10**num_digits)\n",
    "problems.append(f\"{num1} x {num2} =\")\n",
    "answers.append(f\" {num1 * num2}\")\n",
    "\n",
    "# Display the problems\n",
    "for problem, answer in zip(problems, answers):\n",
    "    print(problem + answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<|endoftext|>', '6', '8', '0', '6', '7', ' x', ' ', '4', '4', '0', '8', '6', ' =']\n",
      "Tokenized answer: [' ', '3', '0', '0', '0', '8', '0', '1', '7', '6', '2']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20.62</span><span style=\"font-weight: bold\"> Prob: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">84.42</span><span style=\"font-weight: bold\">% Token: | |</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m20.62\u001b[0m\u001b[1m Prob: \u001b[0m\u001b[1;36m84.42\u001b[0m\u001b[1m% Token: | |\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 20.62 Prob: 84.42% Token: | |\n",
      "Top 1th token. Logit: 17.73 Prob:  4.66% Token: | ?|\n",
      "Top 2th token. Logit: 17.67 Prob:  4.40% Token: | ?\n",
      "|\n",
      "Top 3th token. Logit: 16.26 Prob:  1.07% Token: | ?\n",
      "\n",
      "|\n",
      "Top 4th token. Logit: 15.32 Prob:  0.42% Token: | (|\n",
      "Top 5th token. Logit: 15.22 Prob:  0.38% Token: | x|\n",
      "Top 6th token. Logit: 14.43 Prob:  0.17% Token: | __|\n",
      "Top 7th token. Logit: 14.39 Prob:  0.17% Token: | $|\n",
      "Top 8th token. Logit: 14.35 Prob:  0.16% Token: |1|\n",
      "Top 9th token. Logit: 14.16 Prob:  0.13% Token: | \\|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">21.82</span><span style=\"font-weight: bold\"> Prob: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">73.58</span><span style=\"font-weight: bold\">% Token: |</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"font-weight: bold\">|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m21.82\u001b[0m\u001b[1m Prob: \u001b[0m\u001b[1;36m73.58\u001b[0m\u001b[1m% Token: |\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1m|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 21.82 Prob: 73.58% Token: |3|\n",
      "Top 1th token. Logit: 20.34 Prob: 16.88% Token: |2|\n",
      "Top 2th token. Logit: 18.31 Prob:  2.21% Token: |1|\n",
      "Top 3th token. Logit: 18.24 Prob:  2.05% Token: |4|\n",
      "Top 4th token. Logit: 17.93 Prob:  1.50% Token: |6|\n",
      "Top 5th token. Logit: 17.39 Prob:  0.88% Token: |8|\n",
      "Top 6th token. Logit: 17.27 Prob:  0.78% Token: |5|\n",
      "Top 7th token. Logit: 17.19 Prob:  0.72% Token: |0|\n",
      "Top 8th token. Logit: 17.11 Prob:  0.67% Token: |9|\n",
      "Top 9th token. Logit: 17.06 Prob:  0.63% Token: |7|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">19.41</span><span style=\"font-weight: bold\"> Prob: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12.84</span><span style=\"font-weight: bold\">% Token: |</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m19.41\u001b[0m\u001b[1m Prob: \u001b[0m\u001b[1;36m12.84\u001b[0m\u001b[1m% Token: |\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 20.86 Prob: 54.88% Token: |1|\n",
      "Top 1th token. Logit: 19.79 Prob: 18.67% Token: |2|\n",
      "Top 2th token. Logit: 19.41 Prob: 12.84% Token: |0|\n",
      "Top 3th token. Logit: 18.36 Prob:  4.49% Token: |3|\n",
      "Top 4th token. Logit: 18.33 Prob:  4.37% Token: |.|\n",
      "Top 5th token. Logit: 17.63 Prob:  2.17% Token: |4|\n",
      "Top 6th token. Logit: 16.80 Prob:  0.94% Token: |5|\n",
      "Top 7th token. Logit: 15.82 Prob:  0.36% Token: |6|\n",
      "Top 8th token. Logit: 15.24 Prob:  0.20% Token: |\n",
      "|\n",
      "Top 9th token. Logit: 15.23 Prob:  0.20% Token: |,|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">18.05</span><span style=\"font-weight: bold\"> Prob: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11.63</span><span style=\"font-weight: bold\">% Token: |</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m18.05\u001b[0m\u001b[1m Prob: \u001b[0m\u001b[1;36m11.63\u001b[0m\u001b[1m% Token: |\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 18.31 Prob: 15.14% Token: |7|\n",
      "Top 1th token. Logit: 18.05 Prob: 11.63% Token: |0|\n",
      "Top 2th token. Logit: 17.83 Prob:  9.39% Token: |1|\n",
      "Top 3th token. Logit: 17.72 Prob:  8.38% Token: |2|\n",
      "Top 4th token. Logit: 17.70 Prob:  8.21% Token: |3|\n",
      "Top 5th token. Logit: 17.64 Prob:  7.75% Token: |4|\n",
      "Top 6th token. Logit: 17.63 Prob:  7.64% Token: |5|\n",
      "Top 7th token. Logit: 17.61 Prob:  7.49% Token: |6|\n",
      "Top 8th token. Logit: 17.60 Prob:  7.43% Token: |9|\n",
      "Top 9th token. Logit: 17.58 Prob:  7.28% Token: |8|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">18.35</span><span style=\"font-weight: bold\"> Prob: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13.43</span><span style=\"font-weight: bold\">% Token: |</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m18.35\u001b[0m\u001b[1m Prob: \u001b[0m\u001b[1;36m13.43\u001b[0m\u001b[1m% Token: |\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 18.35 Prob: 13.43% Token: |0|\n",
      "Top 1th token. Logit: 18.10 Prob: 10.43% Token: |1|\n",
      "Top 2th token. Logit: 17.97 Prob:  9.17% Token: |7|\n",
      "Top 3th token. Logit: 17.96 Prob:  9.09% Token: |8|\n",
      "Top 4th token. Logit: 17.94 Prob:  8.94% Token: |6|\n",
      "Top 5th token. Logit: 17.92 Prob:  8.74% Token: |9|\n",
      "Top 6th token. Logit: 17.90 Prob:  8.62% Token: |2|\n",
      "Top 7th token. Logit: 17.89 Prob:  8.47% Token: |4|\n",
      "Top 8th token. Logit: 17.85 Prob:  8.16% Token: |3|\n",
      "Top 9th token. Logit: 17.78 Prob:  7.65% Token: |5|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16.30</span><span style=\"font-weight: bold\"> Prob:  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.55</span><span style=\"font-weight: bold\">% Token: |</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span><span style=\"font-weight: bold\">|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m4\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m16.30\u001b[0m\u001b[1m Prob:  \u001b[0m\u001b[1;36m2.55\u001b[0m\u001b[1m% Token: |\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1m|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 19.61 Prob: 69.52% Token: |0|\n",
      "Top 1th token. Logit: 16.73 Prob:  3.90% Token: |1|\n",
      "Top 2th token. Logit: 16.51 Prob:  3.14% Token: |3|\n",
      "Top 3th token. Logit: 16.39 Prob:  2.78% Token: |2|\n",
      "Top 4th token. Logit: 16.30 Prob:  2.55% Token: |8|\n",
      "Top 5th token. Logit: 16.29 Prob:  2.51% Token: |6|\n",
      "Top 6th token. Logit: 16.25 Prob:  2.43% Token: |4|\n",
      "Top 7th token. Logit: 16.17 Prob:  2.23% Token: |7|\n",
      "Top 8th token. Logit: 16.09 Prob:  2.05% Token: |9|\n",
      "Top 9th token. Logit: 15.97 Prob:  1.82% Token: |5|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">17.25</span><span style=\"font-weight: bold\"> Prob: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">18.50</span><span style=\"font-weight: bold\">% Token: |</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m17.25\u001b[0m\u001b[1m Prob: \u001b[0m\u001b[1;36m18.50\u001b[0m\u001b[1m% Token: |\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 17.25 Prob: 18.50% Token: |0|\n",
      "Top 1th token. Logit: 16.59 Prob:  9.62% Token: |8|\n",
      "Top 2th token. Logit: 16.55 Prob:  9.18% Token: |2|\n",
      "Top 3th token. Logit: 16.54 Prob:  9.11% Token: |4|\n",
      "Top 4th token. Logit: 16.51 Prob:  8.81% Token: |6|\n",
      "Top 5th token. Logit: 16.30 Prob:  7.18% Token: |1|\n",
      "Top 6th token. Logit: 16.26 Prob:  6.90% Token: |5|\n",
      "Top 7th token. Logit: 16.24 Prob:  6.76% Token: |9|\n",
      "Top 8th token. Logit: 16.19 Prob:  6.41% Token: |7|\n",
      "Top 9th token. Logit: 16.16 Prob:  6.23% Token: |3|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">17.44</span><span style=\"font-weight: bold\"> Prob:  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7.80</span><span style=\"font-weight: bold\">% Token: |</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m5\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m17.44\u001b[0m\u001b[1m Prob:  \u001b[0m\u001b[1;36m7.80\u001b[0m\u001b[1m% Token: |\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 18.17 Prob: 16.08% Token: |0|\n",
      "Top 1th token. Logit: 18.06 Prob: 14.45% Token: |8|\n",
      "Top 2th token. Logit: 17.84 Prob: 11.60% Token: |4|\n",
      "Top 3th token. Logit: 17.74 Prob: 10.51% Token: |2|\n",
      "Top 4th token. Logit: 17.59 Prob:  9.03% Token: |3|\n",
      "Top 5th token. Logit: 17.44 Prob:  7.80% Token: |1|\n",
      "Top 6th token. Logit: 17.25 Prob:  6.40% Token: |6|\n",
      "Top 7th token. Logit: 17.10 Prob:  5.54% Token: |9|\n",
      "Top 8th token. Logit: 16.88 Prob:  4.44% Token: |7|\n",
      "Top 9th token. Logit: 16.82 Prob:  4.17% Token: |5|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16.95</span><span style=\"font-weight: bold\"> Prob:  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4.75</span><span style=\"font-weight: bold\">% Token: |</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span><span style=\"font-weight: bold\">|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m7\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m16.95\u001b[0m\u001b[1m Prob:  \u001b[0m\u001b[1;36m4.75\u001b[0m\u001b[1m% Token: |\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1m|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 18.89 Prob: 33.17% Token: |2|\n",
      "Top 1th token. Logit: 18.11 Prob: 15.21% Token: |6|\n",
      "Top 2th token. Logit: 17.34 Prob:  7.06% Token: |8|\n",
      "Top 3th token. Logit: 17.34 Prob:  7.03% Token: |0|\n",
      "Top 4th token. Logit: 17.25 Prob:  6.43% Token: |4|\n",
      "Top 5th token. Logit: 17.25 Prob:  6.40% Token: |9|\n",
      "Top 6th token. Logit: 17.01 Prob:  5.03% Token: |3|\n",
      "Top 7th token. Logit: 16.95 Prob:  4.75% Token: |7|\n",
      "Top 8th token. Logit: 16.68 Prob:  3.65% Token: |1|\n",
      "Top 9th token. Logit: 16.67 Prob:  3.59% Token: |5|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">18.13</span><span style=\"font-weight: bold\"> Prob: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">24.32</span><span style=\"font-weight: bold\">% Token: |</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span><span style=\"font-weight: bold\">|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m18.13\u001b[0m\u001b[1m Prob: \u001b[0m\u001b[1;36m24.32\u001b[0m\u001b[1m% Token: |\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1m|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 18.13 Prob: 24.32% Token: |6|\n",
      "Top 1th token. Logit: 17.96 Prob: 20.61% Token: |2|\n",
      "Top 2th token. Logit: 16.87 Prob:  6.88% Token: |\n",
      "|\n",
      "Top 3th token. Logit: 16.84 Prob:  6.72% Token: |0|\n",
      "Top 4th token. Logit: 16.78 Prob:  6.29% Token: |8|\n",
      "Top 5th token. Logit: 16.64 Prob:  5.49% Token: |4|\n",
      "Top 6th token. Logit: 16.50 Prob:  4.76% Token: |3|\n",
      "Top 7th token. Logit: 16.43 Prob:  4.44% Token: |7|\n",
      "Top 8th token. Logit: 16.41 Prob:  4.37% Token: |9|\n",
      "Top 9th token. Logit: 16.25 Prob:  3.72% Token: |1|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14.49</span><span style=\"font-weight: bold\"> Prob:  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.08</span><span style=\"font-weight: bold\">% Token: |</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m9\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m14.49\u001b[0m\u001b[1m Prob:  \u001b[0m\u001b[1;36m1.08\u001b[0m\u001b[1m% Token: |\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1m|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 18.39 Prob: 53.30% Token: |\n",
      "|\n",
      "Top 1th token. Logit: 16.77 Prob: 10.56% Token: |0|\n",
      "Top 2th token. Logit: 15.91 Prob:  4.46% Token: |\n",
      "\n",
      "|\n",
      "Top 3th token. Logit: 15.34 Prob:  2.53% Token: |8|\n",
      "Top 4th token. Logit: 15.11 Prob:  2.00% Token: | |\n",
      "Top 5th token. Logit: 14.98 Prob:  1.75% Token: |6|\n",
      "Top 6th token. Logit: 14.98 Prob:  1.75% Token: |\\n|\n",
      "Top 7th token. Logit: 14.78 Prob:  1.44% Token: |4|\n",
      "Top 8th token. Logit: 14.54 Prob:  1.14% Token: |3|\n",
      "Top 9th token. Logit: 14.49 Prob:  1.08% Token: |2|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' '</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">)</span>, <span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'3'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">)</span>, <span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'0'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">)</span>, <span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'0'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">)</span>, <span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'0'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">)</span>, <span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'8'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"font-weight: bold\">)</span>, <span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'0'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">)</span>, <span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'1'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span><span style=\"font-weight: bold\">)</span>, <span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'7'</span>, \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span><span style=\"font-weight: bold\">)</span>, <span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'6'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">)</span>, <span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'2'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' '\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m, \u001b[1m(\u001b[0m\u001b[32m'3'\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m, \u001b[1m(\u001b[0m\u001b[32m'0'\u001b[0m, \u001b[1;36m2\u001b[0m\u001b[1m)\u001b[0m, \u001b[1m(\u001b[0m\u001b[32m'0'\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m, \u001b[1m(\u001b[0m\u001b[32m'0'\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m, \u001b[1m(\u001b[0m\u001b[32m'8'\u001b[0m, \u001b[1;36m4\u001b[0m\u001b[1m)\u001b[0m, \u001b[1m(\u001b[0m\u001b[32m'0'\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m, \u001b[1m(\u001b[0m\u001b[32m'1'\u001b[0m, \u001b[1;36m5\u001b[0m\u001b[1m)\u001b[0m, \u001b[1m(\u001b[0m\u001b[32m'7'\u001b[0m, \n",
       "\u001b[1;36m7\u001b[0m\u001b[1m)\u001b[0m, \u001b[1m(\u001b[0m\u001b[32m'6'\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m, \u001b[1m(\u001b[0m\u001b[32m'2'\u001b[0m, \u001b[1;36m9\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "utils.test_prompt(problems[0], answers[0], tl_model, prepend_bos=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now want to find a reference prompt to run the model on. Even though our ultimate goal is to reverse engineer how this behaviour is done in general, often the best way to start out in mechanistic interpretability is by zooming in on a concrete example and understanding it in detail, and only _then_ zooming out and verifying that our analysis generalises.\n",
    "\n",
    "We'll run the model on 4 instances of this task, each prompt given twice - one with the correct answer, one with an incorrect answer (e.g., random perturbation such as a single digit change). To make our lives easier, we'll carefully choose prompts with single token answers and the corresponding answers in the same token positions.\n",
    "\n",
    "<details> <summary>(*) <b>Aside on tokenization</b></summary>\n",
    "\n",
    "We want models that can take in arbitrary text, but models need to have a fixed vocabulary. So the solution is to define a vocabulary of **tokens** and to deterministically break up arbitrary text into tokens. Tokens are, essentially, subwords, and are determined by finding the most frequent substrings - this means that tokens vary a lot in length and frequency!\n",
    "\n",
    "Tokens are a _massive_ headache and are one of the most annoying things about reverse engineering language models... Different names will be different numbers of tokens, different prompts will have the relevant tokens at different positions, different prompts will have different total numbers of tokens, etc. Language models often devote significant amounts of parameters in early layers to convert inputs from tokens to a more sensible internal format (and do the reverse in later layers). You really, really want to avoid needing to think about tokenization wherever possible when doing exploratory analysis (though, of course, it's relevant later when trying to flesh out your analysis and make it rigorous!). HookedTransformer comes with several helper methods to deal with tokens: `to_tokens, to_string, to_str_tokens, to_single_token, get_token_position`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['70620 x 99460 =',\n",
       "  '99460 x 70620 =',\n",
       "  '42399 x 65985 =',\n",
       "  '65985 x 42399 =',\n",
       "  '33706 x 18222 =',\n",
       "  '18222 x 33706 =',\n",
       "  '89222 x 89728 =',\n",
       "  '89728 x 89222 ='],\n",
       " [(' 7023865200', ' 238652007'),\n",
       "  (' 7023865200', ' 702386520'),\n",
       "  (' 2797698015', ' 125677899'),\n",
       "  (' 2797698015', ' 5279769801'),\n",
       "  (' 614190732', ' 503089621'),\n",
       "  (' 614190732', ' 725201843'),\n",
       "  (' 8005711616', ' 4002855808'),\n",
       "  (' 8005711616', ' 8005711617')],\n",
       " tensor([[[151643,    220,     22,     15,     17,     18,     23,     21,\n",
       "               20,     17,     15,     15],\n",
       "          [151643,    220,     17,     18,     23,     21,     20,     17,\n",
       "               15,     15,     22,      0]],\n",
       " \n",
       "         [[151643,    220,     22,     15,     17,     18,     23,     21,\n",
       "               20,     17,     15,     15],\n",
       "          [151643,    220,     22,     15,     17,     18,     23,     21,\n",
       "               20,     17,     15,      0]],\n",
       " \n",
       "         [[151643,    220,     17,     22,     24,     22,     21,     24,\n",
       "               23,     15,     16,     20],\n",
       "          [151643,    220,     16,     17,     20,     21,     22,     22,\n",
       "               23,     24,     24,      0]],\n",
       " \n",
       "         [[151643,    220,     17,     22,     24,     22,     21,     24,\n",
       "               23,     15,     16,     20],\n",
       "          [151643,    220,     20,     17,     22,     24,     22,     21,\n",
       "               24,     23,     15,     16]],\n",
       " \n",
       "         [[151643,    220,     21,     16,     19,     16,     24,     15,\n",
       "               22,     18,     17,      0],\n",
       "          [151643,    220,     20,     15,     18,     15,     23,     24,\n",
       "               21,     17,     16,      0]],\n",
       " \n",
       "         [[151643,    220,     21,     16,     19,     16,     24,     15,\n",
       "               22,     18,     17,      0],\n",
       "          [151643,    220,     22,     17,     20,     17,     15,     16,\n",
       "               23,     19,     18,      0]],\n",
       " \n",
       "         [[151643,    220,     23,     15,     15,     20,     22,     16,\n",
       "               16,     21,     16,     21],\n",
       "          [151643,    220,     19,     15,     15,     17,     23,     20,\n",
       "               20,     23,     15,     23]],\n",
       " \n",
       "         [[151643,    220,     23,     15,     15,     20,     22,     16,\n",
       "               16,     21,     16,     21],\n",
       "          [151643,    220,     23,     15,     15,     20,     22,     16,\n",
       "               16,     21,     16,     22]]], device='cuda:0'))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompts = []\n",
    "answers = []\n",
    "answer_tokens = []\n",
    "\n",
    "num_digits = 5\n",
    "num_examples = 4\n",
    "\n",
    "def generate_perturbations(correct_answer):\n",
    "    result = int(correct_answer.strip())\n",
    "    perturbations = [\n",
    "        result + 1,                      # Add 1\n",
    "        result - 1,                      # Subtract 1\n",
    "        result + 10,                     # Add 10\n",
    "        result - 10,                     # Subtract 10\n",
    "        result * 2,                      # Double\n",
    "        result // 2 if result > 1 else 1,  # Halve (with min of 1)\n",
    "        int(str(result)[::-1]) if len(str(result)) > 1 else result + 2,  # Reverse digits\n",
    "        int(''.join(sorted(str(result)))),  # Sort digits\n",
    "        int(str(result)[1:] + str(result)[0]) if len(str(result)) > 1 else result + 3,  # Rotate digits left\n",
    "        int(str(result)[-1] + str(result)[:-1]) if len(str(result)) > 1 else result + 4,  # Rotate digits right\n",
    "        int(''.join([str((int(d) + 1) % 10) for d in str(result)])),  # Add 1 to each digit\n",
    "        int(''.join([str((int(d) - 1) % 10) for d in str(result)])),  # Subtract 1 from each digit\n",
    "    ]\n",
    "    return perturbations\n",
    "\n",
    "max_len = 0\n",
    "for _ in range(num_examples):\n",
    "    num1 = np.random.randint(10 ** (num_digits - 1), 10**num_digits)\n",
    "    num2 = np.random.randint(10 ** (num_digits - 1), 10**num_digits)\n",
    "    multiplicands = (num1, num2)\n",
    "    for j in range(2):\n",
    "        prompts.append(f\"{multiplicands[j]} x {multiplicands[1 - j]} =\")\n",
    "        correct_answer = f\" {str(multiplicands[j] * multiplicands[1 - j])}\"\n",
    "        \n",
    "        perturbations = generate_perturbations(correct_answer)\n",
    "        incorrect_answer = f\" {str(np.random.choice(perturbations))}\"\n",
    "        \n",
    "        answers.append((correct_answer, incorrect_answer))\n",
    "        \n",
    "        correct_tokens = tl_model.to_tokens(answers[-1][0], prepend_bos=True).squeeze().tolist()\n",
    "        incorrect_tokens = tl_model.to_tokens(answers[-1][1], prepend_bos=True).squeeze().tolist()\n",
    "        \n",
    "        max_len = max(max_len, len(correct_tokens), len(incorrect_tokens))\n",
    "        correct_tokens = correct_tokens + [0] * (max_len - len(correct_tokens))\n",
    "        incorrect_tokens = incorrect_tokens + [0] * (max_len - len(incorrect_tokens))\n",
    "        \n",
    "        answer_tokens.append((correct_tokens, incorrect_tokens))\n",
    "\n",
    "answer_tokens = torch.tensor(answer_tokens).to(device)\n",
    "prompts, answers, answer_tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gotcha**: It's important that all of your prompts have the same number of tokens. If they're different lengths, then the position of the \"final\" logit where you can check logit difference will differ between prompts, and this will break the below code. The easiest solution is just to choose your prompts carefully to have the same number of tokens (you can eg add filler words like The, or newlines to start).\n",
    "\n",
    "There's a range of other ways of solving this, eg you can index more intelligently to get the final logit. A better way is to just use left padding by setting `model.tokenizer.padding_side = 'left'` before tokenizing the inputs and running the model; this way, you can use something like `logits[:, -1, :]` to easily access the final token outputs without complicated indexing. TransformerLens checks the value of `padding_side` of the tokenizer internally, and if the flag is set to be `'left'`, it adjusts the calculation of absolute position embedding and causal masking accordingly.\n",
    "\n",
    "In this demo, though, we stick to using the prompts of the same number of tokens because we want to show some visualisations aggregated along the batch dimension later in the demo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt length: 13\n",
      "Prompt as tokens: ['7', '0', '6', '2', '0', ' x', ' ', '9', '9', '4', '6', '0', ' =']\n",
      "Prompt length: 13\n",
      "Prompt as tokens: ['9', '9', '4', '6', '0', ' x', ' ', '7', '0', '6', '2', '0', ' =']\n",
      "Prompt length: 13\n",
      "Prompt as tokens: ['4', '2', '3', '9', '9', ' x', ' ', '6', '5', '9', '8', '5', ' =']\n",
      "Prompt length: 13\n",
      "Prompt as tokens: ['6', '5', '9', '8', '5', ' x', ' ', '4', '2', '3', '9', '9', ' =']\n",
      "Prompt length: 13\n",
      "Prompt as tokens: ['3', '3', '7', '0', '6', ' x', ' ', '1', '8', '2', '2', '2', ' =']\n",
      "Prompt length: 13\n",
      "Prompt as tokens: ['1', '8', '2', '2', '2', ' x', ' ', '3', '3', '7', '0', '6', ' =']\n",
      "Prompt length: 13\n",
      "Prompt as tokens: ['8', '9', '2', '2', '2', ' x', ' ', '8', '9', '7', '2', '8', ' =']\n",
      "Prompt length: 13\n",
      "Prompt as tokens: ['8', '9', '7', '2', '8', ' x', ' ', '8', '9', '2', '2', '2', ' =']\n"
     ]
    }
   ],
   "source": [
    "for prompt in prompts:\n",
    "    str_tokens = tl_model.to_str_tokens(prompt)\n",
    "    print(\"Prompt length:\", len(str_tokens))\n",
    "    print(\"Prompt as tokens:\", str_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cache layer activations\n",
    "\n",
    "The first basic operation when doing mechanistic interpretability is to break open the black box of the model and look at all of the internal activations of a model. This can be done with `logits, cache = model.run_with_cache(tokens)`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tl_model.to_tokens(prompts, prepend_bos=True)\n",
    "\n",
    "# Run the model and cache all activations\n",
    "original_logits, cache = tl_model.run_with_cache(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll later be evaluating how model performance differs upon performing various interventions, so it's useful to have a metric to measure model performance. Our metric here will be the **logit difference**, the difference in logit between the predicted and ground truth product.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per prompt logit difference: tensor([ 0.4090,  0.4020,  0.3710,  0.0000,  0.3600,  0.0820,  0.2270, -0.0210])\n",
      "Average logit difference: 0.229\n"
     ]
    }
   ],
   "source": [
    "def logits_to_ave_logit_diff(logits, answer_tokens, per_prompt=False):\n",
    "    # Only the final logits are relevant for the answer\n",
    "    final_logits = logits[:, -1, :]\n",
    "    # answer_tokens shape is [batch_size, 2, token_length]\n",
    "    # Calculate logit diff across all tokens in the answer\n",
    "    _, _, token_length = answer_tokens.shape\n",
    "    \n",
    "    all_token_logit_diffs = []\n",
    "    for i in range(token_length):\n",
    "        answer_tokens_at_pos = answer_tokens[:, :, i]\n",
    "        answer_logits = final_logits.gather(dim=-1, index=answer_tokens_at_pos)\n",
    "        token_logit_diff = answer_logits[:, 0] - answer_logits[:, 1]\n",
    "        all_token_logit_diffs.append(token_logit_diff)\n",
    "    stacked_diffs = torch.stack(all_token_logit_diffs, dim=1)\n",
    "    answer_logit_diff = stacked_diffs.mean(dim=1)\n",
    "    \n",
    "    if per_prompt:\n",
    "        return answer_logit_diff\n",
    "    else:\n",
    "        return answer_logit_diff.mean()\n",
    "\n",
    "\n",
    "print(\n",
    "    \"Per prompt logit difference:\",\n",
    "    logits_to_ave_logit_diff(original_logits, answer_tokens, per_prompt=True)\n",
    "    .detach()\n",
    "    .cpu()\n",
    "    .round(decimals=3),\n",
    ")\n",
    "original_average_logit_diff = logits_to_ave_logit_diff(original_logits, answer_tokens)\n",
    "print(\n",
    "    \"Average logit difference:\",\n",
    "    round(logits_to_ave_logit_diff(original_logits, answer_tokens).item(), 3),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the average logit difference is 0.229- for context, this represents putting an $e^{0.229}\\approx 1.257 \\times$ higher probability on the correct answer, which isn't a lot! Clearly, even though the model is able to do the task, it's is only able to do it with a small margin.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Direct Logit Attribution\n",
    "\n",
    "_Look up unfamiliar terms in the [mech interp explainer](https://neelnanda.io/glossary)_\n",
    "\n",
    "Further, the easiest part of the model to understand is the output - this is what the model is trained to optimize, and so it can always be directly interpreted! Often the right approach to reverse engineering a circuit is to start at the end, understand how the model produces the right answer, and to then work backwards. The main technique used to do this is called **direct logit attribution**\n",
    "\n",
    "**Background:** The central object of a transformer is the **residual stream**. This is the sum of the outputs of each layer and of the original token and positional embedding. Importantly, this means that any linear function of the residual stream can be perfectly decomposed into the contribution of each layer of the transformer. Further, each attention layer's output can be broken down into the sum of the output of each head (See [A Mathematical Framework for Transformer Circuits](https://transformer-circuits.pub/2021/framework/index.html) for details), and each MLP layer's output can be broken down into the sum of the output of each neuron (and a bias term for each layer).\n",
    "\n",
    "The logits of a model are `logits=Unembed(LayerNorm(final_residual_stream))`. The Unembed is a linear map, and LayerNorm is approximately a linear map, so we can decompose the logits into the sum of the contributions of each component, and look at which components contribute the most to the logit of the correct token! This is called **direct logit attribution**. Here we look at the direct attribution to the logit difference!\n",
    "\n",
    "<details> <summary>(*) <b>Background and motivation of the logit difference</b></summary>\n",
    "\n",
    "Logit difference is actually a _really_ nice and elegant metric. In general, there are two natural ways to interpret the model's outputs: the output logits, or the output log probabilities (or probabilities).\n",
    "\n",
    "The logits are much nicer and easier to understand, as noted above. However, the model is trained to optimize the cross-entropy loss (the average of log probability of the correct token). This means it does not directly optimize the logits, and indeed if the model adds an arbitrary constant to every logit, the log probabilities are unchanged.\n",
    "\n",
    "But `log_probs == logits.log_softmax(dim=-1) == logits - logsumexp(logits)`, and so `log_probs(\" 7\") - log_probs(\" 8\") = logits(\" 7\") - logits(\" 8\")` - the ability to add an arbitrary constant cancels out!\n",
    "\n",
    "Further, the metric helps us isolate the precise capability we care about - figuring out _which_ number is the product. There are many other components of the task - deciding what the task at hand is, etc. By taking the logit difference we control for all of that.\n",
    "\n",
    "Our metric is further refined, because each prompt is repeated twice, for each possible product. This controls for irrelevant behaviour such as the model learning that 7 is a more frequent token than 8.\n",
    "\n",
    "</details>\n",
    "\n",
    "<details> <summary>Ignoring LayerNorm</summary>\n",
    "\n",
    "LayerNorm is an analogous normalization technique to BatchNorm (that's friendlier to massive parallelization) that transformers use. Every time a transformer layer reads information from the residual stream, it applies a LayerNorm to normalize the vector at each position (translating to set the mean to 0 and scaling to set the variance to 1) and then applying a learned vector of weights and biases to scale and translate the normalized vector. This is _almost_ a linear map, apart from the scaling step, because that divides by the norm of the vector and the norm is not a linear function. (The `fold_ln` flag when loading a model factors out all the linear parts).\n",
    "\n",
    "But if we fixed the scale factor, the LayerNorm would be fully linear. And the scale of the residual stream is a global property that's a function of _all_ components of the stream, while in practice there is normally just a few directions relevant to any particular component, so in practice this is an acceptable approximation. So when doing direct logit attribution we use the `apply_ln` flag on the `cache` to apply the global layernorm scaling factor to each constant. See [this clean GPT-2 implementation](https://colab.research.google.com/github/TransformerLensOrg/TransformerLens/blob/clean-transformer-demo/Clean_Transformer_Demo.ipynb#scrollTo=Clean_Transformer_Implementation) for more on LayerNorm.\n",
    "\n",
    "</details>\n",
    "\n",
    "Getting an output logit is equivalent to projecting onto a direction in the residual stream. We use `model.tokens_to_residual_directions` to map the answer tokens to that direction, and then convert this to a logit difference direction for each batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer residual directions shape: torch.Size([8, 2, 12, 896])\n",
      "Logit difference directions shape: torch.Size([8, 12, 896])\n"
     ]
    }
   ],
   "source": [
    "answer_residual_directions = tl_model.tokens_to_residual_directions(answer_tokens)\n",
    "print(\"Answer residual directions shape:\", answer_residual_directions.shape)\n",
    "logit_diff_directions = (\n",
    "    answer_residual_directions[:, 0] - answer_residual_directions[:, 1]\n",
    ")\n",
    "print(\"Logit difference directions shape:\", logit_diff_directions.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final residual stream shape: torch.Size([8, 14, 896])\n",
      "Calculated average logit diff: 2.746\n",
      "Original logit difference: 0.229\n"
     ]
    }
   ],
   "source": [
    "# cache syntax - resid_post is the residual stream at the end of the layer, -1 gets the final layer. The general syntax is [activation_name, layer_index, sub_layer_type].\n",
    "final_residual_stream = cache[\"resid_post\", -1]\n",
    "print(\"Final residual stream shape:\", final_residual_stream.shape)\n",
    "final_token_residual_stream = final_residual_stream[:, -1, :]\n",
    "# Apply LayerNorm scaling\n",
    "# pos_slice is the subset of the positions we take - here the final token of each prompt\n",
    "scaled_final_token_residual_stream = cache.apply_ln_to_stack(\n",
    "    final_token_residual_stream, layer=-1, pos_slice=-1\n",
    ")\n",
    "\n",
    "# Fix the einsum to account for the extra dimension in logit_diff_directions\n",
    "average_logit_diff = einsum(\n",
    "    \"batch d_model, batch head d_model -> \",\n",
    "    scaled_final_token_residual_stream,\n",
    "    logit_diff_directions,\n",
    ") / len(prompts)\n",
    "print(\"Calculated average logit diff:\", round(average_logit_diff.item(), 3))\n",
    "print(\"Original logit difference:\", round(original_average_logit_diff.item(), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logit Lens\n",
    "\n",
    "We can now decompose the residual stream! First we apply a technique called the [**logit lens**](https://www.alignmentforum.org/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens) - this looks at the residual stream after each layer and calculates the logit difference from that. This simulates what happens if we delete all subsequence layers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_stack_to_logit_diff(\n",
    "    residual_stack: Float[torch.Tensor, \"components batch d_model\"],\n",
    "    cache: ActivationCache,\n",
    ") -> float:\n",
    "    scaled_residual_stack = cache.apply_ln_to_stack(\n",
    "        residual_stack, layer=-1, pos_slice=-1\n",
    "    )\n",
    "    return einsum(\n",
    "        \"... batch d_model, batch head d_model -> ...\",\n",
    "        scaled_residual_stack,\n",
    "        logit_diff_directions,\n",
    "    ) / len(prompts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, we see that the model essentially improves with each layer nearly exactly linearly.\n",
    "\n",
    "**Note:** Hover over each data point to see what residual stream position it's from!\n",
    "\n",
    "<details> <summary>Details on `accumulated_resid`</summary>\n",
    "**Key:** `n_pre` means the residual stream at the start of layer n, `n_mid` means the residual stream after the attention part of layer n (`n_post` is the same as `n+1_pre` so is not included)\n",
    "\n",
    "- `layer` is the layer for which we input the residual stream (this is used to identify _which_ layer norm scaling factor we want)\n",
    "- `incl_mid` is whether to include the residual stream in the middle of a layer, ie after attention & before MLP\n",
    "- `pos_slice` is the subset of the positions used. See `utils.Slice` for details on the syntax.\n",
    "- return_labels is whether to return the labels for each component returned (useful for plotting)\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        </script>\n",
       "        <script type=\"module\">import \"https://cdn.plot.ly/plotly-3.0.1.min\"</script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
       "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-3.0.1.min.js\"></script>                <div id=\"554646d8-b5e2-4973-8581-d23d7ca6a809\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                window.PLOTLYENV=window.PLOTLYENV || {};                                if (document.getElementById(\"554646d8-b5e2-4973-8581-d23d7ca6a809\")) {                    Plotly.newPlot(                        \"554646d8-b5e2-4973-8581-d23d7ca6a809\",                        [{\"hovertemplate\":\"\\u003cb\\u003e%{hovertext}\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e=%{x}\\u003cbr\\u003eindex=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"hovertext\":[\"0_pre\",\"0_mid\",\"1_pre\",\"1_mid\",\"2_pre\",\"2_mid\",\"3_pre\",\"3_mid\",\"4_pre\",\"4_mid\",\"5_pre\",\"5_mid\",\"6_pre\",\"6_mid\",\"7_pre\",\"7_mid\",\"8_pre\",\"8_mid\",\"9_pre\",\"9_mid\",\"10_pre\",\"10_mid\",\"11_pre\",\"11_mid\",\"12_pre\",\"12_mid\",\"13_pre\",\"13_mid\",\"14_pre\",\"14_mid\",\"15_pre\",\"15_mid\",\"16_pre\",\"16_mid\",\"17_pre\",\"17_mid\",\"18_pre\",\"18_mid\",\"19_pre\",\"19_mid\",\"20_pre\",\"20_mid\",\"21_pre\",\"21_mid\",\"22_pre\",\"22_mid\",\"23_pre\",\"23_mid\",\"final_post\"],\"legendgroup\":\"\",\"line\":{\"color\":\"#636efa\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"\",\"orientation\":\"h\",\"showlegend\":false,\"x\":{\"dtype\":\"f8\",\"bdata\":\"AAAAAAAAAAAAAAAAAADgPwAAAAAAAPA\\u002fAAAAAAAA+D8AAAAAAAAAQAAAAAAAAARAAAAAAAAACEAAAAAAAAAMQAAAAAAAABBAAAAAAAAAEkAAAAAAAAAUQAAAAAAAABZAAAAAAAAAGEAAAAAAAAAaQAAAAAAAABxAAAAAAAAAHkAAAAAAAAAgQAAAAAAAACFAAAAAAAAAIkAAAAAAAAAjQAAAAAAAACRAAAAAAAAAJUAAAAAAAAAmQAAAAAAAACdAAAAAAAAAKEAAAAAAAAApQAAAAAAAACpAAAAAAAAAK0AAAAAAAAAsQAAAAAAAAC1AAAAAAAAALkAAAAAAAAAvQAAAAAAAADBAAAAAAACAMEAAAAAAAAAxQAAAAAAAgDFAAAAAAAAAMkAAAAAAAIAyQAAAAAAAADNAAAAAAACAM0AAAAAAAAA0QAAAAAAAgDRAAAAAAAAANUAAAAAAAIA1QAAAAAAAADZAAAAAAACANkAAAAAAAAA3QAAAAAAAgDdAAAAAAAAAOEA=\"},\"xaxis\":\"x\",\"y\":{\"dtype\":\"i1\",\"bdata\":\"AAECAwQFBgcICQoLDA0ODxAREhMUFRYXGBkaGxwdHh8gISIjJCUmJygpKissLS4vMA==\"},\"yaxis\":\"y\",\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermap\":[{\"type\":\"scattermap\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"index\"}},\"legend\":{\"tracegroupgap\":0},\"title\":{\"text\":\"Logit Difference From Accumulate Residual Stream\"}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('554646d8-b5e2-4973-8581-d23d7ca6a809');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "accumulated_residual, labels = cache.accumulated_resid(\n",
    "    layer=-1, incl_mid=True, pos_slice=-1, return_labels=True\n",
    ")\n",
    "logit_lens_logit_diffs = residual_stack_to_logit_diff(accumulated_residual, cache)\n",
    "line(\n",
    "    logit_lens_logit_diffs,\n",
    "    x=np.arange(tl_model.cfg.n_layers * 2 + 1) / 2,\n",
    "    hover_name=labels,\n",
    "    title=\"Logit Difference From Accumulate Residual Stream\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer Attribution\n",
    "\n",
    "We can repeat the above analysis but for each layer (this is equivalent to the differences between adjacent residual streams)\n",
    "\n",
    "Note: Annoying terminology overload - layer k of a transformer means the kth **transformer block**, but each block consists of an **attention layer** (to move information around) _and_ an **MLP layer** (to process information).\n",
    "\n",
    "Interestingly, we see that while attn layer 23 matters, mostly it's the later MLP layers that matter! This gives some evidence that the model is passing around intermediate memorized values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
       "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-3.0.1.min.js\"></script>                <div id=\"c510f1ae-0afe-446e-93cc-4996da8b4f4d\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                window.PLOTLYENV=window.PLOTLYENV || {};                                if (document.getElementById(\"c510f1ae-0afe-446e-93cc-4996da8b4f4d\")) {                    Plotly.newPlot(                        \"c510f1ae-0afe-446e-93cc-4996da8b4f4d\",                        [{\"hovertemplate\":\"\\u003cb\\u003e%{hovertext}\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003evariable=0\\u003cbr\\u003eindex=%{x}\\u003cbr\\u003evalue=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"hovertext\":[\"embed\",\"0_attn_out\",\"0_mlp_out\",\"1_attn_out\",\"1_mlp_out\",\"2_attn_out\",\"2_mlp_out\",\"3_attn_out\",\"3_mlp_out\",\"4_attn_out\",\"4_mlp_out\",\"5_attn_out\",\"5_mlp_out\",\"6_attn_out\",\"6_mlp_out\",\"7_attn_out\",\"7_mlp_out\",\"8_attn_out\",\"8_mlp_out\",\"9_attn_out\",\"9_mlp_out\",\"10_attn_out\",\"10_mlp_out\",\"11_attn_out\",\"11_mlp_out\",\"12_attn_out\",\"12_mlp_out\",\"13_attn_out\",\"13_mlp_out\",\"14_attn_out\",\"14_mlp_out\",\"15_attn_out\",\"15_mlp_out\",\"16_attn_out\",\"16_mlp_out\",\"17_attn_out\",\"17_mlp_out\",\"18_attn_out\",\"18_mlp_out\",\"19_attn_out\",\"19_mlp_out\",\"20_attn_out\",\"20_mlp_out\",\"21_attn_out\",\"21_mlp_out\",\"22_attn_out\",\"22_mlp_out\",\"23_attn_out\",\"23_mlp_out\"],\"legendgroup\":\"0\",\"line\":{\"color\":\"#636efa\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"0\",\"orientation\":\"v\",\"showlegend\":true,\"x\":{\"dtype\":\"i1\",\"bdata\":\"AAECAwQFBgcICQoLDA0ODxAREhMUFRYXGBkaGxwdHh8gISIjJCUmJygpKissLS4vMA==\"},\"xaxis\":\"x\",\"y\":{\"dtype\":\"f4\",\"bdata\":\"95VZPRbywb3Br6U87EgLPVIQO74MPYs9XBQWPQKbj70p8MO+8NIWO9326Tq5fpq9l4uzPpPwh72216K+Ge6DPgDNuT4vh0y9PTm+vuNPE758r5s+RNltuywbuz3u4ZO8yGTBveirTb3shkU+KKlyvTcHXD62\\u002f3K9GmopvSpC1b1ITwG+bOG4O+jFwD67VBC+DX8KPzu42DyQ2MC+5vT9vT0kej1YZ\\u002fW7dJKzP9IVUz4ukae+USqXvq\\u002fxCL4\\u002f\\u002fVM\\u002f1yuDPw==\"},\"yaxis\":\"y\",\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermap\":[{\"type\":\"scattermap\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"index\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"value\"}},\"legend\":{\"title\":{\"text\":\"variable\"},\"tracegroupgap\":0},\"title\":{\"text\":\"Logit Difference From Each Layer\"}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('c510f1ae-0afe-446e-93cc-4996da8b4f4d');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "per_layer_residual, labels = cache.decompose_resid(\n",
    "    layer=-1, pos_slice=-1, return_labels=True\n",
    ")\n",
    "per_layer_logit_diffs = residual_stack_to_logit_diff(per_layer_residual, cache)\n",
    "line(per_layer_logit_diffs, hover_name=labels, title=\"Logit Difference From Each Layer\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Head Attribution\n",
    "\n",
    "We can further break down the output of each attention layer into the sum of the outputs of each attention head. Each attention layer consists of 14 heads, which each act independently and additively.\n",
    "\n",
    "<details> <summary>Decomposing attention output into sums of heads</summary> \n",
    "The standard way to compute the output of an attention layer is by concatenating the mixed values of each head, and multiplying by a big output weight matrix. But as described in [A Mathematical Framework](https://transformer-circuits.pub/2021/framework/index.html) this is equivalent to splitting the output weight matrix into a per-head output (here `model.blocks[k].attn.W_O`) and adding them up (including an overall bias term for the entire layer)\n",
    "</details>\n",
    "\n",
    "We see that only a few heads really matter - head L23H1 contributes a lot positively, while head L22H7 contributes a lot negatively. These correspond to (some of) the name movers and negative name movers discussed in the paper. There are also several heads that matter positively or negatively but less strongly.\n",
    "\n",
    "There are a few meta observations worth making here - our model has 24 layers \\* 14 heads/layer = 336 heads, yet we could localise this behaviour to a handful of specific heads, using straightforward, general techniques. This supports the claim in [A Mathematical Framework](https://transformer-circuits.pub/2021/framework/index.html) that attention heads are the right level of abstraction to understand attention. It also really surprising that there are _negative_ heads - eg L22H7 makes the incorrect logit e^-0.137 = 0.872x _more_ likely. I'm not sure what's going on there, though the paper discusses some possibilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
       "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-3.0.1.min.js\"></script>                <div id=\"a5408293-3b3f-4cbe-b318-db72a738eb96\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                window.PLOTLYENV=window.PLOTLYENV || {};                                if (document.getElementById(\"a5408293-3b3f-4cbe-b318-db72a738eb96\")) {                    Plotly.newPlot(                        \"a5408293-3b3f-4cbe-b318-db72a738eb96\",                        [{\"coloraxis\":\"coloraxis\",\"name\":\"0\",\"z\":{\"dtype\":\"f4\",\"bdata\":\"5ELru0E9D71Bte+75NaHu8UnBLvFRr66bAQjuyEspLk7vUw74lezuxZsqbrQwrC8I4IUuzJd8LuQ3gc7TSCoPDz52DuQE545mnh4PBhUzTqgpZK8GHNAuwpwOzxaZ9M8NJ4Du9gMGjyGt3G8u3SuvOFy0jyIjoy7nQauu5aq0rtypi48LU7TO\\u002fF0Ir2\\u002f0wI9LmE2PaF9FzwzwAm88Y9wPDpUkbwzYNk7HPK4u7RL1bs7\\u002ffu7b5Dfu46Hmru1Fgq8yCZRus6U3rumXzM89ug6vT5HXjzu1Zi7ZFHyu5dVKzyQbOc6ymXcve7AeT1YH0U9EN0sulBqKTzvIny9cYnlPAQSBboASX883FbgO1ih\\u002fToQUWy7mzYuO9Qqc7pfIg69aCtHO8piNbwUl1m9XEkSu2r5gT0cngg8l7GsPML+I70ekxK9xY99PEgyXLtcd7e7Yvo2PTqUR7wEoMU7zpKLvB5AX70zemS8UB2TvCtNwjwzYrs8VKq6veBV\\u002frouIKq8EoAjPOyrZD2WyEo8Tk8yu4P2Vz0mqjW8eGGBPbhvOD0IJ4465SQCPc8Whzw27AG8HqKJPRjv6Ls0yUI8\\u002fmh5vJLvtjy4R+u7rr4EPKxByz34BoS9BkqBvTekrbuEioe9SgY0PHpZUb0QCO+6YAEbPRkAiTwBxWU8izgGvNdOdz2MJ9W9F9UGvRXUTjxEHRq9pONRvONcYrwoR+m7zFYpO53R7Tty7Tm73KxMuxgGibs0i3a9Ug9OPTQKOr1t45C9QgKjPDXjm73SEps91Mi+vC49wruAnSy8IehlPMI87zyLwkE9ZodVPd5h8jwcYiG9ooPbO+y2K7vyGHG8TPWQO5CA7rpK3WE7IRz8u30t1Ttmmrc7hlZgvE6Nn7m1QcI7yUHwvGK8x7zIcvY8xb0APKgzSbtyv2K9BxQ7vSkNiLytPPs7jaGVPHAUGDvAUku8lwctPSfW5jylxqG7+1QQvLsBK71Unwk9J3nDvKwT1bzvJR+8Nmvfu+oDmTyKYPc7nIPPvHpwiDw\\u002fk4k71G3vO14En7zYfRu9kGArvZhyx7zonT088NSNuulIZrz6\\u002fRE+2iFXva+\\u002fijxYIZ+8Rl6OuyXet7yASeY7SCioOiKCqL28yZw8j579PIg0yjpYyAA8q0yWvDBfeL1Uq289bpCtPHmPE778BEg8vCZRu+fnUz0E5Ac7MMyFuZDrADtbj8O6oIMFOfplprsRQjo7tq+tuuezsDzewVQ7dC7wOxZHh7xCYNQ6240rvBzYK73FWEW8vufPvIDwK7uAiou7XxpTvBQgm7wYQxG8U1cvPON7J7xIsvk64BH8uixET7wwfe26EpXOPO0FRj2U\\u002fi+7QFRfu3gudjwhziI8eklQvZisRLzoNIq7V8F2vSza1zr6G0A9rPYSPBpdejtKiaC83I0XPPG\\u002fVDxzeVO8SpZyPIFpmDzryMG8p\\u002fNpPKe9rjyw+Mi6TPcuO\\u002fhEBTz1Z6a9cGqyvSX6hjxCdzo8ZlYVvCu7FDxqZTC9Bj89u5Ah5zvyW6c7EM+Qu1KfATwovIK7\\u002figZPeoHsbx4pYm81l2LPBkFJT2Kpyq8O7j+PIIiSLzwMX28PoPjPKqB0j0GUeU7oIMDvKNjTTtWrxc8QCrfuhS9fzyZD1292sxavXu4rDy7Axk9lpJ3Pey5Vb0OITK9Z6UMvu6TOrwa3QI9NnwWPbH797saCe+9oNXcu9F41j1xjXA+spU+PqD0Cj7fCL09ykLnPMoG5z3Szw29Gio6PUAUfr0gbH48deylPBy4vTvxy2m9\",\"shape\":\"24, 14\"},\"type\":\"heatmap\",\"xaxis\":\"x\",\"yaxis\":\"y\",\"hovertemplate\":\"Head: %{x}\\u003cbr\\u003eLayer: %{y}\\u003cbr\\u003ecolor: %{z}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermap\":[{\"type\":\"scattermap\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"scaleanchor\":\"y\",\"constrain\":\"domain\",\"title\":{\"text\":\"Head\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"autorange\":\"reversed\",\"constrain\":\"domain\",\"title\":{\"text\":\"Layer\"}},\"coloraxis\":{\"colorscale\":[[0.0,\"rgb(103,0,31)\"],[0.1,\"rgb(178,24,43)\"],[0.2,\"rgb(214,96,77)\"],[0.3,\"rgb(244,165,130)\"],[0.4,\"rgb(253,219,199)\"],[0.5,\"rgb(247,247,247)\"],[0.6,\"rgb(209,229,240)\"],[0.7,\"rgb(146,197,222)\"],[0.8,\"rgb(67,147,195)\"],[0.9,\"rgb(33,102,172)\"],[1.0,\"rgb(5,48,97)\"]],\"cmid\":0.0},\"title\":{\"text\":\"Logit Difference From Each Head\"}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('a5408293-3b3f-4cbe-b318-db72a738eb96');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "per_head_residual, labels = cache.stack_head_results(\n",
    "    layer=-1, pos_slice=-1, return_labels=True\n",
    ")\n",
    "per_head_logit_diffs = residual_stack_to_logit_diff(per_head_residual, cache)\n",
    "per_head_logit_diffs = einops.rearrange(\n",
    "    per_head_logit_diffs,\n",
    "    \"(layer head_index) -> layer head_index\",\n",
    "    layer=tl_model.cfg.n_layers,\n",
    "    head_index=tl_model.cfg.n_heads,\n",
    ")\n",
    "imshow(\n",
    "    per_head_logit_diffs,\n",
    "    labels={\"x\": \"Head\", \"y\": \"Layer\"},\n",
    "    title=\"Logit Difference From Each Head\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Analysis\n",
    "\n",
    "Attention heads are particularly easy to study because we can look directly at their attention patterns and study from what positions they move information from and two. This is particularly easy here as we're looking at the direct effect on the logits so we need only look at the attention patterns from the final token.\n",
    "\n",
    "We use Alan Cooney's circuitsvis library to visualize the attention patterns! We visualize the top 3 positive and negative heads by direct logit attribution, and show these for the first prompt (as an illustration).\n",
    "\n",
    "<details> <summary>Interpreting Attention Patterns</summary> \n",
    "An easy mistake to make when looking at attention patterns is thinking that they must convey information about the <i>token</i> looked at (maybe accounting for the context of the token). But actually, all we can confidently say is that it moves information from the *residual stream position* corresponding to that input token. Especially later on in the model, there may be components in the residual stream that are nothing to do with the input token! Eg the period at the end of a sentence may contain summary information for that sentence, and the head may solely move that, rather than caring about whether it ends in \".\", \"!\" or \"?\"\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_attention_patterns(\n",
    "    heads: list[int] | int | Float[torch.Tensor, \"heads\"],\n",
    "    local_cache: ActivationCache,\n",
    "    local_tokens: torch.Tensor,\n",
    "    title: str | None = \"\",\n",
    "    max_width: int | None = 700,\n",
    ") -> str:\n",
    "    # If a single head is given, convert to a list\n",
    "    if isinstance(heads, int):\n",
    "        heads = [heads]\n",
    "\n",
    "    # Create the plotting data\n",
    "    labels: list[str] = []\n",
    "    patterns: list[Float[torch.Tensor, \"dest_pos src_pos\"]] = []\n",
    "\n",
    "    # Assume we have a single batch item\n",
    "    batch_index = 0\n",
    "\n",
    "    for head in heads:\n",
    "        # Set the label\n",
    "        layer = head // tl_model.cfg.n_heads\n",
    "        head_index = head % tl_model.cfg.n_heads\n",
    "        labels.append(f\"L{layer}H{head_index}\")\n",
    "\n",
    "        # Get the attention patterns for the head\n",
    "        # Attention patterns have shape [batch, head_index, query_pos, key_pos]\n",
    "        patterns.append(local_cache[\"attn\", layer][batch_index, head_index])\n",
    "\n",
    "    # Convert the tokens to strings (for the axis labels)\n",
    "    str_tokens = tl_model.to_str_tokens(local_tokens)\n",
    "\n",
    "    # Combine the patterns into a single tensor\n",
    "    patterns: Float[torch.Tensor, \"head_index dest_pos src_pos\"] = torch.stack(\n",
    "        patterns, dim=0\n",
    "    )\n",
    "\n",
    "    # Circuitsvis Plot (note we get the code version so we can concatenate with the title)\n",
    "    plot = attention_heads(\n",
    "        attention=patterns, tokens=str_tokens, attention_head_names=labels\n",
    "    ).show_code()\n",
    "\n",
    "    # Display the title\n",
    "    title_html = f\"<h2>{title}</h2><br/>\"\n",
    "\n",
    "    # Return the visualisation as raw code\n",
    "    return f\"<div style='max-width: {str(max_width)}px;'>{title_html + plot}</div>\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspecting the patterns, we can see information from the residual stream position corresponding to the all tokens attending nearly exclusively to themselves. This gives some evidence that the model is primarily using self-attention to process information at each position. The top positive logit attribution heads show strong diagonal attention patterns, suggesting they're extracting or enhancing position-specific features. Meanwhile, the negative attribution heads appear to be attending to the first token, possibly to suppress or counteract certain information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style='max-width: 700px;'><h2>Top 3 Positive Logit Attribution Heads</h2><br/><div id=\"circuits-vis-eb756dd8-6f15\" style=\"margin: 15px 0;\"/>\n",
       "    <script crossorigin type=\"module\">\n",
       "    import { render, AttentionHeads } from \"https://unpkg.com/circuitsvis@1.43.3/dist/cdn/esm.js\";\n",
       "    render(\n",
       "      \"circuits-vis-eb756dd8-6f15\",\n",
       "      AttentionHeads,\n",
       "      {\"attention\": [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.00790675263851881, 0.9920932650566101, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.09892821311950684, 0.02239222824573517, 0.8786795735359192, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.09339933842420578, 3.2311501854564995e-05, 0.006655882112681866, 0.8999124765396118, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.010489042848348618, 1.0278242825734196e-06, 1.3909279914514627e-05, 0.02217191271483898, 0.9673240184783936, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0056166923604905605, 4.3139662011526525e-06, 1.6747184190535336e-06, 0.0008229924133047462, 0.02974802441895008, 0.9638062119483948, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0014998165424913168, 7.128823199309409e-05, 7.874736184021458e-06, 3.9019094401737675e-05, 6.200549978530034e-05, 0.0008428916335105896, 0.997477114200592, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.001571522792801261, 0.00021434463269542903, 0.00010879615729209036, 0.00010929678683169186, 1.7437560018152e-05, 4.154654379817657e-05, 0.002362573053687811, 0.9955744743347168, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [7.361862662946805e-05, 2.805731810440193e-06, 1.2214316484460142e-05, 4.0106991946231574e-05, 2.7010983103537e-06, 1.1915759046132735e-07, 9.138835821431712e-07, 0.00020241101447027177, 0.9996650218963623, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0005837501958012581, 5.955387223366415e-07, 5.161175977264065e-06, 0.00047195603838190436, 0.00016125754336826503, 2.0594472971424693e-06, 1.6381931118303328e-06, 3.4985652746399865e-05, 0.002544241026043892, 0.9961943626403809, 0.0, 0.0, 0.0, 0.0], [0.0006812610081396997, 2.1055676313608274e-07, 3.840787883291341e-07, 0.0002805389813147485, 0.0007583913393318653, 4.949465437675826e-05, 1.2803072422684636e-05, 4.064454515173566e-06, 8.434769824816613e-07, 0.0024564687628299, 0.9957555532455444, 0.0, 0.0, 0.0], [0.0009923073230311275, 4.056273155583767e-06, 2.7911781330658414e-07, 7.695916428929195e-05, 0.0004600376996677369, 0.000563005858566612, 0.0016537944320589304, 1.8631080820341595e-05, 1.6117354562084074e-07, 1.9215201973565854e-05, 0.006019265856593847, 0.9901922941207886, 0.0, 0.0], [0.0018592876149341464, 5.100570706417784e-05, 1.515141207164561e-06, 1.9532701116986573e-05, 3.9034141082083806e-05, 0.00014801757060922682, 0.003539581084623933, 0.0002592646051198244, 1.3266525229482795e-06, 5.939900233897788e-07, 8.50643300509546e-06, 0.005719394888728857, 0.9883529543876648, 0.0], [0.01576337404549122, 0.00020361111091915518, 0.0002460660762153566, 0.00028785053291358054, 4.534520849119872e-05, 6.991253758314997e-05, 0.00017806286632549018, 0.0140242213383317, 0.00011813058517873287, 7.177788120316109e-06, 4.8909482757153455e-06, 3.125763396383263e-05, 0.012089253403246403, 0.9569308757781982]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9932789206504822, 0.006721033714711666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9545086026191711, 0.012624169699847698, 0.03286729007959366, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8521600961685181, 0.017925968393683434, 0.030046194791793823, 0.09986770898103714, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7809157371520996, 0.011435260064899921, 0.03789394348859787, 0.10569991916418076, 0.06405514478683472, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7753922939300537, 0.009505664929747581, 0.032201725989580154, 0.0828794464468956, 0.05330827459692955, 0.04671269655227661, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6819407939910889, 0.014600157737731934, 0.03916594013571739, 0.09134082496166229, 0.06703730672597885, 0.05599984899163246, 0.04991514980792999, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6971176862716675, 0.03280986100435257, 0.041085317730903625, 0.06023920699954033, 0.028933105990290642, 0.03067207895219326, 0.0169206652790308, 0.09222211688756943, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6831606030464172, 0.013304447755217552, 0.02512306347489357, 0.07286694645881653, 0.03535524383187294, 0.03462478891015053, 0.021563300862908363, 0.10306762903928757, 0.010934034362435341, 0.0, 0.0, 0.0, 0.0, 0.0], [0.626663327217102, 0.011455211788415909, 0.018183644860982895, 0.06510195136070251, 0.032571859657764435, 0.028428923338651657, 0.04228588193655014, 0.11637309938669205, 0.017456723377108574, 0.04147934913635254, 0.0, 0.0, 0.0, 0.0], [0.6226011514663696, 0.00933137908577919, 0.01961475796997547, 0.07196197658777237, 0.03898113593459129, 0.033107589930295944, 0.03608216717839241, 0.1018824651837349, 0.011593371629714966, 0.035488296300172806, 0.019355757161974907, 0.0, 0.0, 0.0], [0.682179868221283, 0.008371316827833652, 0.01721758209168911, 0.05197839066386223, 0.030910518020391464, 0.03453477844595909, 0.03222261741757393, 0.07542897760868073, 0.008843895979225636, 0.025552093982696533, 0.015964239835739136, 0.01679583638906479, 0.0, 0.0], [0.6219140291213989, 0.0083660539239645, 0.015981391072273254, 0.052847541868686676, 0.0312703400850296, 0.02921123430132866, 0.05339955538511276, 0.07283223420381546, 0.008934533223509789, 0.022099396213889122, 0.010770205408334732, 0.015154466032981873, 0.05721898376941681, 0.0], [0.47555992007255554, 0.018658509477972984, 0.015459546819329262, 0.03491590917110443, 0.015509497374296188, 0.015614117495715618, 0.041066840291023254, 0.04563438892364502, 0.01919180527329445, 0.022557109594345093, 0.0061356988735497, 0.015207353048026562, 0.03253239765763283, 0.24195683002471924]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8375455737113953, 0.16245441138744354, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6938365697860718, 0.19501735270023346, 0.11114609986543655, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5209416151046753, 0.1752617061138153, 0.13012467324733734, 0.17367200553417206, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.44921785593032837, 0.11509723961353302, 0.11486907303333282, 0.12967650592327118, 0.1911393404006958, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.397388756275177, 0.06350723654031754, 0.06337281316518784, 0.11687777191400528, 0.17479312419891357, 0.18406029045581818, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.49468743801116943, 0.03571854904294014, 0.03419594094157219, 0.050279632210731506, 0.1311216950416565, 0.10245499759912491, 0.15154170989990234, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.30489203333854675, 0.022042516618967056, 0.023528793826699257, 0.04669453576207161, 0.132200688123703, 0.12937217950820923, 0.16102619469165802, 0.18024305999279022, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.16390161216259003, 0.008120405487716198, 0.01110683660954237, 0.0194139052182436, 0.07436051964759827, 0.11674011498689651, 0.16174159944057465, 0.3204028010368347, 0.12421223521232605, 0.0, 0.0, 0.0, 0.0, 0.0], [0.19071950018405914, 0.0071128071285784245, 0.009937092661857605, 0.016462327912449837, 0.04295692220330238, 0.04950664937496185, 0.13434574007987976, 0.2616943418979645, 0.1366514265537262, 0.1506132185459137, 0.0, 0.0, 0.0, 0.0], [0.13056045770645142, 0.00965011678636074, 0.014039112254977226, 0.02322607859969139, 0.05705089122056961, 0.05639489367604256, 0.11300242692232132, 0.22330041229724884, 0.10860852152109146, 0.12231454998254776, 0.1418525129556656, 0.0, 0.0, 0.0], [0.10008735954761505, 0.009480061940848827, 0.008896737359464169, 0.014245896600186825, 0.040567122399806976, 0.04169641435146332, 0.1161823496222496, 0.254657119512558, 0.09606031328439713, 0.11461570858955383, 0.13602732121944427, 0.06748363375663757, 0.0, 0.0], [0.110212042927742, 0.008654293604195118, 0.005213046912103891, 0.0063978685066103935, 0.020283017307519913, 0.018250752240419388, 0.11127747595310211, 0.25291502475738525, 0.08367151767015457, 0.10554088652133942, 0.12961778044700623, 0.06581699848175049, 0.08214931935071945, 0.0], [0.2779731750488281, 0.011909008957445621, 0.003774533746764064, 0.005794330034404993, 0.007699744310230017, 0.00718641746789217, 0.07565037161111832, 0.1124047040939331, 0.06320389360189438, 0.07616017013788223, 0.04915449768304825, 0.02428307943046093, 0.0471380278468132, 0.237667977809906]]], \"attentionHeadNames\": [\"L23H1\", \"L23H2\", \"L14H7\"], \"tokens\": [\"<|endoftext|>\", \"7\", \"0\", \"6\", \"2\", \"0\", \" x\", \" \", \"9\", \"9\", \"4\", \"6\", \"0\", \" =\"]}\n",
       "    )\n",
       "    </script></div><div style='max-width: 700px;'><h2>Top 3 Negative Logit Attribution Heads</h2><br/><div id=\"circuits-vis-c71c75d4-094c\" style=\"margin: 15px 0;\"/>\n",
       "    <script crossorigin type=\"module\">\n",
       "    import { render, AttentionHeads } from \"https://unpkg.com/circuitsvis@1.43.3/dist/cdn/esm.js\";\n",
       "    render(\n",
       "      \"circuits-vis-c71c75d4-094c\",\n",
       "      AttentionHeads,\n",
       "      {\"attention\": [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8871863484382629, 0.11281363666057587, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8822035193443298, 0.08455698192119598, 0.033239446580410004, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8163889646530151, 0.08077577501535416, 0.037475377321243286, 0.06535990536212921, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7656545042991638, 0.046949949115514755, 0.0401107594370842, 0.06601639837026596, 0.08126835525035858, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6543235182762146, 0.06789857149124146, 0.051697902381420135, 0.1095351129770279, 0.0791371762752533, 0.0374077707529068, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6414458155632019, 0.031132694333791733, 0.022817282006144524, 0.044114649295806885, 0.06259729713201523, 0.03574227914214134, 0.16215001046657562, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6057586669921875, 0.024831661954522133, 0.02114424668252468, 0.02996872551739216, 0.03933519124984741, 0.028379805386066437, 0.1671910583972931, 0.08339066058397293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4374633729457855, 0.014666644856333733, 0.014095219783484936, 0.019039908424019814, 0.04401355981826782, 0.023269128054380417, 0.22626562416553497, 0.14213615655899048, 0.07905037701129913, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5010299682617188, 0.02518542855978012, 0.015604003332555294, 0.01737586408853531, 0.020431337878108025, 0.014511731453239918, 0.16002145409584045, 0.10964661091566086, 0.07394517213106155, 0.06224847957491875, 0.0, 0.0, 0.0, 0.0], [0.5101750493049622, 0.02043505758047104, 0.020024167373776436, 0.02373197115957737, 0.03969660401344299, 0.017921477556228638, 0.12279917299747467, 0.08807671070098877, 0.042312610894441605, 0.056568801403045654, 0.05825834348797798, 0.0, 0.0, 0.0], [0.4345334768295288, 0.02194487862288952, 0.016011644154787064, 0.020718010142445564, 0.029185866937041283, 0.01659306325018406, 0.11955764144659042, 0.08607528358697891, 0.052849918603897095, 0.07295239716768265, 0.06318208575248718, 0.06639577448368073, 0.0, 0.0], [0.4985426366329193, 0.022377323359251022, 0.012696545571088791, 0.01972842775285244, 0.02362101338803768, 0.012298294343054295, 0.1274469494819641, 0.071955107152462, 0.02793206088244915, 0.04307788237929344, 0.03600586950778961, 0.041921935975551605, 0.062396034598350525, 0.0], [0.5390163660049438, 0.01667061261832714, 0.011413933709263802, 0.01813184656202793, 0.01315795723348856, 0.010570981539785862, 0.07101365178823471, 0.031173614785075188, 0.01533197145909071, 0.028525711968541145, 0.01909901574254036, 0.026963215321302414, 0.033275388181209564, 0.16565574705600739]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9801579117774963, 0.019842056557536125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8399078249931335, 0.028592221438884735, 0.13149987161159515, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5739988684654236, 0.03731977194547653, 0.10725894570350647, 0.2814224660396576, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5353171229362488, 0.023327788338065147, 0.06860768049955368, 0.20422698557376862, 0.16852037608623505, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5020427107810974, 0.03658520057797432, 0.057042285799980164, 0.1477741003036499, 0.12759587168693542, 0.12895981967449188, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4286271035671234, 0.13476498425006866, 0.08452346920967102, 0.1458914577960968, 0.08981676399707794, 0.06814157962799072, 0.04823462292551994, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.48741063475608826, 0.21811161935329437, 0.04070379585027695, 0.07042673230171204, 0.04881586134433746, 0.04668818414211273, 0.02664249576628208, 0.06120073050260544, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3785581588745117, 0.02074250392615795, 0.09463182091712952, 0.16818654537200928, 0.1083713099360466, 0.08383259922266006, 0.04187420383095741, 0.09423758834600449, 0.009565262123942375, 0.0, 0.0, 0.0, 0.0, 0.0], [0.36511361598968506, 0.01679588109254837, 0.07489944249391556, 0.16066600382328033, 0.10848027467727661, 0.09423933178186417, 0.043420154601335526, 0.07455413043498993, 0.0073877074755728245, 0.05444347485899925, 0.0, 0.0, 0.0, 0.0], [0.29111066460609436, 0.010843253694474697, 0.06639301031827927, 0.17851153016090393, 0.11142275482416153, 0.10266165435314178, 0.043466273695230484, 0.08356974273920059, 0.006533928215503693, 0.05411253124475479, 0.05137458071112633, 0.0, 0.0, 0.0], [0.3116808235645294, 0.00938916951417923, 0.048330385237932205, 0.14997485280036926, 0.11628349870443344, 0.11532124876976013, 0.03598194941878319, 0.06218825653195381, 0.0061105843633413315, 0.04677529260516167, 0.046571966260671616, 0.05139191821217537, 0.0, 0.0], [0.23587049543857574, 0.025944480672478676, 0.05365178734064102, 0.11849358677864075, 0.0996730849146843, 0.09905043244361877, 0.06942792236804962, 0.06447783857584, 0.015776250511407852, 0.05113682523369789, 0.04789099097251892, 0.04328426718711853, 0.07532205432653427, 0.0], [0.3312184512615204, 0.05468115210533142, 0.01623060181736946, 0.05785088986158371, 0.04466833174228668, 0.046086981892585754, 0.03908543661236763, 0.05500713363289833, 0.058827850967645645, 0.021351680159568787, 0.015774957835674286, 0.014574612490832806, 0.04849524423480034, 0.19614669680595398]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9723257422447205, 0.027674268931150436, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9014445543289185, 0.03487031161785126, 0.06368507444858551, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7299935817718506, 0.026866374537348747, 0.07387369126081467, 0.16926637291908264, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7037324905395508, 0.016585389152169228, 0.058825213462114334, 0.14493146538734436, 0.07592547684907913, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6840784549713135, 0.024424586445093155, 0.04489976912736893, 0.1151127815246582, 0.06536994129419327, 0.0661143958568573, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5951022505760193, 0.01996314339339733, 0.07919720560312271, 0.1568116694688797, 0.07447989284992218, 0.049423251301050186, 0.02502255141735077, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.760270893573761, 0.04952508956193924, 0.023046620190143585, 0.06683353334665298, 0.025654176250100136, 0.018999522551894188, 0.005699033383280039, 0.0499710813164711, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6382248401641846, 0.013745843432843685, 0.0501890666782856, 0.10485793650150299, 0.04843936860561371, 0.04519900679588318, 0.018585192039608955, 0.07988906651735306, 0.0008697671582922339, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6473325490951538, 0.009102785028517246, 0.04353995993733406, 0.11633449792861938, 0.05142318084836006, 0.049251046031713486, 0.01486507523804903, 0.0631403997540474, 0.00048588428762741387, 0.004524616524577141, 0.0, 0.0, 0.0, 0.0], [0.5858492255210876, 0.008467518724501133, 0.04329788684844971, 0.1323118656873703, 0.06183185055851936, 0.057379476726055145, 0.018433483317494392, 0.07668662071228027, 0.00048744792002253234, 0.004776978399604559, 0.010477649979293346, 0.0, 0.0, 0.0], [0.6532017588615417, 0.00606358190998435, 0.0322311706840992, 0.10455664992332458, 0.052841853350400925, 0.05357421189546585, 0.01317841187119484, 0.06227472797036171, 0.0003793534997384995, 0.003804268315434456, 0.00787417870014906, 0.010019829496741295, 0.0, 0.0], [0.6244244575500488, 0.008280117064714432, 0.03668256103992462, 0.09915254265069962, 0.054166458547115326, 0.056265152990818024, 0.01573161408305168, 0.05618046224117279, 0.000709265994373709, 0.004711901303380728, 0.006863398011773825, 0.008583740331232548, 0.028248393908143044, 0.0], [0.5536587834358215, 0.0051557086408138275, 0.012877027504146099, 0.056828517466783524, 0.026568371802568436, 0.02107449434697628, 0.0035565600264817476, 0.03431732952594757, 0.0010675175581127405, 0.0012176008895039558, 0.0015224040253087878, 0.0018646365497261286, 0.014332452788949013, 0.26595863699913025]]], \"attentionHeadNames\": [\"L15H10\", \"L22H7\", \"L22H12\"], \"tokens\": [\"<|endoftext|>\", \"7\", \"0\", \"6\", \"2\", \"0\", \" x\", \" \", \"9\", \"9\", \"4\", \"6\", \"0\", \" =\"]}\n",
       "    )\n",
       "    </script></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_k = 3\n",
    "\n",
    "top_positive_logit_attr_heads = torch.topk(\n",
    "    per_head_logit_diffs.flatten(), k=top_k\n",
    ").indices\n",
    "\n",
    "positive_html = visualize_attention_patterns(\n",
    "    top_positive_logit_attr_heads,\n",
    "    cache,\n",
    "    tokens[0],\n",
    "    f\"Top {top_k} Positive Logit Attribution Heads\",\n",
    ")\n",
    "\n",
    "top_negative_logit_attr_heads = torch.topk(\n",
    "    -per_head_logit_diffs.flatten(), k=top_k\n",
    ").indices\n",
    "\n",
    "negative_html = visualize_attention_patterns(\n",
    "    top_negative_logit_attr_heads,\n",
    "    cache,\n",
    "    tokens[0],\n",
    "    title=f\"Top {top_k} Negative Logit Attribution Heads\",\n",
    ")\n",
    "\n",
    "HTML(positive_html + negative_html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Patching\n",
    "\n",
    "The obvious limitation to the techniques used above is that they only look at the very end of the circuit - the parts that directly affect the logits. Clearly this is not sufficient to understand the circuit! We want to understand how things compose together to produce this final output, and ideally to produce an end-to-end circuit fully explaining this behaviour.\n",
    "\n",
    "The technique we'll use to investigate this is called **activation patching**. This was first introduced in [David Bau and Kevin Meng's excellent ROME paper](https://rome.baulab.info/), there called causal tracing.\n",
    "\n",
    "The setup of activation patching is to take two runs of the model on two different inputs, the clean run and the corrupted run. The clean run outputs the correct answer and the corrupted run does not. The key idea is that we give the model the corrupted input, but then **intervene** on a specific activation and **patch** in the corresponding activation from the clean run (ie replace the corrupted activation with the clean activation), and then continue the run. And we then measure how much the output has updated towards the correct answer.\n",
    "\n",
    "We can then iterate over many possible activations and look at how much they affect the corrupted run. If patching in an activation significantly increases the probability of the correct answer, this allows us to _localise_ which activations matter.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Residual stream\n",
    "\n",
    "One natural thing to patch in is the residual stream at a specific layer and specific position. For example, the model is likely initially doing some processing on the multiplicands' tokens to realise the task at hand, but then uses attention to move that information to the \" =\" token. So patching in the residual stream at the \" =\" token will likely matter a lot in later layers but not at all in early layers.\n",
    "\n",
    "We can zoom in much further and patch in specific activations from specific layers. For example, we think that the output of head L9H9 on the final token is significant for directly connecting to the logits\n",
    "\n",
    "We can patch in specific activations, and can zoom in as far as seems reasonable. For example, if we patch in the output of head L9H9 on the final token, we would predict that it will significantly affect performance.\n",
    "\n",
    "Note that this technique does _not_ tell us how the components of the circuit connect up, just what they are.\n",
    "\n",
    "<details> <summary>Technical details</summary> \n",
    "The choice of clean and corrupted prompt has both pros and cons. By carefully setting up the counterfactual, that <i>only</i> differs in the second subject, we avoid detecting the parts of the model doing irrelevant computation like detecting that the indirect object task is relevant at all or that it should be outputting a name rather than an article or pronoun. Or even context like that John and Mary are names at all.\n",
    "\n",
    "However, it _also_ bakes in some details that _are_ relevant to the task. Such as finding the location of the second subject, and of the names in the first clause. Or that the name mover heads have learned to copy whatever they look at.\n",
    "\n",
    "Some of these could be patched by also changing up the order of the names in the original sentence - patching in \"After <b>John and Mary</b> went to the store, John gave a bottle of milk to\" vs \"After <b>Mary and John</b> went to the store, John gave a bottle of milk to\".\n",
    "\n",
    "In the ROME paper they take a different tack. Rather than carefully setting up counterfactuals between two different but related inputs, they **corrupt** the clean input by adding Gaussian noise to the token embedding for the subject. This is in some ways much lower effort (you don't need to set up a similar but different prompt) but can also introduce some issues, such as ways this noise might break things. In practice, you should take care about how you choose your counterfactuals and try out several. Try to reason beforehand about what they will and will not tell you, and compare the results between different counterfactuals.\n",
    "\n",
    "</details>\n",
    "\n",
    "We first create a set of corrupted tokens - where we reverse the order of the prompts to have a guaranteed wrong answer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrupted Average Logit Diff 0.27\n",
      "Clean Average Logit Diff 0.23\n"
     ]
    }
   ],
   "source": [
    "corrupted_prompts = []\n",
    "for i in range(0, len(prompts)):\n",
    "    corrupted_prompts.append(prompts[len(prompts) - i - 1])\n",
    "corrupted_tokens = tl_model.to_tokens(corrupted_prompts, prepend_bos=True)\n",
    "corrupted_logits, corrupted_cache = tl_model.run_with_cache(\n",
    "    corrupted_tokens, return_type=\"logits\"\n",
    ")\n",
    "corrupted_average_logit_diff = logits_to_ave_logit_diff(corrupted_logits, answer_tokens)\n",
    "print(\"Corrupted Average Logit Diff\", round(corrupted_average_logit_diff.item(), 2))\n",
    "print(\"Clean Average Logit Diff\", round(original_average_logit_diff.item(), 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|endoftext|>89728 x 89222 =',\n",
       " '<|endoftext|>89222 x 89728 =',\n",
       " '<|endoftext|>18222 x 33706 =',\n",
       " '<|endoftext|>33706 x 18222 =',\n",
       " '<|endoftext|>65985 x 42399 =',\n",
       " '<|endoftext|>42399 x 65985 =',\n",
       " '<|endoftext|>99460 x 70620 =',\n",
       " '<|endoftext|>70620 x 99460 =']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tl_model.to_string(corrupted_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now intervene on the corrupted run and patch in the clean residual stream at a specific layer and position.\n",
    "\n",
    "We do the intervention using TransformerLens's `HookPoint` feature. We can design a hook function that takes in a specific activation and returns an edited copy, and temporarily add it in with `model.run_with_hooks`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patch_residual_component(\n",
    "    corrupted_residual_component: Float[torch.Tensor, \"batch pos d_model\"],\n",
    "    hook,\n",
    "    pos,\n",
    "    clean_cache,\n",
    "):\n",
    "    corrupted_residual_component[:, pos, :] = clean_cache[hook.name][:, pos, :]\n",
    "    return corrupted_residual_component\n",
    "\n",
    "\n",
    "def normalize_patched_logit_diff(patched_logit_diff):\n",
    "    # Subtract corrupted logit diff to measure the improvement, divide by the total improvement from clean to corrupted to normalise\n",
    "    # 0 means zero change, negative means actively made worse, 1 means totally recovered clean performance, >1 means actively *improved* on clean performance\n",
    "    return (patched_logit_diff - corrupted_average_logit_diff) / (\n",
    "        original_average_logit_diff - corrupted_average_logit_diff\n",
    "    )\n",
    "\n",
    "\n",
    "patched_residual_stream_diff = torch.zeros(\n",
    "    tl_model.cfg.n_layers, tokens.shape[1], device=device, dtype=torch.float32\n",
    ")\n",
    "for layer in range(tl_model.cfg.n_layers):\n",
    "    for position in range(tokens.shape[1]):\n",
    "        hook_fn = partial(patch_residual_component, pos=position, clean_cache=cache)\n",
    "        patched_logits = tl_model.run_with_hooks(\n",
    "            corrupted_tokens,\n",
    "            fwd_hooks=[(utils.get_act_name(\"resid_pre\", layer), hook_fn)],\n",
    "            return_type=\"logits\",\n",
    "        )\n",
    "        patched_logit_diff = logits_to_ave_logit_diff(patched_logits, answer_tokens)\n",
    "\n",
    "        patched_residual_stream_diff[layer, position] = normalize_patched_logit_diff(\n",
    "            patched_logit_diff\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can immediately see that all relevant computation happens on the final token. Moving the residual stream at the correct position near _exactly_ recovers performance!\n",
    "\n",
    "For reference, tokens and their index from the first prompt are on the x-axis. In an abuse of notation, note that the difference here is averaged over _all_ 8 prompts, while the labels only come from the _first_ prompt.\n",
    "\n",
    "To be easier to interpret, we normalise the logit difference, by subtracting the corrupted logit difference, and dividing by the total improvement from clean to corrupted to normalise\n",
    "0 means zero change, negative means actively made worse, 1 means totally recovered clean performance, >1 means actively _improved_ on clean performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
       "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-3.0.1.min.js\"></script>                <div id=\"073f3846-70ae-4bf3-9328-6ef5f2f234bc\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                window.PLOTLYENV=window.PLOTLYENV || {};                                if (document.getElementById(\"073f3846-70ae-4bf3-9328-6ef5f2f234bc\")) {                    Plotly.newPlot(                        \"073f3846-70ae-4bf3-9328-6ef5f2f234bc\",                        [{\"coloraxis\":\"coloraxis\",\"name\":\"0\",\"x\":[\"\\u003c|endoftext|\\u003e_0\",\"7_1\",\"0_2\",\"6_3\",\"2_4\",\"0_5\",\" x_6\",\" _7\",\"9_8\",\"9_9\",\"4_10\",\"6_11\",\"0_12\",\" =_13\"],\"z\":{\"dtype\":\"f4\",\"bdata\":\"AAAAgIKwPDxtDQi8aLy7PcV7Sz6fG5Y9AAAAgAAAAIBssac+dNDZvGkHAL3y0w09wGq+PQAAAIAAAACAxfLJPMnnVr10Sog9fXExPuBhuTy\\u002fEs49Q5SnO5tmij4EJOu8RpQdvRBtBj1ZiLc9qn\\u002fOvAAAAIDCwi48DsfDvdhDqz0JG\\u002fk94sOKPbY2aD2MxSU9R4+JPo4KUr195Ue9QHsZPIaW1D0CoCQ9AAAAgEtg1zsgEI292Df8vXidWb2fCRg+K7WoPQ6nUz1Yb+E9RpCRuyfLhLzyzru8wvIWPhIZVb0AAACAOxdMPLNVojx1GFK9LXwbvTTtj73Gb\\u002fs9LAJyPSWBBj6yh9I8+VZ\\u002fvN0rh7xdKBs+15LCvQAAAIDqVrK9duFPPVs96btpJCQ9fhvCvVmAmjw2bk09WkzePV\\u002fyFrwyLz28p86tuT5SEz46yoI9AAAAgHZcvr2r0ek7+0WAPAgyEb3\\u002fy8e9nwlVPYWKsDxSipQ9dzd3vIpU3zxYOKI8ozs5PuHbQD4AAACAZKS1vaibNz0bIpo8S6UJvVL8JL67TCY9Q091u8sZgz3OyB29zE70PNb6Lzxu1Bs+g4W0PgAAAIAGw9C6MXRuO3++dTua1Oy8KVbgvXrBE74rlvi8w7YAPhYrDLxJRb47obvAutYKqD1o4Qo\\u002fAAAAgH5eqzw\\u002fBwi8uGB2PYwD\\u002f7tYzei9\\u002fUTyvYC0Uj3V1K09kCasvSSZXT3K8DQ8pZ6NPRNNDD8AAACAV0zoOzoPtLyOxaA8ElcvPOnwz7258\\u002fS99C6HOwq+7z068Ma9dKRAPYzct7wSRjQ94e0qPwAAAIBolK65F7KevPzdEjyZZS08KuaYvewK4b32mG+9KD3NPU975r25zGo9u9\\u002fmu+mX1ztE2zc\\u002fAAAAgLURLLyMZVU8Z5uLPH1gNjwCc4i9QBnDve24fb1OcIg+0IPhvQyraT3Z\\u002f\\u002fa6tPYSOzerMz8AAACAM+j\\u002fvKcTXzw4HKg7ypgCvBaKFr3o6r29m0Z4vZAVoz74bNG9Lz68PBPDKDzrxfE8Xk44PwAAAIBphz+9WHiCPBxR+TsTpPi7SyaPvBHBp71iHaO9BrenPsRjmr0AL948hHyButb2ozw6zi8\\u002fAAAAgGZSzbxbnz+7rQDKOkuuZrv4McO7RpOavaloQbzCqcU+\\u002fznHvO\\u002fAFjyMK426gtxVvQLODT8AAACAigsivaeJ+zu9o5O78uwlvFJeuLzYp4e9HfiBuj9v0z57UAa7EU+Xul7puDqB7pu8kwX\\u002fPgAAAIBzvp68gLRSvJZpQ7x+8eu7ldiNu7qGc72Jk4Q7jATTPvK69DuvQeW7fRsFOwLOw7y6yAg\\u002fAAAAgNZxErywGVi8lGO2vEUvAbxNDrK7KzXovJau9LpYpMo+0pUdOYPWPrwV5x+78okKvUIGCD8AAACA5c8pvAz+SryAb6G8cwNQvB6AWbvsCd680ikkvO2Mwz4iXZQ73guSu9+nsLuhCpa8IZz+PgAAAIBEflW7+fNjvAtOZbyzWGi8yuaRumvKYLwQB5+8NMvIPv1vDryo\\u002f5i7bW\\u002fdO6DkE7yuRAc\\u002fAAAAgLmpNDt1vBO7eoxZvOdIjbwgLrS7DXLcu0zoL7wdBKs+hPKdO4A0k7vGitw6FHLIvLJPMj8AAACAnAifPK3GgTuNKQI6KNLWu\\u002fXKJbuE\\u002fYY6gZRdvAkKoD4zQXg732wivEBnUjsLT6u8vDUvPwAAAIAzaYa6\\u002fLP5OrIVwroW0U28d4cSvCt1yDmPkiq8PXH\\u002fvIKAF7s97G06SJVYu3BDurs4Q4c\\u002f\",\"shape\":\"24, 14\"},\"type\":\"heatmap\",\"xaxis\":\"x\",\"yaxis\":\"y\",\"hovertemplate\":\"Position: %{x}\\u003cbr\\u003eLayer: %{y}\\u003cbr\\u003ecolor: %{z}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermap\":[{\"type\":\"scattermap\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"scaleanchor\":\"y\",\"constrain\":\"domain\",\"title\":{\"text\":\"Position\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"autorange\":\"reversed\",\"constrain\":\"domain\",\"title\":{\"text\":\"Layer\"}},\"coloraxis\":{\"colorscale\":[[0.0,\"rgb(103,0,31)\"],[0.1,\"rgb(178,24,43)\"],[0.2,\"rgb(214,96,77)\"],[0.3,\"rgb(244,165,130)\"],[0.4,\"rgb(253,219,199)\"],[0.5,\"rgb(247,247,247)\"],[0.6,\"rgb(209,229,240)\"],[0.7,\"rgb(146,197,222)\"],[0.8,\"rgb(67,147,195)\"],[0.9,\"rgb(33,102,172)\"],[1.0,\"rgb(5,48,97)\"]],\"cmid\":0.0},\"title\":{\"text\":\"Logit Difference From Patched Residual Stream\"}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('073f3846-70ae-4bf3-9328-6ef5f2f234bc');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt_position_labels = [\n",
    "    f\"{tok}_{i}\" for i, tok in enumerate(tl_model.to_str_tokens(tokens[0]))\n",
    "]\n",
    "imshow(\n",
    "    patched_residual_stream_diff,\n",
    "    x=prompt_position_labels,\n",
    "    title=\"Logit Difference From Patched Residual Stream\",\n",
    "    labels={\"x\": \"Position\", \"y\": \"Layer\"},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layers\n",
    "\n",
    "We can apply exactly the same idea, but this time patching in attention or MLP layers. These are also residual components with identical shapes to the residual stream terms, so we can reuse the same hooks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "patched_attn_diff = torch.zeros(\n",
    "    tl_model.cfg.n_layers, tokens.shape[1], device=device, dtype=torch.float32\n",
    ")\n",
    "patched_mlp_diff = torch.zeros(\n",
    "    tl_model.cfg.n_layers, tokens.shape[1], device=device, dtype=torch.float32\n",
    ")\n",
    "for layer in range(tl_model.cfg.n_layers):\n",
    "    for position in range(tokens.shape[1]):\n",
    "        hook_fn = partial(patch_residual_component, pos=position, clean_cache=cache)\n",
    "        patched_attn_logits = tl_model.run_with_hooks(\n",
    "            corrupted_tokens,\n",
    "            fwd_hooks=[(utils.get_act_name(\"attn_out\", layer), hook_fn)],\n",
    "            return_type=\"logits\",\n",
    "        )\n",
    "        patched_attn_logit_diff = logits_to_ave_logit_diff(\n",
    "            patched_attn_logits, answer_tokens\n",
    "        )\n",
    "        patched_mlp_logits = tl_model.run_with_hooks(\n",
    "            corrupted_tokens,\n",
    "            fwd_hooks=[(utils.get_act_name(\"mlp_out\", layer), hook_fn)],\n",
    "            return_type=\"logits\",\n",
    "        )\n",
    "        patched_mlp_logit_diff = logits_to_ave_logit_diff(\n",
    "            patched_mlp_logits, answer_tokens\n",
    "        )\n",
    "\n",
    "        patched_attn_diff[layer, position] = normalize_patched_logit_diff(\n",
    "            patched_attn_logit_diff\n",
    "        )\n",
    "        patched_mlp_diff[layer, position] = normalize_patched_logit_diff(\n",
    "            patched_mlp_logit_diff\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that several attention layers are significant but that, matching the residual stream results, early layers slightly matter on the multiplicands, but mostly everything matters on the final token. Extremely localised! As with direct logit attribution, layer 22 is positive and layer 3 is not, suggesting that the early layers only matter for direct logit effects.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
       "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-3.0.1.min.js\"></script>                <div id=\"b32182f7-7153-46c5-86b6-c0ebf40a0f4e\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                window.PLOTLYENV=window.PLOTLYENV || {};                                if (document.getElementById(\"b32182f7-7153-46c5-86b6-c0ebf40a0f4e\")) {                    Plotly.newPlot(                        \"b32182f7-7153-46c5-86b6-c0ebf40a0f4e\",                        [{\"coloraxis\":\"coloraxis\",\"name\":\"0\",\"x\":[\"\\u003c|endoftext|\\u003e_0\",\"7_1\",\"0_2\",\"6_3\",\"2_4\",\"0_5\",\" x_6\",\" _7\",\"9_8\",\"9_9\",\"4_10\",\"6_11\",\"0_12\",\" =_13\"],\"z\":{\"dtype\":\"f4\",\"bdata\":\"AAAAgIkl\\u002fzzwpvW9t9yqPFs3Uj1ALMQ8vxLOPUOUpzu3Sek6YwmaPEG\\u002fBbxJ2gq8fiJXOqp\\u002fzrwAAACAg4dpO0CGAjyTKrM7Nawnvd0Wejtg8Iu8Zb8RPRh\\u002fqDtXnAO8M0F4vAZDETyPfmM6+bRJPQAAAIChsyM9UfmbPITemT0U0RW+eE9KPnVYsjoWeFU8WoSmvDuXjDnqOEg9Wg4KvO6GTT0uSN+9AAAAgBUI0Lz1NVk85VkNPQoLPzvkjIO+S0MzvDr7bDyVP7U8Vn+hPXUyMDxnV588rBQWvRlKLL0AAACAWFG6vOUUW7wW5RQ734qMO9960byRyue7b2MvvTuXjDt0AcW7d\\u002f2uPJlWuDx1Njw94fAKPgAAAIB6vcQ7iujlOtbmaLqDoAK95\\u002f6JPCKgOj3kIAq9OieHPIw1ML2ehQ66pzvsPG9YxjxL\\u002fHU9AAAAgLulHrs\\u002f6Nc6IuXrumkeErwcguQ7xaZAPUv+gTtWCcK72SY\\u002fvJFxbzw0+rs8yiDaO4Nf3D0AAACAhb+nO2oRnryhHRc7Y83Fu8OO8ruKYA47DtCmvBxInLuESxY9C6gjvIw06ju8Qn2945GXPgAAAIBXh3Y8risevTesojz1hK68FMp6vJbf37xt2hY9D78mvEMei7uWzN48uYEnvMzP+bzNLYK9AAAAgNKVnbumHki7FeefO8kFwbtCeY67cEM6O8IPeLshGSg9Cg4LPf53pjwTVum7GvyXuhU\\u002flT0AAACAFIKDPCz0Qju9Io66vu8XvGwPE7uLLZi8KHneut48fLwVcYM8y9xoPUcssLtNyX+7SyqbPQAAAIB3OYO7aDLYuvQuB7rcQBO7H0QGu05wCLsHQsu6zId8vZUxBr2uG2O7gTvlurVLdLsvb6e8AAAAgEMKxLtfD7u7w1SqOkSwBjxWYrq7ZjTjO9ni0roYMRm75sN6O2wsN7yaT1s8WUWMPNByZr0AAACAZ+RIu2\\u002fhYztBSKO7EG7GumJknTq6pyk7pvd\\u002fvImk+by98aK709jDu41GpruuCGK8fXO3PAAAAIDbwRg7zDvzO27jbrv0hXM7w7YAO0RhMbz5\\u002fsw6Ps2+uVlwX7u19Ac5lq50u9qDQ71uhmq9AAAAgNUtpropn+A7+3s8uawCVbvqlcy7GRvHO2Saz7odFSa8ZDh5O3zsJLrJtzG7jCsNvGubAb0AAACAn6AnusA7nDq\\u002f9mm72v1rOuQpZzpGfEq7fOE7PNFbVDz4yps8SRTTu9RgnLvO\\u002fRS7N22IvAAAAIAjR0K7HDTVOGDnrTrvNjO4ddm3uek1Art60Qs7tDBbuxoFdbsIXeS65iVRunF797ogTJ48AAAAgIU1xLnUTFW7YoFBOv6x7rk5N0G7Ps2+OW6pJror4Hs8iBQKvONTgDr7mGC6b2JpO2rCyLwAAACAgdAxOzuXDLkAdJA5MKfkONfHObrQZj27WoQmu4Uhfbtzqlc7NzvXOy5xMrnrgQY7T8avPAAAAIBolK67t9wqu+IOTrqpSx07TUj6uY1Gpjr1SSA7BbpyvFIGhjoo+uM6uQuLuQbD0Dr8k489AAAAgKexiTohLKm5h1BdOpNbHjl60Yu5IBGQusJz2bmwmNK7I2RmulQhn7k\\u002f6Ne60PuJO6M9gjwAAACArcYBuCh53je9Io645gittq0AyjmW1oI6C9jIOKnpRjrOfho4etELuX0HPrnxbOW5bxO2PgAAAIAAAACAAAAAgAAAAIAAAACAAAAAgAAAAIAAAACAAAAAgAAAAIAAAACAAAAAgAAAAIBBeCy+\",\"shape\":\"24, 14\"},\"type\":\"heatmap\",\"xaxis\":\"x\",\"yaxis\":\"y\",\"hovertemplate\":\"Position: %{x}\\u003cbr\\u003eLayer: %{y}\\u003cbr\\u003ecolor: %{z}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermap\":[{\"type\":\"scattermap\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"scaleanchor\":\"y\",\"constrain\":\"domain\",\"title\":{\"text\":\"Position\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"autorange\":\"reversed\",\"constrain\":\"domain\",\"title\":{\"text\":\"Layer\"}},\"coloraxis\":{\"colorscale\":[[0.0,\"rgb(103,0,31)\"],[0.1,\"rgb(178,24,43)\"],[0.2,\"rgb(214,96,77)\"],[0.3,\"rgb(244,165,130)\"],[0.4,\"rgb(253,219,199)\"],[0.5,\"rgb(247,247,247)\"],[0.6,\"rgb(209,229,240)\"],[0.7,\"rgb(146,197,222)\"],[0.8,\"rgb(67,147,195)\"],[0.9,\"rgb(33,102,172)\"],[1.0,\"rgb(5,48,97)\"]],\"cmid\":0.0},\"title\":{\"text\":\"Logit Difference From Patched Attention Layer\"}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('b32182f7-7153-46c5-86b6-c0ebf40a0f4e');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "imshow(\n",
    "    patched_attn_diff,\n",
    "    x=prompt_position_labels,\n",
    "    title=\"Logit Difference From Patched Attention Layer\",\n",
    "    labels={\"x\": \"Position\", \"y\": \"Layer\"},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And as before, the MLP layer 7 does matter, which aligns with the idea that the model is simply recalling parts of the answer.\n",
    "\n",
    "An interesting case is MLP 0, which also matters, but this is usually misleading and just a generally true statement about MLP 0 rather than being about the circuit on this task.\n",
    "\n",
    "<details> <summary>Takes on MLP0</summary> \n",
    "It's often observed on GPT-2 XL that MLP0 matters a lot, and that ablating it utterly destroys performance. A current best guess is that the first MLP layer is essentially acting as an extension of the embedding (for whatever reason) and that when later layers want to access the input tokens they mostly read in the output of the first MLP layer, rather than the token embeddings. Within this frame, the first attention layer doesn't do much.\n",
    "\n",
    "In this framing, it makes sense that MLP0 matters on the second subject token, because that's the one position with a different input token!\n",
    "\n",
    "It's not entirely known why this happens, but a guess is that the embedding and unembedding matrices in GPT-2 XL are the same. This is pretty unprincipled, as the tasks of embedding and unembedding tokens are <i>not</i> inverses, but this is common practice, and plausibly models want to dedicate some parameters to overcoming this.\n",
    "\n",
    "There's only suggestive evidence of this though.\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
       "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-3.0.1.min.js\"></script>                <div id=\"0b4e45e5-4448-4cc3-a9ef-26c5c2165731\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                window.PLOTLYENV=window.PLOTLYENV || {};                                if (document.getElementById(\"0b4e45e5-4448-4cc3-a9ef-26c5c2165731\")) {                    Plotly.newPlot(                        \"0b4e45e5-4448-4cc3-a9ef-26c5c2165731\",                        [{\"coloraxis\":\"coloraxis\",\"name\":\"0\",\"x\":[\"\\u003c|endoftext|\\u003e_0\",\"7_1\",\"0_2\",\"6_3\",\"2_4\",\"0_5\",\" x_6\",\" _7\",\"9_8\",\"9_9\",\"4_10\",\"6_11\",\"0_12\",\" =_13\"],\"z\":{\"dtype\":\"f4\",\"bdata\":\"AAAAgB1t2Dx08xW9S8xQPZfXQj70fJY8qN\\u002flPWn2hDsztYE+Z5kFvXiPKr2c9Nc8f3OyPb\\u002fai7wAAACA4ehLPJ3BYby7Yzi9Ry42vVIbEj0uDpe9LUswPCtZZD03I4W8+ycWvWt8UTzT+jk9+R49PQAAAIB+eIk9XUpOvbT\\u002f7718OW69JITRvXf6Yj3O2lg96mRhvBGKpT2OSrK8039Lu736\\u002fzv5zWG9AAAAgGAX0zzNm4G9HyiiPa+ONL4CK0i+tL0KPrFxiz2aCqo8xOOcPZ4YTzy8hm68yo8DPrfIYzsAAACAvMH8vEEioTw+f687PD1OvTVvk703qBY825xWvYgTRLwh53Y7ydjhvBWskTz6rrK7tqEhPQAAAIA\\u002fTLk6nOVivfTfMb0OiCm97qPxPGIAvDoO9ii9Z6oAvIhgE708bjk9wiO\\u002fu+2dXz2+MsM9AAAAgJT8Dr0ojaU8CF4qPJw1eL0VmRA8eYP7OyC41L100Fm8pWyfvcAKsbuAUTc8wiO\\u002fO8wvDT4AAACAA6rCO1pnAru2lLI8x9OavVNm0Lx3WjO9dD2ZO6ElNL3MrL68W9uTO46vUz0lp8o9+kvnPgAAAIBrrbw8FwsXPKaBYzy8uB89tKwPPdeboL1Ei8Q97vkePQ04lDsAQNk9m\\u002f9APfcKgT0Zahw9AAAAgMa7xzzmp5w8wImrPMJLzDvXUZ28m5B7vXjyRTqvriS97lIXPVDdPL24Lwy9lDupvBuIAb0AAACA6lFgPYEewbr9PV08NVEpvNlPEr3bsiO9Dm2LPOR1cL3lYuq8TfUZPeUAFb25Rpm8uNJEPgAAAIAb00Q8BvS7O7Uu0DopbnU8VuoXvYDbmrvc5xo9PXbRvJoBTLwkVCw9wiSFPHeHkjs+TV2+AAAAgM+PELy++PQ6Q0YYuzTJULya2T47AnKFvHDYBrwf2Ay9xeQavWLjlzs8AkA8EIKNPJ3D570AAACAwCvhvICL\\u002f7vg9b+7m\\u002f\\u002fAu9NFAzxs\\u002fhe9UtMUvff3eb0iM3u7IyoeO4FalbvkIIo8RKZiPAAAAIBT+lY8iyKvuzLqi7rHxCW8Tcn\\u002fu6BtMbwcSBy8G0CEPY1jSrzKPX47IhZXvB9hKr2PVhk+AAAAgBUYi7ugHly8+EUKOVs0jLvIB8w7TgynOkdSMjyTw4u8Cx16PIG8ajvelXS7d9Uhu4mhcD4AAACAv\\u002fJdPX2QWzyQQpA7IyoevK3iXzx3TIS8PPwoPa1qPb1OA0m8e1AGO05kWTxl3rs88BR1PgAAAICOEzA83qk7PFOFgDt31aG6Ghm8OiwIirsAnJ07wnflPL4MvDvA4qO7otbZufnEhDoQtD0+AAAAgFULzbzFUBQ7Cx+GO4KxArt67q87UgYGO0RsGrz7ycu73I6iu3gjMTvI6qe7ovP9unZuPD4AAACAdjpIPLoJALtt0bO7Rt4gO4hOUrvzCIU7bm3Su8engTwgS1g4XE+luo9Emzk5VOW668iAPgAAAIANuZm61qwguYzvuDs\\u002fro84vg0CPHUVjLvEb8O7x\\u002fTKPE0OMrqnbFc7kV0puk4pyzpQzsc9AAAAgC224zubbIA7v3fvufkbcbttKqw6YGaoOFKHC7sUI3O7ct3NOlU8uDko+uM67xmPuu2pZ74AAACAOdXqOrXM+TrDjvK54vGpupiNOjrsV20761Abu9xdN7w7GBK7ZH0rORp9nbrd+VU5izyNvQAAAIAAAACAAAAAgAAAAIAAAACAAAAAgAAAAIAAAACAAAAAgAAAAIAAAACAAAAAgAAAAIDdFvq6\",\"shape\":\"24, 14\"},\"type\":\"heatmap\",\"xaxis\":\"x\",\"yaxis\":\"y\",\"hovertemplate\":\"Position: %{x}\\u003cbr\\u003eLayer: %{y}\\u003cbr\\u003ecolor: %{z}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermap\":[{\"type\":\"scattermap\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"scaleanchor\":\"y\",\"constrain\":\"domain\",\"title\":{\"text\":\"Position\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"autorange\":\"reversed\",\"constrain\":\"domain\",\"title\":{\"text\":\"Layer\"}},\"coloraxis\":{\"colorscale\":[[0.0,\"rgb(103,0,31)\"],[0.1,\"rgb(178,24,43)\"],[0.2,\"rgb(214,96,77)\"],[0.3,\"rgb(244,165,130)\"],[0.4,\"rgb(253,219,199)\"],[0.5,\"rgb(247,247,247)\"],[0.6,\"rgb(209,229,240)\"],[0.7,\"rgb(146,197,222)\"],[0.8,\"rgb(67,147,195)\"],[0.9,\"rgb(33,102,172)\"],[1.0,\"rgb(5,48,97)\"]],\"cmid\":0.0},\"title\":{\"text\":\"Logit Difference From Patched MLP Layer\"}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('0b4e45e5-4448-4cc3-a9ef-26c5c2165731');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "imshow(\n",
    "    patched_mlp_diff,\n",
    "    x=prompt_position_labels,\n",
    "    title=\"Logit Difference From Patched MLP Layer\",\n",
    "    labels={\"x\": \"Position\", \"y\": \"Layer\"},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heads\n",
    "\n",
    "We can refine the above analysis by patching in individual heads! This is somewhat more annoying, because there are now three dimensions (head_index, position and layer), so for now lets patch in a head's output across all positions.\n",
    "\n",
    "The easiest way to do this is to patch in the activation `z`, the \"mixed value\" of the attention head. That is, the average of all previous values weighted by the attention pattern, ie the activation that is then multiplied by `W_O`, the output weights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patch_head_vector(\n",
    "    corrupted_head_vector: Float[torch.Tensor, \"batch pos head_index d_head\"],\n",
    "    hook,\n",
    "    head_index,\n",
    "    clean_cache,\n",
    "):\n",
    "    corrupted_head_vector[:, :, head_index, :] = clean_cache[hook.name][\n",
    "        :, :, head_index, :\n",
    "    ]\n",
    "    return corrupted_head_vector\n",
    "\n",
    "\n",
    "patched_head_z_diff = torch.zeros(\n",
    "    tl_model.cfg.n_layers, tl_model.cfg.n_heads, device=device, dtype=torch.float32\n",
    ")\n",
    "for layer in range(tl_model.cfg.n_layers):\n",
    "    for head_index in range(tl_model.cfg.n_heads):\n",
    "        hook_fn = partial(patch_head_vector, head_index=head_index, clean_cache=cache)\n",
    "        patched_logits = tl_model.run_with_hooks(\n",
    "            corrupted_tokens,\n",
    "            fwd_hooks=[(utils.get_act_name(\"z\", layer, \"attn\"), hook_fn)],\n",
    "            return_type=\"logits\",\n",
    "        )\n",
    "        patched_logit_diff = logits_to_ave_logit_diff(patched_logits, answer_tokens)\n",
    "\n",
    "        patched_head_z_diff[layer, head_index] = normalize_patched_logit_diff(\n",
    "            patched_logit_diff\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now see that, in addition to the heads identified before, heads L0H11 and L22H7 matter and are presumably responsible for attending to the final token.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
       "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-3.0.1.min.js\"></script>                <div id=\"f9fed39f-fdf3-4c98-bc6c-f97bf96dcf7c\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                window.PLOTLYENV=window.PLOTLYENV || {};                                if (document.getElementById(\"f9fed39f-fdf3-4c98-bc6c-f97bf96dcf7c\")) {                    Plotly.newPlot(                        \"f9fed39f-fdf3-4c98-bc6c-f97bf96dcf7c\",                        [{\"coloraxis\":\"coloraxis\",\"name\":\"0\",\"z\":{\"dtype\":\"f4\",\"bdata\":\"ICKLPZ6WCb0dH8k8eheDPHyl5zp8Jm25tOcdvYrLwTqukwu8T8uBvDEKe72hbb8+UBkRveW74r0LngC6oHtgPbXDnDx0JIE8zx3\\u002fvDoEy7xg8Iu8+wGUvHPS5DyJ+eW84Zo8PHSWkbvbX8K62MvFPPPXGbwkvBm88IxaPM5+GruP+VE9hBqrOWs3oLsbZUU9I9SuPYQ7nj0+zT68Vk85PYuKHD2rtYu9MqVZu6kzSj0cvji8ACYBPJknUz0dT246a628u+JURbyO9gs7eZdCPROnxD3cz0e9VAgHvvbR9732Eh49U1PPPPppgb3TRQO8ugCiPGULGz1rbqI9SV+cPaMuDbwJ8CW82morPZLco7uuGd29luClvcqYgjv+ygc9g34Mu1\\u002fBqzwtp+490SEMPDkGVrzhGvw84QAkvRe7e7wMsYG8Fu5xOwdf7zuElZm8jNKUO3khJjsd0HM7MDKOPNGlVz0Z1pW83dIOPV8daj34uSA9T6d\\u002fvaOahrqEHre8qwGVPQZ+nzxjr9u8DHTnPPaDaDwOpU29hE4fPrlBxz2T+II8MMXOPA25GbvXDbG8aUFIPXDWer3wFwS73PtgO3EQxDwlm148RUtfvBh2yrxEJV082oQJvenwT7znOFK9T3ggPYXEeL2vbD49ZlLNPEH6k73p1vE8Sfeuuh\\u002fDALs1PKI9hTYKPUc\\u002fMTyb17M8vr6suwfBxTX8llU6xVCUt5rthTzSMoK8N4CJOro7MD0mEwI9qDAEvN\\u002fPPT3fOzc8kHzYPbig1rwsjlu91hogvT1U2zylPXe8lydYPckEezzbBYo91LbIvelhmzxvWQy8slu5vMNJQTyvm6O8z9P7u1M1ZbxJMXe2l\\u002fwEvD+uD7oWzUG9HDRVuGrSA70HwcU1lCquvTYeszvNdbw8SWJiOzT6O7ywmZg829twPcYx5LytHW67zvmIPGtCibyf2u87tkHRvPRoT7va4Me7laZcvOttP7tN9Rk9KzsAOz3sbTpABfy6yxVxPBk4azrkgl+8MPQzvZejDLyDzJs7+0rRvM\\u002fAejyA+wo9N8U6vDALRjyAXia9vK02vE1dh7xCyKC9+nRkPd351buch5m6QQNxO2zxqLwh8Zq9A6pCO2hft7x1BVG9YU8WPdPdlTyB0DG76JnivAqkF71NZuQ8cMkRPduDPr0jeK27V7knOzNakTzB\\u002fgI81qwgty6\\u002fwTuXrGk6c74euhFKxbzCD\\u002fi6FmYaO2NiEryNY0q5DUHxO9d45Dy9efo6euqjvOu7Tj3woCE8lhs0PBSfJz2kkuO8Ep0mvHEVFr0YHdK4XkKxuyH7vTs9a+i6EnDHvAD1lTogBqe7Ne\\u002fSO2\\u002fPKLw1PWK8gBYpPDneSLz5Lzg7KKFrvJ3yTLsc29w7XbA1vBO4vzuqtlA820t7u4hiGTuXciG6JYmjvIsiLzxdulg8g8ybO0CGAjwa6JM90fi4PLlid7oB30M8xQIFvNRsRb3e90q8H\\u002fTqvJRoCL1vk9Q7owyXvPoG5TzeMx+8thqKO2O5\\u002frrdgTM9Zp+cveRqyr2kU8m6hP0GOzxNRj4jUKA7Y7n+ur937znUJY49EhSJvLwZMDyDmzA7AOlmvFQDtbw4pou8AiMwPMe5vLthjKq7g1b+unRuBDseP7Q8SpPNO8UQtLy2QVE872WYvUDBkDyFmF89EaDCPuZsDrvAIoQ89+1WPGmXt702hiA91Gn5O6zlMDyMLM29tSrEvNVVs7z6aYG7i2fgOmsuwjxVH5Q6KZaDPAX3jDwwp+Q55DzovM3Rer3z9L07\",\"shape\":\"24, 14\"},\"type\":\"heatmap\",\"xaxis\":\"x\",\"yaxis\":\"y\",\"hovertemplate\":\"Head: %{x}\\u003cbr\\u003eLayer: %{y}\\u003cbr\\u003ecolor: %{z}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermap\":[{\"type\":\"scattermap\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"scaleanchor\":\"y\",\"constrain\":\"domain\",\"title\":{\"text\":\"Head\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"autorange\":\"reversed\",\"constrain\":\"domain\",\"title\":{\"text\":\"Layer\"}},\"coloraxis\":{\"colorscale\":[[0.0,\"rgb(103,0,31)\"],[0.1,\"rgb(178,24,43)\"],[0.2,\"rgb(214,96,77)\"],[0.3,\"rgb(244,165,130)\"],[0.4,\"rgb(253,219,199)\"],[0.5,\"rgb(247,247,247)\"],[0.6,\"rgb(209,229,240)\"],[0.7,\"rgb(146,197,222)\"],[0.8,\"rgb(67,147,195)\"],[0.9,\"rgb(33,102,172)\"],[1.0,\"rgb(5,48,97)\"]],\"cmid\":0.0},\"title\":{\"text\":\"Logit Difference From Patched Head Output\"}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('f9fed39f-fdf3-4c98-bc6c-f97bf96dcf7c');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "imshow(\n",
    "    patched_head_z_diff,\n",
    "    title=\"Logit Difference From Patched Head Output\",\n",
    "    labels={\"x\": \"Head\", \"y\": \"Layer\"},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decomposing Heads\n",
    "\n",
    "Decomposing attention layers into patching in individual heads has already helped us localise the behaviour a lot. But we can understand it further by decomposing heads. An attention head consists of two semi-independent operations - calculating _where_ to move information from and to (represented by the attention pattern and implemented via the QK-circuit) and calculating _what_ information to move (represented by the value vectors and implemented by the OV circuit). We can disentangle which of these is important by patching in just the attention pattern _or_ the value vectors. (See [A Mathematical Framework](https://transformer-circuits.pub/2021/framework/index.html) or [a walkthrough video](https://www.youtube.com/watch?v=KV5gbOmHbjU) for more on this decomposition. If you're not familiar with the details of how attention is implemented, I recommend checking out [my clean transformer implementation](https://colab.research.google.com/github/TransformerLensOrg/TransformerLens/blob/clean-transformer-demo/Clean_Transformer_Demo.ipynb#scrollTo=3Pb0NYbZ900e) to see how the code works)\n",
    "\n",
    "First let's patch in the value vectors, to measure when figuring out what to move is important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patched_head_v_diff = torch.zeros(\n",
    "    tl_model.cfg.n_layers, tl_model.cfg.n_heads, device=device, dtype=torch.float32\n",
    ")\n",
    "\n",
    "def patch_head_vector(\n",
    "    corrupted_head_vector: Float[torch.Tensor, \"batch head_index pos d_head\"],\n",
    "    hook,\n",
    "    head_index,\n",
    "    clean_cache,\n",
    "):\n",
    "    corrupted_head_vector[:, head_index, : , :] = clean_cache[hook.name][\n",
    "        :, head_index, :, :\n",
    "    ]\n",
    "    return corrupted_head_vector\n",
    "\n",
    "\n",
    "for layer in range(tl_model.cfg.n_layers):\n",
    "    for head_index in range(tl_model.cfg.n_heads):\n",
    "        hook_fn = partial(patch_head_vector, head_index=head_index, clean_cache=cache)\n",
    "        patched_logits = tl_model.run_with_hooks(\n",
    "            corrupted_tokens,\n",
    "            fwd_hooks=[(utils.get_act_name(\"v\", layer, \"attn\"), hook_fn)],\n",
    "            return_type=\"logits\",\n",
    "        )\n",
    "        patched_logit_diff = logits_to_ave_logit_diff(patched_logits, answer_tokens)\n",
    "\n",
    "        patched_head_v_diff[layer, head_index] = normalize_patched_logit_diff(\n",
    "            patched_logit_diff\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot this as a heatmap and it's initially hard to interpret.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
       "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-3.0.1.min.js\"></script>                <div id=\"bdafd6f9-2dd5-48a5-b677-9085c937682e\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                window.PLOTLYENV=window.PLOTLYENV || {};                                if (document.getElementById(\"bdafd6f9-2dd5-48a5-b677-9085c937682e\")) {                    Plotly.newPlot(                        \"bdafd6f9-2dd5-48a5-b677-9085c937682e\",                        [{\"coloraxis\":\"coloraxis\",\"name\":\"0\",\"z\":{\"dtype\":\"f4\",\"bdata\":\"AAAAgHw5Mb5xiq89b3dwPWyODT3UxLo9AAAAgAAAAIC355M8qXJkvPz6tjo+zHg8aJ+XvAAAAIAAAACAPCS2PFYFtrwKCz88EAzwuo5sKL3TKyU91S2mun4i17tFhu08007gOu9TVzzawyM71BKNugAAAIDcXTc9HYp8vI6UNTx2mdI91a4rO+YmF71l\\u002fKU7VzENPqEbkT3oosC864spPfq4Vbx\\u002fhC27AAAAgLNyxrxMqZW9BrZhvQdJI717m8k98d71vJSw\\u002fzsgqNw8YN0KPVnVBjy3elQ7\\u002f5I\\u002fuN6MlzsAAACA\\u002fTJ0u\\u002f\\u002fA3r3g9Pm8mAPXvFgLw7zPS6Q9x80DO8Cd8Tw4zVI8mmMiPFq0yzyev1Y9Q4DgugAAAIB8V1g7\\u002fLP5PFKWAL1fKFM9V9ULve1VYjn16I881PIcPR\\u002fYDLy+c+O8ozgwuxstg7u2VFK8AAAAgFHMvLtAhHa82JTDu3Fe07qbc1c9daxYPdhzkzzoHvQ8uyXePAmD5jtTFLU8mxECPVmZMr0AAACAbSqsvW2+Mj2YXE88hzyXPIITWL0iCk8+p4CePC+cBr0GdcG8tWlePCUmiDrQGK67M8YKPQAAAICBs427\\u002f8MqPE50lLxvnfe86tyJOxJmJL3XKxu8AU4JPcI8Vz2HlEm8Wu\\u002fZOi9SA7t7FDK9AAAAgIagdzxSBoa6FvnaO3nTFryz2e27DrcOuos7xzyMWzI8uVA8PKLgfDwxZ4U8qpksPTJbVr0AAACAwny3PJtXc7zMgKW6dOSgu5Y42LzODIq8pE89PSSeLzy5Cws9WofsPOuZ2Lx3VCE9uDepvQAAAIAgEZA6EmIYvOe4kjvqH7C4citdu20qrLt\\u002f3aU7FRiLu7ulnjrUEo06UCfAusOO8riu6vc7AAAAgOZ+ybtN+uq7Vzqtugv2sjx+rYC8ovUJvBRLgbr+3k29JsImu4+58Tzfp7A7sBlYOrkYdL0AAACAyvA0PBRopTpToqS7d2jiO9R9QLxsXSK8fbkuPHjAFb3FeKG8ywyUu90NnTtaocq7SjrVPAAAAIDOQ4y8+f+SPIyCeTvT6v45Kqi+u5Ih1bwTX0e92TvLvF8IY72Z3I88gR5BO32uRTxuLXI9AAAAgDZ3KzykGDu8Zvqaux+6orvFUJS6S6WJO\\u002f6USryFj4K8diaCvFgQFTzKod+6cMZFvQ5elrwAAACA3HuhvLSJ0zsBy\\u002fw65gitOFCAOLw4\\u002fwO7xVAUNgbgdLt8pi28cqOFOb6hiDq4PoG7X4VXOwAAAIBT3bK888PSuxYqRjz3ZDm7fgWzOjw3N70MCbQ7JIWXPHqCNjzVrqs7\\u002f5I\\u002fOv9h1Dvcwx49AAAAgCvXHjv4Yi67midOu6MbjDtALYo6pKFYu\\u002fV7UDwxnge7hg03vGn\\u002f4btk\\u002fjC7A8dmu0OA4DoAAACAragXvJuJpLmYjbq7STF3usTwSLuAlmi8BfbGO+nD8DxqV5U81qwgOVmY7Luoktm7tBoPvQAAAICLMaQ832tcvP91G7sbmDY7szvEO8U9E7zadZS7Ik\\u002ffvegjxrxWp2s7dVgyO6LWWTwdFSY5AAAAgG3leTvio5o8nlNdvBAM8LtEkpy7\\u002fh4uvE0OsroMiC47Nb7nuo1GpjuITlK7vPyLu4tr7DwAAACAVYkHPfNqWjs\\u002f6Fc6NgGPO7wZsLsnwJs6odhku8kCdT4lf4C7hltGvKWW7zsMVv281BtqPAAAAIDvta27CF3kOp\\u002fa77oYHVK8hDfPuwhUh7tKTVa8BL3DvKLWWTmJk4Q6otZZucpnF7t+x9i9\",\"shape\":\"24, 14\"},\"type\":\"heatmap\",\"xaxis\":\"x\",\"yaxis\":\"y\",\"hovertemplate\":\"Head: %{x}\\u003cbr\\u003eLayer: %{y}\\u003cbr\\u003ecolor: %{z}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermap\":[{\"type\":\"scattermap\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"scaleanchor\":\"y\",\"constrain\":\"domain\",\"title\":{\"text\":\"Head\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"autorange\":\"reversed\",\"constrain\":\"domain\",\"title\":{\"text\":\"Layer\"}},\"coloraxis\":{\"colorscale\":[[0.0,\"rgb(103,0,31)\"],[0.1,\"rgb(178,24,43)\"],[0.2,\"rgb(214,96,77)\"],[0.3,\"rgb(244,165,130)\"],[0.4,\"rgb(253,219,199)\"],[0.5,\"rgb(247,247,247)\"],[0.6,\"rgb(209,229,240)\"],[0.7,\"rgb(146,197,222)\"],[0.8,\"rgb(67,147,195)\"],[0.9,\"rgb(33,102,172)\"],[1.0,\"rgb(5,48,97)\"]],\"cmid\":0.0},\"title\":{\"text\":\"Logit Difference From Patched Head Value\"}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('bdafd6f9-2dd5-48a5-b677-9085c937682e');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "imshow(\n",
    "    patched_head_v_diff,\n",
    "    title=\"Logit Difference From Patched Head Value\",\n",
    "    labels={\"x\": \"Head\", \"y\": \"Layer\"},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But it's very easy to interpret if we plot a scatter plot against patching head outputs. Here we see that the late heads matter most.\n",
    "\n",
    "Meta lesson: Plot things early, often and in diverse ways as you explore a model's internals!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
       "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-3.0.1.min.js\"></script>                <div id=\"fef3b727-4f28-4ecc-84b7-9473969966d8\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                window.PLOTLYENV=window.PLOTLYENV || {};                                if (document.getElementById(\"fef3b727-4f28-4ecc-84b7-9473969966d8\")) {                    Plotly.newPlot(                        \"fef3b727-4f28-4ecc-84b7-9473969966d8\",                        [{\"hovertemplate\":\"\\u003cb\\u003e%{hovertext}\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003eValue Patch=%{x}\\u003cbr\\u003eOutput Patch=%{y}\\u003cbr\\u003eLayer=%{marker.color}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"hovertext\":[\"L0H0\",\"L0H1\",\"L0H2\",\"L0H3\",\"L0H4\",\"L0H5\",\"L0H6\",\"L0H7\",\"L0H8\",\"L0H9\",\"L0H10\",\"L0H11\",\"L0H12\",\"L0H13\",\"L1H0\",\"L1H1\",\"L1H2\",\"L1H3\",\"L1H4\",\"L1H5\",\"L1H6\",\"L1H7\",\"L1H8\",\"L1H9\",\"L1H10\",\"L1H11\",\"L1H12\",\"L1H13\",\"L2H0\",\"L2H1\",\"L2H2\",\"L2H3\",\"L2H4\",\"L2H5\",\"L2H6\",\"L2H7\",\"L2H8\",\"L2H9\",\"L2H10\",\"L2H11\",\"L2H12\",\"L2H13\",\"L3H0\",\"L3H1\",\"L3H2\",\"L3H3\",\"L3H4\",\"L3H5\",\"L3H6\",\"L3H7\",\"L3H8\",\"L3H9\",\"L3H10\",\"L3H11\",\"L3H12\",\"L3H13\",\"L4H0\",\"L4H1\",\"L4H2\",\"L4H3\",\"L4H4\",\"L4H5\",\"L4H6\",\"L4H7\",\"L4H8\",\"L4H9\",\"L4H10\",\"L4H11\",\"L4H12\",\"L4H13\",\"L5H0\",\"L5H1\",\"L5H2\",\"L5H3\",\"L5H4\",\"L5H5\",\"L5H6\",\"L5H7\",\"L5H8\",\"L5H9\",\"L5H10\",\"L5H11\",\"L5H12\",\"L5H13\",\"L6H0\",\"L6H1\",\"L6H2\",\"L6H3\",\"L6H4\",\"L6H5\",\"L6H6\",\"L6H7\",\"L6H8\",\"L6H9\",\"L6H10\",\"L6H11\",\"L6H12\",\"L6H13\",\"L7H0\",\"L7H1\",\"L7H2\",\"L7H3\",\"L7H4\",\"L7H5\",\"L7H6\",\"L7H7\",\"L7H8\",\"L7H9\",\"L7H10\",\"L7H11\",\"L7H12\",\"L7H13\",\"L8H0\",\"L8H1\",\"L8H2\",\"L8H3\",\"L8H4\",\"L8H5\",\"L8H6\",\"L8H7\",\"L8H8\",\"L8H9\",\"L8H10\",\"L8H11\",\"L8H12\",\"L8H13\",\"L9H0\",\"L9H1\",\"L9H2\",\"L9H3\",\"L9H4\",\"L9H5\",\"L9H6\",\"L9H7\",\"L9H8\",\"L9H9\",\"L9H10\",\"L9H11\",\"L9H12\",\"L9H13\",\"L10H0\",\"L10H1\",\"L10H2\",\"L10H3\",\"L10H4\",\"L10H5\",\"L10H6\",\"L10H7\",\"L10H8\",\"L10H9\",\"L10H10\",\"L10H11\",\"L10H12\",\"L10H13\",\"L11H0\",\"L11H1\",\"L11H2\",\"L11H3\",\"L11H4\",\"L11H5\",\"L11H6\",\"L11H7\",\"L11H8\",\"L11H9\",\"L11H10\",\"L11H11\",\"L11H12\",\"L11H13\",\"L12H0\",\"L12H1\",\"L12H2\",\"L12H3\",\"L12H4\",\"L12H5\",\"L12H6\",\"L12H7\",\"L12H8\",\"L12H9\",\"L12H10\",\"L12H11\",\"L12H12\",\"L12H13\",\"L13H0\",\"L13H1\",\"L13H2\",\"L13H3\",\"L13H4\",\"L13H5\",\"L13H6\",\"L13H7\",\"L13H8\",\"L13H9\",\"L13H10\",\"L13H11\",\"L13H12\",\"L13H13\",\"L14H0\",\"L14H1\",\"L14H2\",\"L14H3\",\"L14H4\",\"L14H5\",\"L14H6\",\"L14H7\",\"L14H8\",\"L14H9\",\"L14H10\",\"L14H11\",\"L14H12\",\"L14H13\",\"L15H0\",\"L15H1\",\"L15H2\",\"L15H3\",\"L15H4\",\"L15H5\",\"L15H6\",\"L15H7\",\"L15H8\",\"L15H9\",\"L15H10\",\"L15H11\",\"L15H12\",\"L15H13\",\"L16H0\",\"L16H1\",\"L16H2\",\"L16H3\",\"L16H4\",\"L16H5\",\"L16H6\",\"L16H7\",\"L16H8\",\"L16H9\",\"L16H10\",\"L16H11\",\"L16H12\",\"L16H13\",\"L17H0\",\"L17H1\",\"L17H2\",\"L17H3\",\"L17H4\",\"L17H5\",\"L17H6\",\"L17H7\",\"L17H8\",\"L17H9\",\"L17H10\",\"L17H11\",\"L17H12\",\"L17H13\",\"L18H0\",\"L18H1\",\"L18H2\",\"L18H3\",\"L18H4\",\"L18H5\",\"L18H6\",\"L18H7\",\"L18H8\",\"L18H9\",\"L18H10\",\"L18H11\",\"L18H12\",\"L18H13\",\"L19H0\",\"L19H1\",\"L19H2\",\"L19H3\",\"L19H4\",\"L19H5\",\"L19H6\",\"L19H7\",\"L19H8\",\"L19H9\",\"L19H10\",\"L19H11\",\"L19H12\",\"L19H13\",\"L20H0\",\"L20H1\",\"L20H2\",\"L20H3\",\"L20H4\",\"L20H5\",\"L20H6\",\"L20H7\",\"L20H8\",\"L20H9\",\"L20H10\",\"L20H11\",\"L20H12\",\"L20H13\",\"L21H0\",\"L21H1\",\"L21H2\",\"L21H3\",\"L21H4\",\"L21H5\",\"L21H6\",\"L21H7\",\"L21H8\",\"L21H9\",\"L21H10\",\"L21H11\",\"L21H12\",\"L21H13\",\"L22H0\",\"L22H1\",\"L22H2\",\"L22H3\",\"L22H4\",\"L22H5\",\"L22H6\",\"L22H7\",\"L22H8\",\"L22H9\",\"L22H10\",\"L22H11\",\"L22H12\",\"L22H13\",\"L23H0\",\"L23H1\",\"L23H2\",\"L23H3\",\"L23H4\",\"L23H5\",\"L23H6\",\"L23H7\",\"L23H8\",\"L23H9\",\"L23H10\",\"L23H11\",\"L23H12\",\"L23H13\"],\"legendgroup\":\"\",\"marker\":{\"color\":{\"dtype\":\"i1\",\"bdata\":\"AAAAAAAAAAAAAAAAAAABAQEBAQEBAQEBAQEBAQICAgICAgICAgICAgICAwMDAwMDAwMDAwMDAwMEBAQEBAQEBAQEBAQEBAUFBQUFBQUFBQUFBQUFBgYGBgYGBgYGBgYGBgYHBwcHBwcHBwcHBwcHBwgICAgICAgICAgICAgICQkJCQkJCQkJCQkJCQkKCgoKCgoKCgoKCgoKCgsLCwsLCwsLCwsLCwsLDAwMDAwMDAwMDAwMDAwNDQ0NDQ0NDQ0NDQ0NDQ4ODg4ODg4ODg4ODg4ODw8PDw8PDw8PDw8PDw8QEBAQEBAQEBAQEBAQEBEREREREREREREREREREhISEhISEhISEhISEhITExMTExMTExMTExMTExQUFBQUFBQUFBQUFBQUFRUVFRUVFRUVFRUVFRUWFhYWFhYWFhYWFhYWFhcXFxcXFxcXFxcXFxcX\"},\"coloraxis\":\"coloraxis\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"\",\"orientation\":\"v\",\"showlegend\":false,\"x\":{\"dtype\":\"f4\",\"bdata\":\"AAAAgHw5Mb5xiq89b3dwPWyODT3UxLo9AAAAgAAAAIC355M8qXJkvPz6tjo+zHg8aJ+XvAAAAIAAAACAPCS2PFYFtrwKCz88EAzwuo5sKL3TKyU91S2mun4i17tFhu08007gOu9TVzzawyM71BKNugAAAIDcXTc9HYp8vI6UNTx2mdI91a4rO+YmF71l\\u002fKU7VzENPqEbkT3oosC864spPfq4Vbx\\u002fhC27AAAAgLNyxrxMqZW9BrZhvQdJI717m8k98d71vJSw\\u002fzsgqNw8YN0KPVnVBjy3elQ7\\u002f5I\\u002fuN6MlzsAAACA\\u002fTJ0u\\u002f\\u002fA3r3g9Pm8mAPXvFgLw7zPS6Q9x80DO8Cd8Tw4zVI8mmMiPFq0yzyev1Y9Q4DgugAAAIB8V1g7\\u002fLP5PFKWAL1fKFM9V9ULve1VYjn16I881PIcPR\\u002fYDLy+c+O8ozgwuxstg7u2VFK8AAAAgFHMvLtAhHa82JTDu3Fe07qbc1c9daxYPdhzkzzoHvQ8uyXePAmD5jtTFLU8mxECPVmZMr0AAACAbSqsvW2+Mj2YXE88hzyXPIITWL0iCk8+p4CePC+cBr0GdcG8tWlePCUmiDrQGK67M8YKPQAAAICBs427\\u002f8MqPE50lLxvnfe86tyJOxJmJL3XKxu8AU4JPcI8Vz2HlEm8Wu\\u002fZOi9SA7t7FDK9AAAAgIagdzxSBoa6FvnaO3nTFryz2e27DrcOuos7xzyMWzI8uVA8PKLgfDwxZ4U8qpksPTJbVr0AAACAwny3PJtXc7zMgKW6dOSgu5Y42LzODIq8pE89PSSeLzy5Cws9WofsPOuZ2Lx3VCE9uDepvQAAAIAgEZA6EmIYvOe4kjvqH7C4citdu20qrLt\\u002f3aU7FRiLu7ulnjrUEo06UCfAusOO8riu6vc7AAAAgOZ+ybtN+uq7Vzqtugv2sjx+rYC8ovUJvBRLgbr+3k29JsImu4+58Tzfp7A7sBlYOrkYdL0AAACAyvA0PBRopTpToqS7d2jiO9R9QLxsXSK8fbkuPHjAFb3FeKG8ywyUu90NnTtaocq7SjrVPAAAAIDOQ4y8+f+SPIyCeTvT6v45Kqi+u5Ih1bwTX0e92TvLvF8IY72Z3I88gR5BO32uRTxuLXI9AAAAgDZ3KzykGDu8Zvqaux+6orvFUJS6S6WJO\\u002f6USryFj4K8diaCvFgQFTzKod+6cMZFvQ5elrwAAACA3HuhvLSJ0zsBy\\u002fw65gitOFCAOLw4\\u002fwO7xVAUNgbgdLt8pi28cqOFOb6hiDq4PoG7X4VXOwAAAIBT3bK888PSuxYqRjz3ZDm7fgWzOjw3N70MCbQ7JIWXPHqCNjzVrqs7\\u002f5I\\u002fOv9h1Dvcwx49AAAAgCvXHjv4Yi67midOu6MbjDtALYo6pKFYu\\u002fV7UDwxnge7hg03vGn\\u002f4btk\\u002fjC7A8dmu0OA4DoAAACAragXvJuJpLmYjbq7STF3usTwSLuAlmi8BfbGO+nD8DxqV5U81qwgOVmY7Luoktm7tBoPvQAAAICLMaQ832tcvP91G7sbmDY7szvEO8U9E7zadZS7Ik\\u002ffvegjxrxWp2s7dVgyO6LWWTwdFSY5AAAAgG3leTvio5o8nlNdvBAM8LtEkpy7\\u002fh4uvE0OsroMiC47Nb7nuo1GpjuITlK7vPyLu4tr7DwAAACAVYkHPfNqWjs\\u002f6Fc6NgGPO7wZsLsnwJs6odhku8kCdT4lf4C7hltGvKWW7zsMVv281BtqPAAAAIDvta27CF3kOp\\u002fa77oYHVK8hDfPuwhUh7tKTVa8BL3DvKLWWTmJk4Q6otZZucpnF7t+x9i9\"},\"xaxis\":\"x\",\"y\":{\"dtype\":\"f4\",\"bdata\":\"ICKLPZ6WCb0dH8k8eheDPHyl5zp8Jm25tOcdvYrLwTqukwu8T8uBvDEKe72hbb8+UBkRveW74r0LngC6oHtgPbXDnDx0JIE8zx3\\u002fvDoEy7xg8Iu8+wGUvHPS5DyJ+eW84Zo8PHSWkbvbX8K62MvFPPPXGbwkvBm88IxaPM5+GruP+VE9hBqrOWs3oLsbZUU9I9SuPYQ7nj0+zT68Vk85PYuKHD2rtYu9MqVZu6kzSj0cvji8ACYBPJknUz0dT246a628u+JURbyO9gs7eZdCPROnxD3cz0e9VAgHvvbR9732Eh49U1PPPPppgb3TRQO8ugCiPGULGz1rbqI9SV+cPaMuDbwJ8CW82morPZLco7uuGd29luClvcqYgjv+ygc9g34Mu1\\u002fBqzwtp+490SEMPDkGVrzhGvw84QAkvRe7e7wMsYG8Fu5xOwdf7zuElZm8jNKUO3khJjsd0HM7MDKOPNGlVz0Z1pW83dIOPV8daj34uSA9T6d\\u002fvaOahrqEHre8qwGVPQZ+nzxjr9u8DHTnPPaDaDwOpU29hE4fPrlBxz2T+II8MMXOPA25GbvXDbG8aUFIPXDWer3wFwS73PtgO3EQxDwlm148RUtfvBh2yrxEJV082oQJvenwT7znOFK9T3ggPYXEeL2vbD49ZlLNPEH6k73p1vE8Sfeuuh\\u002fDALs1PKI9hTYKPUc\\u002fMTyb17M8vr6suwfBxTX8llU6xVCUt5rthTzSMoK8N4CJOro7MD0mEwI9qDAEvN\\u002fPPT3fOzc8kHzYPbig1rwsjlu91hogvT1U2zylPXe8lydYPckEezzbBYo91LbIvelhmzxvWQy8slu5vMNJQTyvm6O8z9P7u1M1ZbxJMXe2l\\u002fwEvD+uD7oWzUG9HDRVuGrSA70HwcU1lCquvTYeszvNdbw8SWJiOzT6O7ywmZg829twPcYx5LytHW67zvmIPGtCibyf2u87tkHRvPRoT7va4Me7laZcvOttP7tN9Rk9KzsAOz3sbTpABfy6yxVxPBk4azrkgl+8MPQzvZejDLyDzJs7+0rRvM\\u002fAejyA+wo9N8U6vDALRjyAXia9vK02vE1dh7xCyKC9+nRkPd351buch5m6QQNxO2zxqLwh8Zq9A6pCO2hft7x1BVG9YU8WPdPdlTyB0DG76JnivAqkF71NZuQ8cMkRPduDPr0jeK27V7knOzNakTzB\\u002fgI81qwgty6\\u002fwTuXrGk6c74euhFKxbzCD\\u002fi6FmYaO2NiEryNY0q5DUHxO9d45Dy9efo6euqjvOu7Tj3woCE8lhs0PBSfJz2kkuO8Ep0mvHEVFr0YHdK4XkKxuyH7vTs9a+i6EnDHvAD1lTogBqe7Ne\\u002fSO2\\u002fPKLw1PWK8gBYpPDneSLz5Lzg7KKFrvJ3yTLsc29w7XbA1vBO4vzuqtlA820t7u4hiGTuXciG6JYmjvIsiLzxdulg8g8ybO0CGAjwa6JM90fi4PLlid7oB30M8xQIFvNRsRb3e90q8H\\u002fTqvJRoCL1vk9Q7owyXvPoG5TzeMx+8thqKO2O5\\u002frrdgTM9Zp+cveRqyr2kU8m6hP0GOzxNRj4jUKA7Y7n+ur937znUJY49EhSJvLwZMDyDmzA7AOlmvFQDtbw4pou8AiMwPMe5vLthjKq7g1b+unRuBDseP7Q8SpPNO8UQtLy2QVE872WYvUDBkDyFmF89EaDCPuZsDrvAIoQ89+1WPGmXt702hiA91Gn5O6zlMDyMLM29tSrEvNVVs7z6aYG7i2fgOmsuwjxVH5Q6KZaDPAX3jDwwp+Q55DzovM3Rer3z9L07\"},\"yaxis\":\"y\",\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermap\":[{\"type\":\"scattermap\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Value Patch\"},\"range\":[-0.5,0.5]},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Output Patch\"},\"range\":[-0.5,0.5]},\"coloraxis\":{\"colorbar\":{\"title\":{\"text\":\"Layer\"}},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"legend\":{\"tracegroupgap\":0},\"title\":{\"text\":\"Scatter plot of output patching vs value patching\"}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('fef3b727-4f28-4ecc-84b7-9473969966d8');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "head_labels = [\n",
    "    f\"L{layer}H{head}\"\n",
    "    for layer in range(tl_model.cfg.n_layers)\n",
    "    for head in range(tl_model.cfg.n_heads)\n",
    "]\n",
    "scatter(\n",
    "    x=utils.to_numpy(patched_head_v_diff.flatten()),\n",
    "    y=utils.to_numpy(patched_head_z_diff.flatten()),\n",
    "    xaxis=\"Value Patch\",\n",
    "    yaxis=\"Output Patch\",\n",
    "    caxis=\"Layer\",\n",
    "    hover_name=head_labels,\n",
    "    color=einops.repeat(\n",
    "        np.arange(tl_model.cfg.n_layers),\n",
    "        \"layer -> (layer head)\",\n",
    "        head=tl_model.cfg.n_heads,\n",
    "    ),\n",
    "    range_x=(-0.5, 0.5),\n",
    "    range_y=(-0.5, 0.5),\n",
    "    title=\"Scatter plot of output patching vs value patching\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we patch in attention patterns, we see a different effect - early and late heads matter a lot, middle heads don't. (In fact, the sum of value patching and pattern patching is approx the same as output patching)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patch_head_pattern(\n",
    "    corrupted_head_pattern: Float[torch.Tensor, \"batch head_index query_pos d_head\"],\n",
    "    hook,\n",
    "    head_index,\n",
    "    clean_cache,\n",
    "):\n",
    "    corrupted_head_pattern[:, head_index, :, :] = clean_cache[hook.name][\n",
    "        :, head_index, :, :\n",
    "    ]\n",
    "    return corrupted_head_pattern\n",
    "\n",
    "\n",
    "patched_head_attn_diff = torch.zeros(\n",
    "    tl_model.cfg.n_layers, tl_model.cfg.n_heads, device=device, dtype=torch.float32\n",
    ")\n",
    "for layer in range(tl_model.cfg.n_layers):\n",
    "    for head_index in range(tl_model.cfg.n_heads):\n",
    "        hook_fn = partial(patch_head_pattern, head_index=head_index, clean_cache=cache)\n",
    "        patched_logits = tl_model.run_with_hooks(\n",
    "            corrupted_tokens,\n",
    "            fwd_hooks=[(utils.get_act_name(\"attn\", layer, \"attn\"), hook_fn)],\n",
    "            return_type=\"logits\",\n",
    "        )\n",
    "        patched_logit_diff = logits_to_ave_logit_diff(patched_logits, answer_tokens)\n",
    "\n",
    "        patched_head_attn_diff[layer, head_index] = normalize_patched_logit_diff(\n",
    "            patched_logit_diff\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
       "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-3.0.1.min.js\"></script>                <div id=\"abf24867-54f4-40ee-a5af-02ffdecc8ac5\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                window.PLOTLYENV=window.PLOTLYENV || {};                                if (document.getElementById(\"abf24867-54f4-40ee-a5af-02ffdecc8ac5\")) {                    Plotly.newPlot(                        \"abf24867-54f4-40ee-a5af-02ffdecc8ac5\",                        [{\"coloraxis\":\"coloraxis\",\"name\":\"0\",\"z\":{\"dtype\":\"f4\",\"bdata\":\"4BLkOgfBRbcMxAI9xVAUO898DzocNNW5nr\\u002fWuxwXsbo1Xhi9OmisuwFQj7zO9La7O\\u002fonvPjdnLwzo066Jn10Om7akTvQeoS7llcIvGKyrLuwXgq7F0KZvOvGt7t53HO8cRBEvIGzDbvmJdG7vXl6PMxjATw6tXW821WfPBIUCTxQRGQ8s3eYu+qDkbuFNUS86wokPPdpi7yWkVA5Cov+PIS84LwrToG8peYKuzAjmbxFOaS7j8OVuyeZUz30Sys64bgmvDU9Yrt+LgY86Q+9vVFf\\u002fTvydUO8GjemvNBJmTsoed46kn0ZPRGVDrxHXZs7dOPaPDrnJruFe7u8IGoIPVVZXLvz6VS8QgHmOwffL7zPtZe9QLanPFAnwDrlCrg8KA6ru1sV3DxLoLc8PxErvFnx5Ds8rpk8xU4OvRo3Jr2HWgG80rOHPMaO6DzB2IA8\\u002fEhGO+7U3DvG2Ou7QiSiPIT9BrzsiZ68eHFAu9xbMT2T0bo7v3dvOvTLarwQvZu8LnGyOmkEtLxwEs87Dg57O\\u002fQ3ZLxo0AI8DOZ3Pf9+eLs70VQ7xH64POkN9Lv69++8qTicPCCmVr214Qa8cg65Ow4O+7kACBc8Io05PHa6iLp3y308MpbkvHYmArz2AuM6ph5IOwpj8bzsAIE792Q5OUIMT70E9QY9ZUo1PBLdBj0g4CQ8sN8PuyLcjjsNJRO879LROwfBRbYrO4A6B8FFtjlglDxLdB67V3R1vJg\\u002fKz3m9GU80bTMudBxJjwAL9672NsAO8jVG7xiHeC6ruGaOwsUnTvYAQO9+aomPavC9DyGH3I8eOfcO1LzBDwC3bi7Ji4gvLnGWLtWJua7sgzku4PMG7wHwcW1OX04vCs7gLmNBEC9FAZPuGuA3bxJMXc2hbZJO1xsSTxqHAc9VCGfOQdWkjuJ1qo7BQDqPFQhn7v\\u002fJ4y7HBexu8JzWTrbynU7q+GkvPnN4Tsn1OG79WbEOtOwtrrILU481LmUuwQpPbvxbOU5BPjROxYCuTlUjFK8H0PAvMOO8jp2CJg77IjYvLXhhrzbcwm8sKiNvAASOrvBOZG7PbIlurYPoTuFT6K8iBSKu4yC+brY7bs7xPBIu4bIhbv3d7o8ZJpPOxfG5DvMO3M75okyPOYILbpcTyW6FTWvu2WuFjtSeBY9PR6fPNNiJ7wTLly8GB3SOSEsKbwhSU061qygt3rRCzp0+OY5VR8Uuk0ED73R0fC7STH3N79GBbx60Qu6+2f1u3Xrcjw51eo4LH4mvJ6xpzy\\u002fvKE7yoQ7OnLAKbzm64i7f9MCvDotHrw4daC7YGaou37UR7vNuu06C7sku\\u002fstLTwnQaE6V5wDOzvRVLtuxsq7EItqPPx5sbs9sqW5WDgiu8drrbm0CM4769hyO5+DAzsceYc72faZu9MxvLswp+Q4oHgavD5XojvXKxs7aPiPO+IZNzy3CYo9ierwOzwCwDtKp5S7EQGIu9yZC7z4HXw7vfr\\u002fulL9J7xrwQO7JGJbOy+My7vkqOG6e4rOu\\u002ffGDzsLFJ27dYURvfnWv7sHwUW4HDRVOfPva71NwCK836ewOxxR+TqWdCw7WJr3u0+8DLy1tKc8SlmFuuSoYTuPRBu6D3BRO5OV5rnm64g6WtI1u8BYwLl\\u002f8CY8pKt7vPTysrtmBYS8x+FJvFFf\\u002fTkaymY8RwSjPIHFyDtv4eO7cqMFOnDqwby86Yo8sRdNu7IyZrs6tjs6IwIRvL+8ITsv5UM81qygOtEWozsCj6m5OE2TOzaLcTsHX+87GGwnvCiER7yEfAG7\",\"shape\":\"24, 14\"},\"type\":\"heatmap\",\"xaxis\":\"x\",\"yaxis\":\"y\",\"hovertemplate\":\"Head: %{x}\\u003cbr\\u003eLayer: %{y}\\u003cbr\\u003ecolor: %{z}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermap\":[{\"type\":\"scattermap\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"scaleanchor\":\"y\",\"constrain\":\"domain\",\"title\":{\"text\":\"Head\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"autorange\":\"reversed\",\"constrain\":\"domain\",\"title\":{\"text\":\"Layer\"}},\"coloraxis\":{\"colorscale\":[[0.0,\"rgb(103,0,31)\"],[0.1,\"rgb(178,24,43)\"],[0.2,\"rgb(214,96,77)\"],[0.3,\"rgb(244,165,130)\"],[0.4,\"rgb(253,219,199)\"],[0.5,\"rgb(247,247,247)\"],[0.6,\"rgb(209,229,240)\"],[0.7,\"rgb(146,197,222)\"],[0.8,\"rgb(67,147,195)\"],[0.9,\"rgb(33,102,172)\"],[1.0,\"rgb(5,48,97)\"]],\"cmid\":0.0},\"title\":{\"text\":\"Logit Difference From Patched Head Pattern\"}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('abf24867-54f4-40ee-a5af-02ffdecc8ac5');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
       "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-3.0.1.min.js\"></script>                <div id=\"9b445dd1-9c43-44f0-b247-dd60e88de582\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                window.PLOTLYENV=window.PLOTLYENV || {};                                if (document.getElementById(\"9b445dd1-9c43-44f0-b247-dd60e88de582\")) {                    Plotly.newPlot(                        \"9b445dd1-9c43-44f0-b247-dd60e88de582\",                        [{\"hovertemplate\":\"\\u003cb\\u003e%{hovertext}\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003eAttention Patch=%{x}\\u003cbr\\u003eOutput Patch=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"hovertext\":[\"L0H0\",\"L0H1\",\"L0H2\",\"L0H3\",\"L0H4\",\"L0H5\",\"L0H6\",\"L0H7\",\"L0H8\",\"L0H9\",\"L0H10\",\"L0H11\",\"L0H12\",\"L0H13\",\"L1H0\",\"L1H1\",\"L1H2\",\"L1H3\",\"L1H4\",\"L1H5\",\"L1H6\",\"L1H7\",\"L1H8\",\"L1H9\",\"L1H10\",\"L1H11\",\"L1H12\",\"L1H13\",\"L2H0\",\"L2H1\",\"L2H2\",\"L2H3\",\"L2H4\",\"L2H5\",\"L2H6\",\"L2H7\",\"L2H8\",\"L2H9\",\"L2H10\",\"L2H11\",\"L2H12\",\"L2H13\",\"L3H0\",\"L3H1\",\"L3H2\",\"L3H3\",\"L3H4\",\"L3H5\",\"L3H6\",\"L3H7\",\"L3H8\",\"L3H9\",\"L3H10\",\"L3H11\",\"L3H12\",\"L3H13\",\"L4H0\",\"L4H1\",\"L4H2\",\"L4H3\",\"L4H4\",\"L4H5\",\"L4H6\",\"L4H7\",\"L4H8\",\"L4H9\",\"L4H10\",\"L4H11\",\"L4H12\",\"L4H13\",\"L5H0\",\"L5H1\",\"L5H2\",\"L5H3\",\"L5H4\",\"L5H5\",\"L5H6\",\"L5H7\",\"L5H8\",\"L5H9\",\"L5H10\",\"L5H11\",\"L5H12\",\"L5H13\",\"L6H0\",\"L6H1\",\"L6H2\",\"L6H3\",\"L6H4\",\"L6H5\",\"L6H6\",\"L6H7\",\"L6H8\",\"L6H9\",\"L6H10\",\"L6H11\",\"L6H12\",\"L6H13\",\"L7H0\",\"L7H1\",\"L7H2\",\"L7H3\",\"L7H4\",\"L7H5\",\"L7H6\",\"L7H7\",\"L7H8\",\"L7H9\",\"L7H10\",\"L7H11\",\"L7H12\",\"L7H13\",\"L8H0\",\"L8H1\",\"L8H2\",\"L8H3\",\"L8H4\",\"L8H5\",\"L8H6\",\"L8H7\",\"L8H8\",\"L8H9\",\"L8H10\",\"L8H11\",\"L8H12\",\"L8H13\",\"L9H0\",\"L9H1\",\"L9H2\",\"L9H3\",\"L9H4\",\"L9H5\",\"L9H6\",\"L9H7\",\"L9H8\",\"L9H9\",\"L9H10\",\"L9H11\",\"L9H12\",\"L9H13\",\"L10H0\",\"L10H1\",\"L10H2\",\"L10H3\",\"L10H4\",\"L10H5\",\"L10H6\",\"L10H7\",\"L10H8\",\"L10H9\",\"L10H10\",\"L10H11\",\"L10H12\",\"L10H13\",\"L11H0\",\"L11H1\",\"L11H2\",\"L11H3\",\"L11H4\",\"L11H5\",\"L11H6\",\"L11H7\",\"L11H8\",\"L11H9\",\"L11H10\",\"L11H11\",\"L11H12\",\"L11H13\",\"L12H0\",\"L12H1\",\"L12H2\",\"L12H3\",\"L12H4\",\"L12H5\",\"L12H6\",\"L12H7\",\"L12H8\",\"L12H9\",\"L12H10\",\"L12H11\",\"L12H12\",\"L12H13\",\"L13H0\",\"L13H1\",\"L13H2\",\"L13H3\",\"L13H4\",\"L13H5\",\"L13H6\",\"L13H7\",\"L13H8\",\"L13H9\",\"L13H10\",\"L13H11\",\"L13H12\",\"L13H13\",\"L14H0\",\"L14H1\",\"L14H2\",\"L14H3\",\"L14H4\",\"L14H5\",\"L14H6\",\"L14H7\",\"L14H8\",\"L14H9\",\"L14H10\",\"L14H11\",\"L14H12\",\"L14H13\",\"L15H0\",\"L15H1\",\"L15H2\",\"L15H3\",\"L15H4\",\"L15H5\",\"L15H6\",\"L15H7\",\"L15H8\",\"L15H9\",\"L15H10\",\"L15H11\",\"L15H12\",\"L15H13\",\"L16H0\",\"L16H1\",\"L16H2\",\"L16H3\",\"L16H4\",\"L16H5\",\"L16H6\",\"L16H7\",\"L16H8\",\"L16H9\",\"L16H10\",\"L16H11\",\"L16H12\",\"L16H13\",\"L17H0\",\"L17H1\",\"L17H2\",\"L17H3\",\"L17H4\",\"L17H5\",\"L17H6\",\"L17H7\",\"L17H8\",\"L17H9\",\"L17H10\",\"L17H11\",\"L17H12\",\"L17H13\",\"L18H0\",\"L18H1\",\"L18H2\",\"L18H3\",\"L18H4\",\"L18H5\",\"L18H6\",\"L18H7\",\"L18H8\",\"L18H9\",\"L18H10\",\"L18H11\",\"L18H12\",\"L18H13\",\"L19H0\",\"L19H1\",\"L19H2\",\"L19H3\",\"L19H4\",\"L19H5\",\"L19H6\",\"L19H7\",\"L19H8\",\"L19H9\",\"L19H10\",\"L19H11\",\"L19H12\",\"L19H13\",\"L20H0\",\"L20H1\",\"L20H2\",\"L20H3\",\"L20H4\",\"L20H5\",\"L20H6\",\"L20H7\",\"L20H8\",\"L20H9\",\"L20H10\",\"L20H11\",\"L20H12\",\"L20H13\",\"L21H0\",\"L21H1\",\"L21H2\",\"L21H3\",\"L21H4\",\"L21H5\",\"L21H6\",\"L21H7\",\"L21H8\",\"L21H9\",\"L21H10\",\"L21H11\",\"L21H12\",\"L21H13\",\"L22H0\",\"L22H1\",\"L22H2\",\"L22H3\",\"L22H4\",\"L22H5\",\"L22H6\",\"L22H7\",\"L22H8\",\"L22H9\",\"L22H10\",\"L22H11\",\"L22H12\",\"L22H13\",\"L23H0\",\"L23H1\",\"L23H2\",\"L23H3\",\"L23H4\",\"L23H5\",\"L23H6\",\"L23H7\",\"L23H8\",\"L23H9\",\"L23H10\",\"L23H11\",\"L23H12\",\"L23H13\"],\"legendgroup\":\"\",\"marker\":{\"color\":\"#636efa\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"\",\"orientation\":\"v\",\"showlegend\":false,\"x\":{\"dtype\":\"f4\",\"bdata\":\"4BLkOgfBRbcMxAI9xVAUO898DzocNNW5nr\\u002fWuxwXsbo1Xhi9OmisuwFQj7zO9La7O\\u002fonvPjdnLwzo066Jn10Om7akTvQeoS7llcIvGKyrLuwXgq7F0KZvOvGt7t53HO8cRBEvIGzDbvmJdG7vXl6PMxjATw6tXW821WfPBIUCTxQRGQ8s3eYu+qDkbuFNUS86wokPPdpi7yWkVA5Cov+PIS84LwrToG8peYKuzAjmbxFOaS7j8OVuyeZUz30Sys64bgmvDU9Yrt+LgY86Q+9vVFf\\u002fTvydUO8GjemvNBJmTsoed46kn0ZPRGVDrxHXZs7dOPaPDrnJruFe7u8IGoIPVVZXLvz6VS8QgHmOwffL7zPtZe9QLanPFAnwDrlCrg8KA6ru1sV3DxLoLc8PxErvFnx5Ds8rpk8xU4OvRo3Jr2HWgG80rOHPMaO6DzB2IA8\\u002fEhGO+7U3DvG2Ou7QiSiPIT9BrzsiZ68eHFAu9xbMT2T0bo7v3dvOvTLarwQvZu8LnGyOmkEtLxwEs87Dg57O\\u002fQ3ZLxo0AI8DOZ3Pf9+eLs70VQ7xH64POkN9Lv69++8qTicPCCmVr214Qa8cg65Ow4O+7kACBc8Io05PHa6iLp3y308MpbkvHYmArz2AuM6ph5IOwpj8bzsAIE792Q5OUIMT70E9QY9ZUo1PBLdBj0g4CQ8sN8PuyLcjjsNJRO879LROwfBRbYrO4A6B8FFtjlglDxLdB67V3R1vJg\\u002fKz3m9GU80bTMudBxJjwAL9672NsAO8jVG7xiHeC6ruGaOwsUnTvYAQO9+aomPavC9DyGH3I8eOfcO1LzBDwC3bi7Ji4gvLnGWLtWJua7sgzku4PMG7wHwcW1OX04vCs7gLmNBEC9FAZPuGuA3bxJMXc2hbZJO1xsSTxqHAc9VCGfOQdWkjuJ1qo7BQDqPFQhn7v\\u002fJ4y7HBexu8JzWTrbynU7q+GkvPnN4Tsn1OG79WbEOtOwtrrILU481LmUuwQpPbvxbOU5BPjROxYCuTlUjFK8H0PAvMOO8jp2CJg77IjYvLXhhrzbcwm8sKiNvAASOrvBOZG7PbIlurYPoTuFT6K8iBSKu4yC+brY7bs7xPBIu4bIhbv3d7o8ZJpPOxfG5DvMO3M75okyPOYILbpcTyW6FTWvu2WuFjtSeBY9PR6fPNNiJ7wTLly8GB3SOSEsKbwhSU061qygt3rRCzp0+OY5VR8Uuk0ED73R0fC7STH3N79GBbx60Qu6+2f1u3Xrcjw51eo4LH4mvJ6xpzy\\u002fvKE7yoQ7OnLAKbzm64i7f9MCvDotHrw4daC7YGaou37UR7vNuu06C7sku\\u002fstLTwnQaE6V5wDOzvRVLtuxsq7EItqPPx5sbs9sqW5WDgiu8drrbm0CM4769hyO5+DAzsceYc72faZu9MxvLswp+Q4oHgavD5XojvXKxs7aPiPO+IZNzy3CYo9ierwOzwCwDtKp5S7EQGIu9yZC7z4HXw7vfr\\u002fulL9J7xrwQO7JGJbOy+My7vkqOG6e4rOu\\u002ffGDzsLFJ27dYURvfnWv7sHwUW4HDRVOfPva71NwCK836ewOxxR+TqWdCw7WJr3u0+8DLy1tKc8SlmFuuSoYTuPRBu6D3BRO5OV5rnm64g6WtI1u8BYwLl\\u002f8CY8pKt7vPTysrtmBYS8x+FJvFFf\\u002fTkaymY8RwSjPIHFyDtv4eO7cqMFOnDqwby86Yo8sRdNu7IyZrs6tjs6IwIRvL+8ITsv5UM81qygOtEWozsCj6m5OE2TOzaLcTsHX+87GGwnvCiER7yEfAG7\"},\"xaxis\":\"x\",\"y\":{\"dtype\":\"f4\",\"bdata\":\"ICKLPZ6WCb0dH8k8eheDPHyl5zp8Jm25tOcdvYrLwTqukwu8T8uBvDEKe72hbb8+UBkRveW74r0LngC6oHtgPbXDnDx0JIE8zx3\\u002fvDoEy7xg8Iu8+wGUvHPS5DyJ+eW84Zo8PHSWkbvbX8K62MvFPPPXGbwkvBm88IxaPM5+GruP+VE9hBqrOWs3oLsbZUU9I9SuPYQ7nj0+zT68Vk85PYuKHD2rtYu9MqVZu6kzSj0cvji8ACYBPJknUz0dT246a628u+JURbyO9gs7eZdCPROnxD3cz0e9VAgHvvbR9732Eh49U1PPPPppgb3TRQO8ugCiPGULGz1rbqI9SV+cPaMuDbwJ8CW82morPZLco7uuGd29luClvcqYgjv+ygc9g34Mu1\\u002fBqzwtp+490SEMPDkGVrzhGvw84QAkvRe7e7wMsYG8Fu5xOwdf7zuElZm8jNKUO3khJjsd0HM7MDKOPNGlVz0Z1pW83dIOPV8daj34uSA9T6d\\u002fvaOahrqEHre8qwGVPQZ+nzxjr9u8DHTnPPaDaDwOpU29hE4fPrlBxz2T+II8MMXOPA25GbvXDbG8aUFIPXDWer3wFwS73PtgO3EQxDwlm148RUtfvBh2yrxEJV082oQJvenwT7znOFK9T3ggPYXEeL2vbD49ZlLNPEH6k73p1vE8Sfeuuh\\u002fDALs1PKI9hTYKPUc\\u002fMTyb17M8vr6suwfBxTX8llU6xVCUt5rthTzSMoK8N4CJOro7MD0mEwI9qDAEvN\\u002fPPT3fOzc8kHzYPbig1rwsjlu91hogvT1U2zylPXe8lydYPckEezzbBYo91LbIvelhmzxvWQy8slu5vMNJQTyvm6O8z9P7u1M1ZbxJMXe2l\\u002fwEvD+uD7oWzUG9HDRVuGrSA70HwcU1lCquvTYeszvNdbw8SWJiOzT6O7ywmZg829twPcYx5LytHW67zvmIPGtCibyf2u87tkHRvPRoT7va4Me7laZcvOttP7tN9Rk9KzsAOz3sbTpABfy6yxVxPBk4azrkgl+8MPQzvZejDLyDzJs7+0rRvM\\u002fAejyA+wo9N8U6vDALRjyAXia9vK02vE1dh7xCyKC9+nRkPd351buch5m6QQNxO2zxqLwh8Zq9A6pCO2hft7x1BVG9YU8WPdPdlTyB0DG76JnivAqkF71NZuQ8cMkRPduDPr0jeK27V7knOzNakTzB\\u002fgI81qwgty6\\u002fwTuXrGk6c74euhFKxbzCD\\u002fi6FmYaO2NiEryNY0q5DUHxO9d45Dy9efo6euqjvOu7Tj3woCE8lhs0PBSfJz2kkuO8Ep0mvHEVFr0YHdK4XkKxuyH7vTs9a+i6EnDHvAD1lTogBqe7Ne\\u002fSO2\\u002fPKLw1PWK8gBYpPDneSLz5Lzg7KKFrvJ3yTLsc29w7XbA1vBO4vzuqtlA820t7u4hiGTuXciG6JYmjvIsiLzxdulg8g8ybO0CGAjwa6JM90fi4PLlid7oB30M8xQIFvNRsRb3e90q8H\\u002fTqvJRoCL1vk9Q7owyXvPoG5TzeMx+8thqKO2O5\\u002frrdgTM9Zp+cveRqyr2kU8m6hP0GOzxNRj4jUKA7Y7n+ur937znUJY49EhSJvLwZMDyDmzA7AOlmvFQDtbw4pou8AiMwPMe5vLthjKq7g1b+unRuBDseP7Q8SpPNO8UQtLy2QVE872WYvUDBkDyFmF89EaDCPuZsDrvAIoQ89+1WPGmXt702hiA91Gn5O6zlMDyMLM29tSrEvNVVs7z6aYG7i2fgOmsuwjxVH5Q6KZaDPAX3jDwwp+Q55DzovM3Rer3z9L07\"},\"yaxis\":\"y\",\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermap\":[{\"type\":\"scattermap\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Attention Patch\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Output Patch\"}},\"legend\":{\"tracegroupgap\":0},\"title\":{\"text\":\"Scatter plot of output patching vs attention patching\"}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('9b445dd1-9c43-44f0-b247-dd60e88de582');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "imshow(\n",
    "    patched_head_attn_diff,\n",
    "    title=\"Logit Difference From Patched Head Pattern\",\n",
    "    labels={\"x\": \"Head\", \"y\": \"Layer\"},\n",
    ")\n",
    "head_labels = [\n",
    "    f\"L{layer}H{head}\"\n",
    "    for layer in range(tl_model.cfg.n_layers)\n",
    "    for head in range(tl_model.cfg.n_heads)\n",
    "]\n",
    "scatter(\n",
    "    x=utils.to_numpy(patched_head_attn_diff.flatten()),\n",
    "    y=utils.to_numpy(patched_head_z_diff.flatten()),\n",
    "    hover_name=head_labels,\n",
    "    xaxis=\"Attention Patch\",\n",
    "    yaxis=\"Output Patch\",\n",
    "    title=\"Scatter plot of output patching vs attention patching\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Consolidating Understanding\n",
    "\n",
    "OK, let's zoom out and reconsolidate. At a high-level, we find that all the action is pretty much on the final token. And that attention layers matter much less than MLP layers.\n",
    "\n",
    "We've further localised important behaviour to late heads (L23H1, L22H7, L0H11) whose output matters on the final token and whose behaviour is determined by their attention patterns.\n",
    "\n",
    "A natural speculation is that the early heads are simply pointing to each subproblem of the multiplication, and the late heads are just copying the result.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Attention Patterns\n",
    "\n",
    "We can validate this by looking at the attention patterns of these heads! Let's take the top 10 heads by output patching (in absolute value).\n",
    "\n",
    "We see that early heads attend to every token equally, while later heads attend to just the bos token, which is completely consistent with the above speculation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style='max-width: 700px;'><h2>Top Late Heads</h2><br/><div id=\"circuits-vis-f39fbbaf-67ed\" style=\"margin: 15px 0;\"/>\n",
       "    <script crossorigin type=\"module\">\n",
       "    import { render, AttentionHeads } from \"https://unpkg.com/circuitsvis@1.43.3/dist/cdn/esm.js\";\n",
       "    render(\n",
       "      \"circuits-vis-f39fbbaf-67ed\",\n",
       "      AttentionHeads,\n",
       "      {\"attention\": [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9801579117774963, 0.019842056557536125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8399078249931335, 0.028592221438884735, 0.13149987161159515, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5739988684654236, 0.03731977194547653, 0.10725894570350647, 0.2814224660396576, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5353171229362488, 0.023327788338065147, 0.06860768049955368, 0.20422698557376862, 0.16852037608623505, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5020427107810974, 0.03658520057797432, 0.057042285799980164, 0.1477741003036499, 0.12759587168693542, 0.12895981967449188, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4286271035671234, 0.13476498425006866, 0.08452346920967102, 0.1458914577960968, 0.08981676399707794, 0.06814157962799072, 0.04823462292551994, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.48741063475608826, 0.21811161935329437, 0.04070379585027695, 0.07042673230171204, 0.04881586134433746, 0.04668818414211273, 0.02664249576628208, 0.06120073050260544, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3785581588745117, 0.02074250392615795, 0.09463182091712952, 0.16818654537200928, 0.1083713099360466, 0.08383259922266006, 0.04187420383095741, 0.09423758834600449, 0.009565262123942375, 0.0, 0.0, 0.0, 0.0, 0.0], [0.36511361598968506, 0.01679588109254837, 0.07489944249391556, 0.16066600382328033, 0.10848027467727661, 0.09423933178186417, 0.043420154601335526, 0.07455413043498993, 0.0073877074755728245, 0.05444347485899925, 0.0, 0.0, 0.0, 0.0], [0.29111066460609436, 0.010843253694474697, 0.06639301031827927, 0.17851153016090393, 0.11142275482416153, 0.10266165435314178, 0.043466273695230484, 0.08356974273920059, 0.006533928215503693, 0.05411253124475479, 0.05137458071112633, 0.0, 0.0, 0.0], [0.3116808235645294, 0.00938916951417923, 0.048330385237932205, 0.14997485280036926, 0.11628349870443344, 0.11532124876976013, 0.03598194941878319, 0.06218825653195381, 0.0061105843633413315, 0.04677529260516167, 0.046571966260671616, 0.05139191821217537, 0.0, 0.0], [0.23587049543857574, 0.025944480672478676, 0.05365178734064102, 0.11849358677864075, 0.0996730849146843, 0.09905043244361877, 0.06942792236804962, 0.06447783857584, 0.015776250511407852, 0.05113682523369789, 0.04789099097251892, 0.04328426718711853, 0.07532205432653427, 0.0], [0.3312184512615204, 0.05468115210533142, 0.01623060181736946, 0.05785088986158371, 0.04466833174228668, 0.046086981892585754, 0.03908543661236763, 0.05500713363289833, 0.058827850967645645, 0.021351680159568787, 0.015774957835674286, 0.014574612490832806, 0.04849524423480034, 0.19614669680595398]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9772462844848633, 0.022753726691007614, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0015752729959785938, 0.9959004521369934, 0.002524279523640871, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [3.187491586231772e-07, 0.051012132316827774, 0.9445596933364868, 0.0044279019348323345, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [3.124044078983701e-10, 6.1187411120045e-06, 0.0350872166454792, 0.9611881375312805, 0.003718536114320159, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.2577316965689533e-10, 2.316317360140374e-09, 3.911754447472049e-06, 0.03602474927902222, 0.9611755013465881, 0.0027958329301327467, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.00032922785612754524, 0.00010586369171505794, 0.00010698416735976934, 0.0019331811927258968, 0.10446339845657349, 0.7245888113975525, 0.1684724986553192, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0008473825873807073, 7.70400365581736e-05, 3.6669400742539437e-06, 4.511377028393326e-06, 0.0003561099583748728, 0.0849578008055687, 0.7791382670402527, 0.13461525738239288, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.1250500392634422e-05, 0.0001323506294284016, 8.490666800753388e-07, 5.968320504479152e-09, 1.204843158575386e-08, 2.1230474885669537e-05, 0.07413709908723831, 0.9091201424598694, 0.016577091068029404, 0.0, 0.0, 0.0, 0.0, 0.0], [1.2718653685794834e-08, 1.531808084109798e-05, 1.8427577742841095e-05, 1.9786935467891453e-07, 1.1830553203751037e-09, 2.4532391673659504e-09, 8.404665550187929e-07, 0.0032462819945067167, 0.993349015712738, 0.0033698352053761482, 0.0, 0.0, 0.0, 0.0], [2.2776366903620726e-10, 8.947344554144365e-08, 1.1777507097576745e-05, 2.961994323413819e-05, 2.4300263135046407e-07, 1.8870112139524053e-09, 7.570887250452074e-10, 4.6872153802723915e-07, 0.03893251717090607, 0.9577663540840149, 0.0032589060720056295, 0.0, 0.0, 0.0], [8.180927046907982e-10, 1.2450462882895863e-09, 7.400620205544328e-08, 1.7075022697099485e-05, 2.7538866561371833e-05, 3.3825571676970867e-07, 1.2320628961504099e-09, 7.322377149066028e-10, 6.152445166662801e-06, 0.03893651068210602, 0.9573239684104919, 0.0036882476415485144, 0.0, 0.0], [6.168463784206324e-08, 1.372767233220884e-09, 3.89957982571687e-10, 4.431264599702445e-08, 7.19666695658816e-06, 1.3777434105577413e-05, 1.4222898414573137e-07, 2.529039200283023e-10, 2.2272692579150544e-09, 3.780705583267263e-06, 0.03588421642780304, 0.9613931179046631, 0.0026977199595421553, 0.0], [0.003553529502823949, 0.0005042823031544685, 1.916764085763134e-05, 9.588953616912477e-06, 6.958049925742671e-05, 0.0011667803628370166, 0.0024033745285123587, 0.0005612043314613402, 2.733807014010381e-05, 2.9390397685347125e-05, 0.0009406007593497634, 0.0863599181175232, 0.5314620137214661, 0.37289324402809143]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9822894930839539, 0.017710449174046516, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9929763674736023, 0.004491562955081463, 0.002532119629904628, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9501737356185913, 0.008157048374414444, 0.010380439460277557, 0.031288743019104004, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9718255996704102, 0.00327907782047987, 0.0044193207286298275, 0.008567607030272484, 0.011908361688256264, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9270309209823608, 0.012010728009045124, 0.01055090967565775, 0.01350146159529686, 0.020065827295184135, 0.016840115189552307, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.740828275680542, 0.08139348030090332, 0.03521706908941269, 0.035071440041065216, 0.028265472501516342, 0.017418017610907555, 0.061806317418813705, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3952910006046295, 0.36550843715667725, 0.08139881491661072, 0.0428217276930809, 0.027719244360923767, 0.015061533078551292, 0.03815072029829025, 0.034048497676849365, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7635952830314636, 0.02851956896483898, 0.02808399498462677, 0.037178196012973785, 0.023730408400297165, 0.010804700665175915, 0.0672321617603302, 0.03045213408768177, 0.010403580032289028, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7184218168258667, 0.032764192670583725, 0.015143940225243568, 0.022788049653172493, 0.017707308754324913, 0.0115661034360528, 0.1120862290263176, 0.03850672021508217, 0.0166787076741457, 0.014336992055177689, 0.0, 0.0, 0.0, 0.0], [0.8307138085365295, 0.009883861988782883, 0.006328819785267115, 0.015251890756189823, 0.01328847836703062, 0.008202695287764072, 0.06594384461641312, 0.025328967720270157, 0.008977901190519333, 0.009661238640546799, 0.006418447010219097, 0.0, 0.0, 0.0], [0.821557879447937, 0.0036786918062716722, 0.004790863487869501, 0.012586429715156555, 0.014950676821172237, 0.009914218448102474, 0.07117823511362076, 0.03221217915415764, 0.008257831446826458, 0.009818526916205883, 0.005421573296189308, 0.0056328014470636845, 0.0, 0.0], [0.7330792546272278, 0.01635129004716873, 0.009911814704537392, 0.01519609335809946, 0.012574308551847935, 0.00837187934666872, 0.09093150496482849, 0.052285052835941315, 0.023739948868751526, 0.012187282554805279, 0.006715638563036919, 0.005423454102128744, 0.013232455588877201, 0.0], [0.8112127780914307, 0.03321310132741928, 0.009793085046112537, 0.006748042535036802, 0.005586203653365374, 0.0024571153335273266, 0.025665849447250366, 0.027784565463662148, 0.035264503210783005, 0.010609950870275497, 0.00391794927418232, 0.003328651888296008, 0.00514221703633666, 0.019275978207588196]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5581724643707275, 0.4418274760246277, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6227694153785706, 0.032016586512327194, 0.34521403908729553, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6943106651306152, 0.0029860774520784616, 0.008881024084985256, 0.2938222289085388, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7543718218803406, 0.004114949144423008, 0.0006944765918888152, 0.011804400943219662, 0.2290143072605133, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.46191272139549255, 0.020469723269343376, 0.0009433443774469197, 0.0015832411590963602, 0.011578847654163837, 0.5035120844841003, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6570398211479187, 0.0046067750081419945, 0.0030242293141782284, 0.0014980504056438804, 0.00265860534273088, 0.007584601175040007, 0.3235878646373749, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3883019983768463, 0.0024960865266621113, 0.003573859576135874, 0.0007195955258794129, 0.0010382038308307528, 0.001250476110726595, 0.03847183659672737, 0.5641478896141052, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3628130555152893, 0.002720331307500601, 0.010624445974826813, 0.003866433398798108, 0.0010656181257218122, 0.00039831738104112446, 0.01192310731858015, 0.16451150178909302, 0.4420771300792694, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5308182835578918, 0.0008921929402276874, 0.0018526784842833877, 0.004870223347097635, 0.0027714825700968504, 0.0005484380526468158, 0.0026505968999117613, 0.0068505979143083096, 0.02507815510034561, 0.42366740107536316, 0.0, 0.0, 0.0, 0.0], [0.3610285222530365, 0.004733884707093239, 0.0007671991479583085, 0.002883716020733118, 0.008607239462435246, 0.0074924626387655735, 0.003858168376609683, 0.002198476344347, 0.0016884024953469634, 0.015043445862829685, 0.591698408126831, 0.0, 0.0, 0.0], [0.4368314743041992, 0.027149807661771774, 0.0006580635090358555, 0.000750081439036876, 0.006737596355378628, 0.04006659612059593, 0.006790908984839916, 0.0026773265562951565, 0.0004981662495993078, 0.0007820974569767714, 0.019046755507588387, 0.45801106095314026, 0.0, 0.0], [0.2585664987564087, 0.04613356664776802, 0.0012799472315236926, 0.00011147998156957328, 0.00025251481565646827, 0.0076554506085813046, 0.003813674906268716, 0.004061694256961346, 0.0005409070872701705, 6.091715840739198e-05, 0.00017509785539004952, 0.016007795929908752, 0.6613404154777527, 0.0], [0.4745607376098633, 0.0014442685060203075, 0.0002979215933009982, 0.0003509631205815822, 0.0015571755357086658, 0.0025189751759171486, 0.0058411406353116035, 0.0050410619005560875, 0.001713705132715404, 0.0019715693779289722, 0.0025541367940604687, 0.005033628083765507, 0.004032942000776529, 0.49308183789253235]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9825226068496704, 0.01747734844684601, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8109061121940613, 0.11287897825241089, 0.07621489465236664, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8248053789138794, 0.13184794783592224, 0.0430481843650341, 0.00029845800600014627, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.15355609357357025, 0.17923752963542938, 0.659443736076355, 0.007695595733821392, 6.70992667437531e-05, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.06819247454404831, 0.03366551548242569, 0.779695451259613, 0.1158425435423851, 0.002553770551458001, 5.019269519834779e-05, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8628752827644348, 0.05774027854204178, 0.02491001971065998, 0.006778161972761154, 0.0034441370517015457, 0.0019982263911515474, 0.0422537624835968, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5770450830459595, 0.008367315866053104, 0.005020377226173878, 0.004872691817581654, 0.0062871468253433704, 0.016580166295170784, 0.31569865345954895, 0.06612860411405563, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.39540791511535645, 0.017369814217090607, 0.02951694279909134, 0.027842331677675247, 0.017618346959352493, 0.03603058680891991, 0.35154959559440613, 0.09947604686021805, 0.025188472121953964, 0.0, 0.0, 0.0, 0.0, 0.0], [0.22273634374141693, 0.008237957023084164, 0.07505892962217331, 0.06469053030014038, 0.13058800995349884, 0.22823376953601837, 0.17457851767539978, 0.0490686371922493, 0.03379008546471596, 0.013017240911722183, 0.0, 0.0, 0.0, 0.0], [0.4264262616634369, 0.0008228494552895427, 0.007944794371724129, 0.01147626992315054, 0.007991276681423187, 0.04653686285018921, 0.1378771960735321, 0.10090756416320801, 0.17042846977710724, 0.08827675879001617, 0.0013117033522576094, 0.0, 0.0, 0.0], [0.17837020754814148, 8.119055564748123e-05, 0.002510770922526717, 0.001060557086020708, 0.0005361127550713718, 0.001629635109566152, 0.010586136020720005, 0.02427312359213829, 0.23977947235107422, 0.5359377264976501, 0.005089611746370792, 0.00014540897973347455, 0.0, 0.0], [0.02109161950647831, 7.238813395815669e-06, 0.0002573640667833388, 0.00046986673260107636, 0.0005875244387425482, 0.0003633321903180331, 0.001975497929379344, 0.012145349755883217, 0.06824853271245956, 0.767472505569458, 0.12520664930343628, 0.002133473753929138, 4.1081915696850047e-05, 0.0], [0.5236634612083435, 0.005397687200456858, 0.0006811206694692373, 0.0030450692865997553, 0.002499031601473689, 0.004082532599568367, 0.13937610387802124, 0.13448558747768402, 0.04494018480181694, 0.002254016697406769, 0.0020275407005101442, 0.00218160729855299, 0.001068333862349391, 0.1342976987361908]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9771690368652344, 0.02283100038766861, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9534196257591248, 0.04477500915527344, 0.0018054225947707891, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.656300961971283, 0.27738112211227417, 0.06563031673431396, 0.0006876075640320778, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.00784461759030819, 0.005751375108957291, 0.7900513410568237, 0.19611020386219025, 0.0002424300619168207, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0012073114048689604, 2.7022266294807196e-05, 0.003514885902404785, 0.9645757079124451, 0.030643831938505173, 3.131231642328203e-05, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.745050847530365, 0.03567741438746452, 0.03705528751015663, 0.14288710057735443, 0.028388088569045067, 0.0075537399388849735, 0.00338747794739902, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7780994176864624, 0.0465330109000206, 0.018250728026032448, 0.01533550675958395, 0.025325654074549675, 0.033259067684412, 0.06542965769767761, 0.017767051234841347, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5910347104072571, 0.011984539218246937, 0.0035059149377048016, 0.00910236220806837, 0.009534601122140884, 0.040714941918849945, 0.26957234740257263, 0.04668022319674492, 0.01787032000720501, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6557325124740601, 0.006406444124877453, 0.008750486187636852, 0.0029222453013062477, 0.0027606114745140076, 0.01459326408803463, 0.07776829600334167, 0.07437196373939514, 0.14815761148929596, 0.008536579087376595, 0.0, 0.0, 0.0, 0.0], [0.23917743563652039, 2.8771146389772184e-05, 6.656552432104945e-05, 0.00010794835543492809, 3.0500224966090173e-05, 0.00013769828365184367, 0.0035683822352439165, 0.027670489624142647, 0.6860268115997314, 0.04311194643378258, 7.34458735678345e-05, 0.0, 0.0, 0.0], [0.005965479649603367, 2.2930703380552586e-07, 5.806378339912044e-06, 5.737757419410627e-06, 8.168950103026873e-07, 2.553585716214002e-07, 1.2766146028297953e-05, 0.000190978535101749, 0.014980203472077847, 0.9643422961235046, 0.014483907260000706, 1.1537013961060438e-05, 0.0, 0.0], [0.002094743773341179, 5.730523255920161e-08, 3.321987094295764e-07, 1.6449318991362816e-06, 3.5623222629510565e-06, 3.4424741102156986e-07, 5.188272098166635e-06, 5.5314179917331785e-05, 8.993940718937665e-05, 0.025769732892513275, 0.9235650300979614, 0.04839342087507248, 2.0735165890073404e-05, 0.0], [0.9170337319374084, 0.003401221241801977, 0.0004237664397805929, 0.0014788418775424361, 0.0011227671056985855, 0.002538465429097414, 0.011861820705235004, 0.019708530977368355, 0.02374952659010887, 0.0020812733564525843, 0.003346616169437766, 0.003709648037329316, 0.0020265562925487757, 0.007517208345234394]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.923840343952179, 0.07615962624549866, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9242520928382874, 0.05390534922480583, 0.021842531859874725, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8565113544464111, 0.06747837364673615, 0.030718596652150154, 0.04529161378741264, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5589109659194946, 0.019068751484155655, 0.029015807434916496, 0.15415218472480774, 0.23885227739810944, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.43616926670074463, 0.07509396970272064, 0.049173589795827866, 0.11021355539560318, 0.2131900191307068, 0.11615961045026779, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.28985244035720825, 0.031180664896965027, 0.019867945462465286, 0.06874444335699081, 0.28367578983306885, 0.25601184368133545, 0.050666891038417816, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.16139286756515503, 0.03178552910685539, 0.030069749802350998, 0.12482921034097672, 0.29863545298576355, 0.27299433946609497, 0.06314302980899811, 0.017149852588772774, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.1606202870607376, 0.022249868139624596, 0.03934523090720177, 0.14087983965873718, 0.23144882917404175, 0.26218703389167786, 0.07136627286672592, 0.05349859967827797, 0.018404049798846245, 0.0, 0.0, 0.0, 0.0, 0.0], [0.28669309616088867, 0.02380632422864437, 0.04340513050556183, 0.12316442281007767, 0.12258594483137131, 0.18459685146808624, 0.08435018360614777, 0.0675685778260231, 0.039874132722616196, 0.02395530417561531, 0.0, 0.0, 0.0, 0.0], [0.4575786888599396, 0.017085686326026917, 0.020521994680166245, 0.06133236736059189, 0.09036624431610107, 0.0990559384226799, 0.04997606948018074, 0.11158506572246552, 0.04614248499274254, 0.024629224091768265, 0.021726233884692192, 0.0, 0.0, 0.0], [0.41061896085739136, 0.010439760982990265, 0.009034182876348495, 0.051250673830509186, 0.050526950508356094, 0.06097397208213806, 0.07766382396221161, 0.09662413597106934, 0.057443372905254364, 0.06297637522220612, 0.07050715386867523, 0.04194057732820511, 0.0, 0.0], [0.41086432337760925, 0.018526485189795494, 0.010941238142549992, 0.042542193084955215, 0.06482290476560593, 0.05887535214424133, 0.08413499593734741, 0.09481298178434372, 0.046778354793787, 0.039649516344070435, 0.04524135962128639, 0.053678352385759354, 0.029131900519132614, 0.0], [0.13776437938213348, 0.03540769964456558, 0.03580937534570694, 0.04955481365323067, 0.055680375546216965, 0.05956912413239479, 0.23597697913646698, 0.0908176451921463, 0.049427833408117294, 0.01454407162964344, 0.00935352686792612, 0.01456548273563385, 0.038720276206731796, 0.17280837893486023]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9094971418380737, 0.09050286561250687, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.49392879009246826, 0.3861246407032013, 0.11994665116071701, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.09710855036973953, 0.16857893764972687, 0.4353431463241577, 0.2989693582057953, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.015918971970677376, 0.017600273713469505, 0.128404900431633, 0.6139048933982849, 0.2241709679365158, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0142045384272933, 0.005356911104172468, 0.020793059840798378, 0.2025715857744217, 0.5234061479568481, 0.2336677461862564, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0060067735612392426, 0.0004478675255086273, 0.00037254937342368066, 0.004531038459390402, 0.11474203318357468, 0.5543973445892334, 0.3195023834705353, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.00042188880615867674, 3.179690247634426e-05, 1.050555852089019e-06, 1.3605579169961857e-06, 0.000135181617224589, 0.044857483357191086, 0.9399420619010925, 0.014609163627028465, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0004924473469145596, 0.0002833761682268232, 0.00034893647534772754, 0.00040453876135870814, 0.0004479407798498869, 0.0020148351322859526, 0.05946176499128342, 0.8971024751663208, 0.03944360837340355, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0005270896945148706, 0.0002596454578451812, 0.0006366263842210174, 0.0012680354993790388, 0.0009306353167630732, 0.0015780957182869315, 0.022555485367774963, 0.6503759622573853, 0.20055077970027924, 0.12131766229867935, 0.0, 0.0, 0.0, 0.0], [0.0009435969404876232, 0.00018085140618495643, 0.00047382639604620636, 0.0019233559723943472, 0.002164699137210846, 0.0023969446774572134, 0.011621820740401745, 0.17453549802303314, 0.12659290432929993, 0.46474936604499817, 0.21441715955734253, 0.0, 0.0, 0.0], [0.0023748958483338356, 0.00016392095130868256, 0.00015601638006046414, 0.0006126567022874951, 0.001413923455402255, 0.002820861293002963, 0.013323337770998478, 0.06170903891324997, 0.021610409021377563, 0.15459536015987396, 0.45965999364852905, 0.28155964612960815, 0.0, 0.0], [0.005151176825165749, 0.0003238176868762821, 0.00010808699153130874, 0.0001979246299015358, 0.0005275685107335448, 0.0019463258795440197, 0.012570026330649853, 0.029282430186867714, 0.004420108627527952, 0.021446052938699722, 0.15875165164470673, 0.5687544941902161, 0.19652032852172852, 0.0], [0.0016101521905511618, 0.00015059014549478889, 3.993787686340511e-06, 2.0216073153278558e-06, 1.73942135006655e-05, 0.0004671389178838581, 0.004280027002096176, 0.000641716702375561, 2.797095112327952e-05, 3.6298872146289796e-05, 0.0015175915323197842, 0.18530355393886566, 0.6163738965988159, 0.1895676702260971]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.32037678360939026, 0.6796231865882874, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.26568472385406494, 0.026817932724952698, 0.7074973583221436, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2904929220676422, 0.0024178139865398407, 0.018628772348165512, 0.6884605288505554, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3237868547439575, 0.0025800582952797413, 0.0010408364469185472, 0.015982385724782944, 0.6566098928451538, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.19885562360286713, 0.0032639962155371904, 9.914167458191514e-05, 0.00027385904104448855, 0.019207719713449478, 0.7782995700836182, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.430844247341156, 0.020986847579479218, 0.00018130017269868404, 1.128239182435209e-05, 3.2753145205788314e-05, 0.005135911516845226, 0.5428076982498169, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.22232751548290253, 0.006268353667110205, 0.0006558758905157447, 2.0708306692540646e-05, 6.111773018346867e-06, 1.9749151761061512e-05, 0.01248614676296711, 0.758215606212616, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.14187432825565338, 0.0006611442659050226, 0.0012426070170477033, 0.0008459357777610421, 7.459645712515339e-05, 3.6349149468151154e-06, 0.00040399833233095706, 0.012890822254121304, 0.8420029282569885, 0.0, 0.0, 0.0, 0.0, 0.0], [0.20185542106628418, 0.00015507201896980405, 0.00044562143739312887, 0.0022372673265635967, 0.0017789416015148163, 4.347516005509533e-05, 0.0002240507456008345, 0.0005955742672085762, 0.019227728247642517, 0.7734367251396179, 0.0, 0.0, 0.0, 0.0], [0.13391292095184326, 0.00023894160403870046, 7.779227598803118e-05, 0.00025068511604331434, 0.0033182823099195957, 0.0009292513714171946, 0.0012032885570079088, 0.00025281321723014116, 0.0004746205813717097, 0.019740330055356026, 0.8396011590957642, 0.0, 0.0, 0.0], [0.2719283103942871, 0.0012741314712911844, 6.822930299676955e-05, 3.593645669752732e-05, 0.0013272595824673772, 0.012223353609442711, 0.014995490200817585, 0.001403085421770811, 0.00010678239050321281, 0.0002753387962002307, 0.03114878199994564, 0.6652133464813232, 0.0, 0.0], [0.2495344579219818, 0.0029642360750585794, 0.00016291408974211663, 9.853482879407238e-06, 2.4927374397520907e-05, 0.0008825751719996333, 0.007679190021008253, 0.006900749634951353, 0.00027535215485841036, 2.7893409423995763e-05, 0.00042167649371549487, 0.024479007348418236, 0.7066371440887451, 0.0], [0.2670253813266754, 0.010363082401454449, 0.0013464231742545962, 3.291423126938753e-05, 8.078260407273774e-07, 1.9563726709748153e-06, 0.0006151689449325204, 0.007851659320294857, 0.005011851899325848, 9.117060108110309e-05, 2.0014578694826923e-05, 1.8211318092653528e-05, 0.0026739765889942646, 0.7049473524093628]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8956234455108643, 0.10437658429145813, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9007123708724976, 0.05960393324494362, 0.03968369960784912, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8794002532958984, 0.03619878366589546, 0.035411328077316284, 0.048989638686180115, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8395003080368042, 0.025023698806762695, 0.03212537243962288, 0.04160716384649277, 0.06174341216683388, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8039843440055847, 0.026626328006386757, 0.0330892950296402, 0.0441359207034111, 0.05568467080593109, 0.036479443311691284, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.721079409122467, 0.01925572194159031, 0.014901289716362953, 0.02449408546090126, 0.06429840624332428, 0.07929249107837677, 0.07667864114046097, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7467319369316101, 0.015869542956352234, 0.01474791020154953, 0.024130376055836678, 0.05338607355952263, 0.05450769141316414, 0.035846978425979614, 0.05477951094508171, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7997636198997498, 0.005678670946508646, 0.00788411870598793, 0.014273611828684807, 0.030766824260354042, 0.035897769033908844, 0.0190696083009243, 0.027656255289912224, 0.059009432792663574, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7855356335639954, 0.0034045367501676083, 0.004503246862441301, 0.009224043227732182, 0.024502895772457123, 0.03416083753108978, 0.01801406964659691, 0.026679379865527153, 0.053019363433122635, 0.0409560427069664, 0.0, 0.0, 0.0, 0.0], [0.81943279504776, 0.0021087806671857834, 0.003035762580111623, 0.007097180467098951, 0.01695769652724266, 0.025882206857204437, 0.014116928912699223, 0.020900843665003777, 0.030567942187190056, 0.030542708933353424, 0.02935723587870598, 0.0, 0.0, 0.0], [0.7820701599121094, 0.0019051837734878063, 0.00274364510551095, 0.006236993242055178, 0.013870167545974255, 0.02092333883047104, 0.01222662441432476, 0.022095894441008568, 0.038750581443309784, 0.03869524598121643, 0.03260093554854393, 0.027881167829036713, 0.0, 0.0], [0.7406590580940247, 0.001285201869904995, 0.0017084534047171474, 0.003415263257920742, 0.006662486121058464, 0.011460528708994389, 0.0099190603941679, 0.021536417305469513, 0.03798515349626541, 0.0458051860332489, 0.043222472071647644, 0.044980213046073914, 0.03136051818728447, 0.0], [0.43525153398513794, 0.005559141747653484, 0.003563696751371026, 0.004489265847951174, 0.006379684433341026, 0.01028935331851244, 0.0331721305847168, 0.08385492861270905, 0.04310521110892296, 0.03333389386534691, 0.04286014661192894, 0.04482324793934822, 0.09435758739709854, 0.15896019339561462]]], \"attentionHeadNames\": [\"L22H7\", \"L0H11\", \"L20H12\", \"L7H4\", \"L3H12\", \"L3H13\", \"L5H4\", \"L0H13\", \"L4H12\", \"L10H4\"], \"tokens\": [\"<|endoftext|>\", \"7\", \"0\", \"6\", \"2\", \"0\", \" x\", \" \", \"9\", \"9\", \"4\", \"6\", \"0\", \" =\"]}\n",
       "    )\n",
       "    </script></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_k = 10\n",
    "top_heads_by_output_patch = torch.topk(\n",
    "    patched_head_z_diff.abs().flatten(), k=top_k\n",
    ").indices\n",
    "first_layer = 0\n",
    "late_heads = top_heads_by_output_patch[\n",
    "    tl_model.cfg.n_heads * first_layer <= top_heads_by_output_patch\n",
    "]\n",
    "\n",
    "late = visualize_attention_patterns(\n",
    "    late_heads, cache, tokens[0], title=\"Top Late Heads\"\n",
    ")\n",
    "\n",
    "HTML(late)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
